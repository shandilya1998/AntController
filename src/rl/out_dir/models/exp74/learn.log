running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_3
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 573      |
| time/              |          |
|    fps             | 308      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | -2.8       |
| reward_contact          | -0.0461    |
| reward_ctrl             | 6.04e-05   |
| reward_motion           | 1.47e-06   |
| reward_position         | 0.00022    |
| reward_torque           | -3.44      |
| reward_velocity         | 0.677      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 392        |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 2          |
|    time_elapsed         | 8          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.15049204 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18        |
|    explained_variance   | -7.55e-05  |
|    learning_rate        | 0.0003     |
|    loss                 | 209        |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.082     |
|    std                  | 0.368      |
|    value_loss           | 1.18e+03   |
----------------------------------------
----------------------------------------
| reward                  | -2.23      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.73e-05   |
| reward_motion           | 9.24e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.83      |
| reward_velocity         | 0.645      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 481        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 3          |
|    time_elapsed         | 13         |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.10099134 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.247      |
|    learning_rate        | 0.0003     |
|    loss                 | 93.9       |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.089     |
|    std                  | 0.369      |
|    value_loss           | 857        |
----------------------------------------
----------------------------------------
| reward                  | -2.12      |
| reward_contact          | -0.0474    |
| reward_ctrl             | 3.22e-05   |
| reward_motion           | 8.05e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.706      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 615        |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 4          |
|    time_elapsed         | 18         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.15403327 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.2      |
|    explained_variance   | 0.2        |
|    learning_rate        | 0.0003     |
|    loss                 | 18.9       |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.368      |
|    value_loss           | 672        |
----------------------------------------
----------------------------------------
| reward                  | -2.09      |
| reward_contact          | -0.0475    |
| reward_ctrl             | 2.92e-05   |
| reward_motion           | 7.39e-07   |
| reward_position         | 0.000111   |
| reward_torque           | -2.8       |
| reward_velocity         | 0.755      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 631        |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 5          |
|    time_elapsed         | 23         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.13035062 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.1      |
|    explained_variance   | 0.408      |
|    learning_rate        | 0.0003     |
|    loss                 | 67.5       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.368      |
|    value_loss           | 684        |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 630.72
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | -2          |
| reward_contact          | -0.047      |
| reward_ctrl             | 2.97e-05    |
| reward_motion           | 6.88e-07    |
| reward_position         | 0.000103    |
| reward_torque           | -2.7        |
| reward_velocity         | 0.75        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 542         |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 6           |
|    time_elapsed         | 28          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.052000042 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | 78.7        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0641     |
|    std                  | 0.368       |
|    value_loss           | 745         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.03       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 2.66e-05    |
| reward_motion           | 6.1e-07     |
| reward_position         | 9.15e-05    |
| reward_torque           | -2.75       |
| reward_velocity         | 0.767       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 550         |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 7           |
|    time_elapsed         | 34          |
|    total_timesteps      | 7168        |
| train/                  |             |
|    approx_kl            | 0.037831835 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.3       |
|    explained_variance   | 0.398       |
|    learning_rate        | 0.0003      |
|    loss                 | 88.6        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0412     |
|    std                  | 0.368       |
|    value_loss           | 572         |
-----------------------------------------
----------------------------------------
| reward                  | -2.04      |
| reward_contact          | -0.0473    |
| reward_ctrl             | 2.74e-05   |
| reward_motion           | 6.05e-07   |
| reward_position         | 9.08e-05   |
| reward_torque           | -2.8       |
| reward_velocity         | 0.8        |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 634        |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 8          |
|    time_elapsed         | 39         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.04968562 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.9      |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | 62.2       |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0584    |
|    std                  | 0.368      |
|    value_loss           | 608        |
----------------------------------------
----------------------------------------
| reward                  | -1.93      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 2.63e-05   |
| reward_motion           | 5.87e-07   |
| reward_position         | 8.8e-05    |
| reward_torque           | -2.66      |
| reward_velocity         | 0.781      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 618        |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 9          |
|    time_elapsed         | 44         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.10000177 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.6      |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.0003     |
|    loss                 | 53.1       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0886    |
|    std                  | 0.368      |
|    value_loss           | 664        |
----------------------------------------
-----------------------------------------
| reward                  | -2          |
| reward_contact          | -0.0473     |
| reward_ctrl             | 2.54e-05    |
| reward_motion           | 5.8e-07     |
| reward_position         | 8.71e-05    |
| reward_torque           | -2.72       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 659         |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 10          |
|    time_elapsed         | 49          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.045438074 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.7       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 114         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0477     |
|    std                  | 0.368       |
|    value_loss           | 853         |
-----------------------------------------
-----------------------------------------
| reward                  | -1.87       |
| reward_contact          | -0.0473     |
| reward_ctrl             | 2.56e-05    |
| reward_motion           | 5.89e-07    |
| reward_position         | 8.84e-05    |
| reward_torque           | -2.59       |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 679         |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 11          |
|    time_elapsed         | 54          |
|    total_timesteps      | 11264       |
| train/                  |             |
|    approx_kl            | 0.087017946 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.4         |
|    entropy_loss         | -14.2       |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0003      |
|    loss                 | 60.4        |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0688     |
|    std                  | 0.368       |
|    value_loss           | 643         |
-----------------------------------------
Num timesteps: 12000
Best mean reward: 630.72 - Last mean reward per episode: 678.77
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | -1.94       |
| reward_contact          | -0.0473     |
| reward_ctrl             | 2.5e-05     |
| reward_motion           | 5.78e-07    |
| reward_position         | 8.67e-05    |
| reward_torque           | -2.68       |
| reward_velocity         | 0.786       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 666         |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 12          |
|    time_elapsed         | 59          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.037214965 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.8       |
|    explained_variance   | 0.802       |
|    learning_rate        | 0.0003      |
|    loss                 | 108         |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0488     |
|    std                  | 0.368       |
|    value_loss           | 700         |
-----------------------------------------
----------------------------------------
| reward                  | -1.95      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 3.45e-05   |
| reward_motion           | 7.29e-07   |
| reward_position         | 0.000109   |
| reward_torque           | -2.69      |
| reward_velocity         | 0.787      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 701        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 13         |
|    time_elapsed         | 65         |
|    total_timesteps      | 13312      |
| train/                  |            |
|    approx_kl            | 0.03277895 |
|    clip_fraction        | 0.0793     |
|    clip_range           | 0.4        |
|    entropy_loss         | -17        |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | 91.2       |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0453    |
|    std                  | 0.368      |
|    value_loss           | 757        |
----------------------------------------
----------------------------------------
| reward                  | -1.93      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 3.49e-05   |
| reward_motion           | 7.56e-07   |
| reward_position         | 0.000113   |
| reward_torque           | -2.67      |
| reward_velocity         | 0.782      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 730        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 14         |
|    time_elapsed         | 70         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.09893784 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14        |
|    explained_variance   | 0.858      |
|    learning_rate        | 0.0003     |
|    loss                 | 88         |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0915    |
|    std                  | 0.368      |
|    value_loss           | 739        |
----------------------------------------
-----------------------------------------
| reward                  | -1.93       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 3.75e-05    |
| reward_motion           | 7.95e-07    |
| reward_position         | 0.000119    |
| reward_torque           | -2.68       |
| reward_velocity         | 0.79        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 761         |
| time/                   |             |
|    fps                  | 203         |
|    iterations           | 15          |
|    time_elapsed         | 75          |
|    total_timesteps      | 15360       |
| train/                  |             |
|    approx_kl            | 0.059402347 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16         |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | 56.6        |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0687     |
|    std                  | 0.368       |
|    value_loss           | 501         |
-----------------------------------------
----------------------------------------
| reward                  | -1.92      |
| reward_contact          | -0.0473    |
| reward_ctrl             | 3.97e-05   |
| reward_motion           | 8.62e-07   |
| reward_position         | 0.000129   |
| reward_torque           | -2.68      |
| reward_velocity         | 0.802      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 772        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 16         |
|    time_elapsed         | 80         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.05526676 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15        |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | 166        |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0708    |
|    std                  | 0.368      |
|    value_loss           | 819        |
----------------------------------------
-----------------------------------------
| reward                  | -1.93       |
| reward_contact          | -0.0473     |
| reward_ctrl             | 4.02e-05    |
| reward_motion           | 8.65e-07    |
| reward_position         | 0.00013     |
| reward_torque           | -2.68       |
| reward_velocity         | 0.803       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 792         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 17          |
|    time_elapsed         | 85          |
|    total_timesteps      | 17408       |
| train/                  |             |
|    approx_kl            | 0.072550446 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.6       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 165         |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0741     |
|    std                  | 0.368       |
|    value_loss           | 811         |
-----------------------------------------
Num timesteps: 18000
Best mean reward: 678.77 - Last mean reward per episode: 791.68
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | -1.88       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.31e-05    |
| reward_motion           | 9e-07       |
| reward_position         | 0.000135    |
| reward_torque           | -2.64       |
| reward_velocity         | 0.809       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 797         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 18          |
|    time_elapsed         | 91          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.050225984 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.4       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | 94          |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0662     |
|    std                  | 0.368       |
|    value_loss           | 709         |
-----------------------------------------
----------------------------------------
| reward                  | -1.88      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.34e-05   |
| reward_motion           | 9.03e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -2.65      |
| reward_velocity         | 0.82       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 790        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 19         |
|    time_elapsed         | 96         |
|    total_timesteps      | 19456      |
| train/                  |            |
|    approx_kl            | 0.04830476 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.4      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0003     |
|    loss                 | 161        |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0546    |
|    std                  | 0.368      |
|    value_loss           | 733        |
----------------------------------------
----------------------------------------
| reward                  | -1.89      |
| reward_contact          | -0.0473    |
| reward_ctrl             | 4.26e-05   |
| reward_motion           | 8.8e-07    |
| reward_position         | 0.000132   |
| reward_torque           | -2.65      |
| reward_velocity         | 0.814      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 773        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 20         |
|    time_elapsed         | 101        |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.06929466 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.3      |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0003     |
|    loss                 | 116        |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0651    |
|    std                  | 0.368      |
|    value_loss           | 760        |
----------------------------------------
-----------------------------------------
| reward                  | -1.91       |
| reward_contact          | -0.0473     |
| reward_ctrl             | 4.11e-05    |
| reward_motion           | 8.51e-07    |
| reward_position         | 0.000128    |
| reward_torque           | -2.67       |
| reward_velocity         | 0.808       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 775         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 21          |
|    time_elapsed         | 106         |
|    total_timesteps      | 21504       |
| train/                  |             |
|    approx_kl            | 0.062169574 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.4         |
|    entropy_loss         | -14.9       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.27e+03    |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0579     |
|    std                  | 0.368       |
|    value_loss           | 864         |
-----------------------------------------
-----------------------------------------
| reward                  | -1.94       |
| reward_contact          | -0.0473     |
| reward_ctrl             | 3.99e-05    |
| reward_motion           | 8.29e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -2.7        |
| reward_velocity         | 0.806       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 771         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 22          |
|    time_elapsed         | 111         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.046546943 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.1       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 722         |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0547     |
|    std                  | 0.368       |
|    value_loss           | 815         |
-----------------------------------------
----------------------------------------
| reward                  | -1.98      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.12e-05   |
| reward_motion           | 8.62e-07   |
| reward_position         | 0.000129   |
| reward_torque           | -2.74      |
| reward_velocity         | 0.802      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 792        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 23         |
|    time_elapsed         | 116        |
|    total_timesteps      | 23552      |
| train/                  |            |
|    approx_kl            | 0.03915607 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.2      |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0003     |
|    loss                 | 146        |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0404    |
|    std                  | 0.368      |
|    value_loss           | 648        |
----------------------------------------
Num timesteps: 24000
Best mean reward: 791.68 - Last mean reward per episode: 791.71
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | -2.04       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.23e-05    |
| reward_motion           | 8.77e-07    |
| reward_position         | 0.000132    |
| reward_torque           | -2.79       |
| reward_velocity         | 0.804       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 765         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 24          |
|    time_elapsed         | 121         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.039605044 |
|    clip_fraction        | 0.089       |
|    clip_range           | 0.4         |
|    entropy_loss         | -14.9       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | 219         |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0567     |
|    std                  | 0.368       |
|    value_loss           | 903         |
-----------------------------------------
---------------------------------------
| reward                  | -2.02     |
| reward_contact          | -0.0472   |
| reward_ctrl             | 4.09e-05  |
| reward_motion           | 8.53e-07  |
| reward_position         | 0.000128  |
| reward_torque           | -2.77     |
| reward_velocity         | 0.797     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 771       |
| time/                   |           |
|    fps                  | 202       |
|    iterations           | 25        |
|    time_elapsed         | 126       |
|    total_timesteps      | 25600     |
| train/                  |           |
|    approx_kl            | 0.0371536 |
|    clip_fraction        | 0.0811    |
|    clip_range           | 0.4       |
|    entropy_loss         | -16.7     |
|    explained_variance   | 0.951     |
|    learning_rate        | 0.0003    |
|    loss                 | 354       |
|    n_updates            | 480       |
|    policy_gradient_loss | -0.0502   |
|    std                  | 0.368     |
|    value_loss           | 939       |
---------------------------------------
-----------------------------------------
| reward                  | -2          |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.01e-05    |
| reward_motion           | 8.38e-07    |
| reward_position         | 0.000126    |
| reward_torque           | -2.74       |
| reward_velocity         | 0.79        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 768         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 26          |
|    time_elapsed         | 131         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.063086435 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.7       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 92.9        |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0678     |
|    std                  | 0.368       |
|    value_loss           | 771         |
-----------------------------------------
-----------------------------------------
| reward                  | -1.99       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 3.95e-05    |
| reward_motion           | 8.27e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -2.73       |
| reward_velocity         | 0.788       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 769         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 27          |
|    time_elapsed         | 136         |
|    total_timesteps      | 27648       |
| train/                  |             |
|    approx_kl            | 0.055129383 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.4         |
|    entropy_loss         | -14.3       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.1e+03     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0717     |
|    std                  | 0.368       |
|    value_loss           | 819         |
-----------------------------------------
---------------------------------------
| reward                  | -2.03     |
| reward_contact          | -0.0472   |
| reward_ctrl             | 3.95e-05  |
| reward_motion           | 8.25e-07  |
| reward_position         | 0.000124  |
| reward_torque           | -2.77     |
| reward_velocity         | 0.783     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 744       |
| time/                   |           |
|    fps                  | 202       |
|    iterations           | 28        |
|    time_elapsed         | 141       |
|    total_timesteps      | 28672     |
| train/                  |           |
|    approx_kl            | 0.0534485 |
|    clip_fraction        | 0.145     |
|    clip_range           | 0.4       |
|    entropy_loss         | -15.5     |
|    explained_variance   | 0.964     |
|    learning_rate        | 0.0003    |
|    loss                 | 203       |
|    n_updates            | 540       |
|    policy_gradient_loss | -0.0704   |
|    std                  | 0.368     |
|    value_loss           | 804       |
---------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 3.83e-05   |
| reward_motion           | 8.01e-07   |
| reward_position         | 0.00012    |
| reward_torque           | -2.81      |
| reward_velocity         | 0.786      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 723        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 29         |
|    time_elapsed         | 146        |
|    total_timesteps      | 29696      |
| train/                  |            |
|    approx_kl            | 0.05150444 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.8      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.27e+03   |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.0435    |
|    std                  | 0.368      |
|    value_loss           | 829        |
----------------------------------------
Num timesteps: 30000
Best mean reward: 791.71 - Last mean reward per episode: 722.75
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.7e-07    |
| reward_position         | 0.00013    |
| reward_torque           | -2.84      |
| reward_velocity         | 0.78       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 701        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 30         |
|    time_elapsed         | 151        |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.06510179 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.9      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.35e+03   |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.046     |
|    std                  | 0.368      |
|    value_loss           | 806        |
----------------------------------------
-----------------------------------------
| reward                  | -2.12       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.18e-05    |
| reward_motion           | 8.55e-07    |
| reward_position         | 0.000128    |
| reward_torque           | -2.85       |
| reward_velocity         | 0.777       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 705         |
| time/                   |             |
|    fps                  | 203         |
|    iterations           | 31          |
|    time_elapsed         | 156         |
|    total_timesteps      | 31744       |
| train/                  |             |
|    approx_kl            | 0.051414184 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.3       |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.0003      |
|    loss                 | 642         |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0474     |
|    std                  | 0.368       |
|    value_loss           | 672         |
-----------------------------------------
----------------------------------------
| reward                  | -2.15      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.15e-05   |
| reward_motion           | 8.57e-07   |
| reward_position         | 0.000129   |
| reward_torque           | -2.87      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 719        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 32         |
|    time_elapsed         | 161        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.07030794 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0003     |
|    loss                 | 680        |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0665    |
|    std                  | 0.368      |
|    value_loss           | 708        |
----------------------------------------
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.1e-05    |
| reward_motion           | 8.48e-07   |
| reward_position         | 0.000127   |
| reward_torque           | -2.89      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 728        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 33         |
|    time_elapsed         | 166        |
|    total_timesteps      | 33792      |
| train/                  |            |
|    approx_kl            | 0.07115512 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.8      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.18e+03   |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.081     |
|    std                  | 0.368      |
|    value_loss           | 857        |
----------------------------------------
----------------------------------------
| reward                  | -2.12      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.23e-05   |
| reward_motion           | 8.63e-07   |
| reward_position         | 0.000129   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 735        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 34         |
|    time_elapsed         | 171        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.05076515 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.6      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 129        |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.0651    |
|    std                  | 0.367      |
|    value_loss           | 843        |
----------------------------------------
-----------------------------------------
| reward                  | -2.1        |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.14e-05    |
| reward_motion           | 8.46e-07    |
| reward_position         | 0.000127    |
| reward_torque           | -2.82       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 720         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 35          |
|    time_elapsed         | 177         |
|    total_timesteps      | 35840       |
| train/                  |             |
|    approx_kl            | 0.095722534 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.2       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 565         |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.0874     |
|    std                  | 0.367       |
|    value_loss           | 699         |
-----------------------------------------
Num timesteps: 36000
Best mean reward: 791.71 - Last mean reward per episode: 720.11
-----------------------------------------
| reward                  | -2.12       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.06e-05    |
| reward_motion           | 8.34e-07    |
| reward_position         | 0.000125    |
| reward_torque           | -2.84       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 709         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 36          |
|    time_elapsed         | 182         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.056049634 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.2       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0406     |
|    std                  | 0.367       |
|    value_loss           | 864         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.09       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4e-05       |
| reward_motion           | 8.26e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -2.81       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 718         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 37          |
|    time_elapsed         | 187         |
|    total_timesteps      | 37888       |
| train/                  |             |
|    approx_kl            | 0.044259906 |
|    clip_fraction        | 0.0995      |
|    clip_range           | 0.4         |
|    entropy_loss         | -14.7       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 88.2        |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0577     |
|    std                  | 0.367       |
|    value_loss           | 972         |
-----------------------------------------
----------------------------------------
| reward                  | -2.09      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.33e-05   |
| reward_motion           | 8.9e-07    |
| reward_position         | 0.000133   |
| reward_torque           | -2.81      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 719        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 38         |
|    time_elapsed         | 192        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.06741989 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.2      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 185        |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.073     |
|    std                  | 0.367      |
|    value_loss           | 703        |
----------------------------------------
----------------------------------------
| reward                  | -2.1       |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.68e-05   |
| reward_motion           | 9.5e-07    |
| reward_position         | 0.000143   |
| reward_torque           | -2.81      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 720        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 39         |
|    time_elapsed         | 197        |
|    total_timesteps      | 39936      |
| train/                  |            |
|    approx_kl            | 0.05378554 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.6      |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.0003     |
|    loss                 | 598        |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.056     |
|    std                  | 0.367      |
|    value_loss           | 841        |
----------------------------------------
----------------------------------------
| reward                  | -2.1       |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.63e-05   |
| reward_motion           | 9.42e-07   |
| reward_position         | 0.000141   |
| reward_torque           | -2.82      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 706        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 40         |
|    time_elapsed         | 202        |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.05185401 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.6      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 106        |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.0418    |
|    std                  | 0.367      |
|    value_loss           | 594        |
----------------------------------------
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.55e-05   |
| reward_motion           | 9.29e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 705        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 41         |
|    time_elapsed         | 207        |
|    total_timesteps      | 41984      |
| train/                  |            |
|    approx_kl            | 0.08426472 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | 147        |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0562    |
|    std                  | 0.367      |
|    value_loss           | 873        |
----------------------------------------
Num timesteps: 42000
Best mean reward: 791.71 - Last mean reward per episode: 705.46
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.53e-05   |
| reward_motion           | 9.28e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 713        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 42         |
|    time_elapsed         | 212        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.04333145 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16        |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 129        |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.0615    |
|    std                  | 0.367      |
|    value_loss           | 876        |
----------------------------------------
----------------------------------------
| reward                  | -2.12      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.46e-05   |
| reward_motion           | 9.15e-07   |
| reward_position         | 0.000137   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 709        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 43         |
|    time_elapsed         | 217        |
|    total_timesteps      | 44032      |
| train/                  |            |
|    approx_kl            | 0.06598474 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.8      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | 89.2       |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.0792    |
|    std                  | 0.367      |
|    value_loss           | 806        |
----------------------------------------
-----------------------------------------
| reward                  | -2.11       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.42e-05    |
| reward_motion           | 9.11e-07    |
| reward_position         | 0.000137    |
| reward_torque           | -2.83       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 706         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 44          |
|    time_elapsed         | 223         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.065347165 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15         |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | 118         |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.0702     |
|    std                  | 0.367       |
|    value_loss           | 765         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.13       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 4.59e-05    |
| reward_motion           | 9.43e-07    |
| reward_position         | 0.000141    |
| reward_torque           | -2.84       |
| reward_velocity         | 0.758       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 703         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 45          |
|    time_elapsed         | 228         |
|    total_timesteps      | 46080       |
| train/                  |             |
|    approx_kl            | 0.054258935 |
|    clip_fraction        | 0.0969      |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.3       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | 200         |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.0616     |
|    std                  | 0.367       |
|    value_loss           | 722         |
-----------------------------------------
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.57e-05   |
| reward_motion           | 9.38e-07   |
| reward_position         | 0.000141   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.755      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 717        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 46         |
|    time_elapsed         | 233        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.06634159 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | 115        |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.0705    |
|    std                  | 0.367      |
|    value_loss           | 761        |
----------------------------------------
Num timesteps: 48000
Best mean reward: 791.71 - Last mean reward per episode: 716.92
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 4.87e-05   |
| reward_motion           | 9.85e-07   |
| reward_position         | 0.000148   |
| reward_torque           | -2.83      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 712        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 47         |
|    time_elapsed         | 238        |
|    total_timesteps      | 48128      |
| train/                  |            |
|    approx_kl            | 0.05172252 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.2      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 64.5       |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.0734    |
|    std                  | 0.367      |
|    value_loss           | 678        |
----------------------------------------
-----------------------------------------
| reward                  | -2.11       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.97e-05    |
| reward_motion           | 1.02e-06    |
| reward_position         | 0.000153    |
| reward_torque           | -2.83       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 713         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 48          |
|    time_elapsed         | 243         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.035609327 |
|    clip_fraction        | 0.0871      |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.3       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 181         |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0352     |
|    std                  | 0.367       |
|    value_loss           | 728         |
-----------------------------------------
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.93e-05   |
| reward_motion           | 1.02e-06   |
| reward_position         | 0.000152   |
| reward_torque           | -2.83      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 697        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 49         |
|    time_elapsed         | 248        |
|    total_timesteps      | 50176      |
| train/                  |            |
|    approx_kl            | 0.05193235 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.5      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | 342        |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.0529    |
|    std                  | 0.367      |
|    value_loss           | 686        |
----------------------------------------
-----------------------------------------
| reward                  | -2.09       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.98e-05    |
| reward_motion           | 1.03e-06    |
| reward_position         | 0.000155    |
| reward_torque           | -2.81       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 687         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 50          |
|    time_elapsed         | 253         |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.053019665 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.4       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.43e+03    |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.0614     |
|    std                  | 0.367       |
|    value_loss           | 727         |
-----------------------------------------
---------------------------------------
| reward                  | -2.08     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 4.91e-05  |
| reward_motion           | 1.02e-06  |
| reward_position         | 0.000153  |
| reward_torque           | -2.8      |
| reward_velocity         | 0.761     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 671       |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 51        |
|    time_elapsed         | 259       |
|    total_timesteps      | 52224     |
| train/                  |           |
|    approx_kl            | 0.0752333 |
|    clip_fraction        | 0.163     |
|    clip_range           | 0.4       |
|    entropy_loss         | -14.9     |
|    explained_variance   | 0.972     |
|    learning_rate        | 0.0003    |
|    loss                 | 125       |
|    n_updates            | 1000      |
|    policy_gradient_loss | -0.0765   |
|    std                  | 0.367     |
|    value_loss           | 665       |
---------------------------------------
-----------------------------------------
| reward                  | -2.06       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.96e-05    |
| reward_motion           | 1.02e-06    |
| reward_position         | 0.000154    |
| reward_torque           | -2.78       |
| reward_velocity         | 0.759       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 669         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 52          |
|    time_elapsed         | 264         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.042688258 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.7       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 103         |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.0545     |
|    std                  | 0.367       |
|    value_loss           | 612         |
-----------------------------------------
Num timesteps: 54000
Best mean reward: 791.71 - Last mean reward per episode: 669.18
-----------------------------------------
| reward                  | -2.08       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.93e-05    |
| reward_motion           | 1.01e-06    |
| reward_position         | 0.000152    |
| reward_torque           | -2.79       |
| reward_velocity         | 0.759       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 667         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 53          |
|    time_elapsed         | 269         |
|    total_timesteps      | 54272       |
| train/                  |             |
|    approx_kl            | 0.032322936 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.7       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 129         |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.04       |
|    std                  | 0.367       |
|    value_loss           | 814         |
-----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.88e-05   |
| reward_motion           | 1.01e-06   |
| reward_position         | 0.000151   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.757      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 673        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 54         |
|    time_elapsed         | 274        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.06756666 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.3      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 138        |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.0784    |
|    std                  | 0.367      |
|    value_loss           | 715        |
----------------------------------------
-----------------------------------------
| reward                  | -2.06       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.84e-05    |
| reward_motion           | 9.95e-07    |
| reward_position         | 0.000149    |
| reward_torque           | -2.77       |
| reward_velocity         | 0.756       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 676         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 55          |
|    time_elapsed         | 280         |
|    total_timesteps      | 56320       |
| train/                  |             |
|    approx_kl            | 0.044751495 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.6       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0542     |
|    std                  | 0.367       |
|    value_loss           | 824         |
-----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.9e-05    |
| reward_motion           | 1.01e-06   |
| reward_position         | 0.000152   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.758      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 681        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 56         |
|    time_elapsed         | 285        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.08345644 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.2      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 158        |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.0713    |
|    std                  | 0.367      |
|    value_loss           | 586        |
----------------------------------------
-----------------------------------------
| reward                  | -2.07       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.9e-05     |
| reward_motion           | 1.01e-06    |
| reward_position         | 0.000151    |
| reward_torque           | -2.79       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 679         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 57          |
|    time_elapsed         | 290         |
|    total_timesteps      | 58368       |
| train/                  |             |
|    approx_kl            | 0.043336414 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.9       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.6        |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.0627     |
|    std                  | 0.367       |
|    value_loss           | 733         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.05       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.86e-05    |
| reward_motion           | 9.99e-07    |
| reward_position         | 0.00015     |
| reward_torque           | -2.77       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 672         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 58          |
|    time_elapsed         | 295         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.057172395 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15         |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 166         |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.0677     |
|    std                  | 0.367       |
|    value_loss           | 715         |
-----------------------------------------
Num timesteps: 60000
Best mean reward: 791.71 - Last mean reward per episode: 672.44
----------------------------------------
| reward                  | -2.06      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.79e-05   |
| reward_motion           | 9.88e-07   |
| reward_position         | 0.000148   |
| reward_torque           | -2.77      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 676        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 59         |
|    time_elapsed         | 300        |
|    total_timesteps      | 60416      |
| train/                  |            |
|    approx_kl            | 0.07305443 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.3      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 160        |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.0693    |
|    std                  | 0.367      |
|    value_loss           | 658        |
----------------------------------------
-----------------------------------------
| reward                  | -2.07       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.83e-05    |
| reward_motion           | 9.9e-07     |
| reward_position         | 0.000148    |
| reward_torque           | -2.79       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 675         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 60          |
|    time_elapsed         | 305         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.048278354 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17         |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 562         |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0548     |
|    std                  | 0.367       |
|    value_loss           | 745         |
-----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.78e-05   |
| reward_motion           | 9.8e-07    |
| reward_position         | 0.000147   |
| reward_torque           | -2.79      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 672        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 61         |
|    time_elapsed         | 310        |
|    total_timesteps      | 62464      |
| train/                  |            |
|    approx_kl            | 0.06619827 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15        |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 646        |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.0718    |
|    std                  | 0.367      |
|    value_loss           | 901        |
----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.72e-05   |
| reward_motion           | 9.69e-07   |
| reward_position         | 0.000145   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 666        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 62         |
|    time_elapsed         | 315        |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.07789113 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15        |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.18e+03   |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.0805    |
|    std                  | 0.367      |
|    value_loss           | 870        |
----------------------------------------
-----------------------------------------
| reward                  | -2.05       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.68e-05    |
| reward_motion           | 9.63e-07    |
| reward_position         | 0.000144    |
| reward_torque           | -2.77       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 664         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 63          |
|    time_elapsed         | 320         |
|    total_timesteps      | 64512       |
| train/                  |             |
|    approx_kl            | 0.051378116 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17         |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | 64.8        |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.0606     |
|    std                  | 0.367       |
|    value_loss           | 646         |
-----------------------------------------
----------------------------------------
| reward                  | -2.06      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.66e-05   |
| reward_motion           | 9.63e-07   |
| reward_position         | 0.000144   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 657        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 64         |
|    time_elapsed         | 325        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.07146649 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.5      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 188        |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.0743    |
|    std                  | 0.367      |
|    value_loss           | 703        |
----------------------------------------
Num timesteps: 66000
Best mean reward: 791.71 - Last mean reward per episode: 656.82
----------------------------------------
| reward                  | -2.06      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.64e-05   |
| reward_motion           | 9.62e-07   |
| reward_position         | 0.000144   |
| reward_torque           | -2.77      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 652        |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 65         |
|    time_elapsed         | 329        |
|    total_timesteps      | 66560      |
| train/                  |            |
|    approx_kl            | 0.06033282 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.6      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 71.9       |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.0711    |
|    std                  | 0.367      |
|    value_loss           | 651        |
----------------------------------------
-----------------------------------------
| reward                  | -2.06       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.6e-05     |
| reward_motion           | 9.54e-07    |
| reward_position         | 0.000143    |
| reward_torque           | -2.78       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 657         |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 66          |
|    time_elapsed         | 333         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.047285896 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.9       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | 277         |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.0579     |
|    std                  | 0.367       |
|    value_loss           | 624         |
-----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.59e-05   |
| reward_motion           | 9.48e-07   |
| reward_position         | 0.000142   |
| reward_torque           | -2.79      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 657        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 67         |
|    time_elapsed         | 337        |
|    total_timesteps      | 68608      |
| train/                  |            |
|    approx_kl            | 0.07419628 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.3      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 111        |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.0796    |
|    std                  | 0.367      |
|    value_loss           | 697        |
----------------------------------------
---------------------------------------
| reward                  | -2.07     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.56e-05  |
| reward_motion           | 9.41e-07  |
| reward_position         | 0.000141  |
| reward_torque           | -2.78     |
| reward_velocity         | 0.765     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 654       |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 68        |
|    time_elapsed         | 341       |
|    total_timesteps      | 69632     |
| train/                  |           |
|    approx_kl            | 0.0687863 |
|    clip_fraction        | 0.2       |
|    clip_range           | 0.4       |
|    entropy_loss         | -16.4     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 348       |
|    n_updates            | 1340      |
|    policy_gradient_loss | -0.0664   |
|    std                  | 0.367     |
|    value_loss           | 631       |
---------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.51e-05   |
| reward_motion           | 9.29e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.78      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 653        |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 69         |
|    time_elapsed         | 344        |
|    total_timesteps      | 70656      |
| train/                  |            |
|    approx_kl            | 0.06904027 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18        |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 42.4       |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.0647    |
|    std                  | 0.367      |
|    value_loss           | 371        |
----------------------------------------
----------------------------------------
| reward                  | -2.07      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.67e-05   |
| reward_motion           | 9.54e-07   |
| reward_position         | 0.000143   |
| reward_torque           | -2.79      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 651        |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 70         |
|    time_elapsed         | 347        |
|    total_timesteps      | 71680      |
| train/                  |            |
|    approx_kl            | 0.08205227 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.4      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 159        |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0798    |
|    std                  | 0.367      |
|    value_loss           | 587        |
----------------------------------------
Num timesteps: 72000
Best mean reward: 791.71 - Last mean reward per episode: 650.60
-----------------------------------------
| reward                  | -2.08       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.63e-05    |
| reward_motion           | 9.47e-07    |
| reward_position         | 0.000142    |
| reward_torque           | -2.79       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 646         |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 71          |
|    time_elapsed         | 351         |
|    total_timesteps      | 72704       |
| train/                  |             |
|    approx_kl            | 0.059771188 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.4       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | 61.5        |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.071      |
|    std                  | 0.367       |
|    value_loss           | 594         |
-----------------------------------------
----------------------------------------
| reward                  | -2.09      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.58e-05   |
| reward_motion           | 9.38e-07   |
| reward_position         | 0.000141   |
| reward_torque           | -2.8       |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 645        |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 72         |
|    time_elapsed         | 355        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.04455894 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.3      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 136        |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.0585    |
|    std                  | 0.367      |
|    value_loss           | 499        |
----------------------------------------
-----------------------------------------
| reward                  | -2.09       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.53e-05    |
| reward_motion           | 9.3e-07     |
| reward_position         | 0.000139    |
| reward_torque           | -2.81       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 647         |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 73          |
|    time_elapsed         | 360         |
|    total_timesteps      | 74752       |
| train/                  |             |
|    approx_kl            | 0.051120788 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.9       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | 659         |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.055      |
|    std                  | 0.367       |
|    value_loss           | 591         |
-----------------------------------------
----------------------------------------
| reward                  | -2.1       |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.5e-05    |
| reward_motion           | 9.24e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.82      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 642        |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 74         |
|    time_elapsed         | 364        |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.07270974 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.8      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 657        |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.0771    |
|    std                  | 0.367      |
|    value_loss           | 553        |
----------------------------------------
----------------------------------------
| reward                  | -2.08      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.53e-05   |
| reward_motion           | 9.26e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.8       |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 639        |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 75         |
|    time_elapsed         | 368        |
|    total_timesteps      | 76800      |
| train/                  |            |
|    approx_kl            | 0.08604793 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.8      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 810        |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.0809    |
|    std                  | 0.367      |
|    value_loss           | 433        |
----------------------------------------
----------------------------------------
| reward                  | -2.09      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.51e-05   |
| reward_motion           | 9.25e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.81      |
| reward_velocity         | 0.761      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 640        |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 76         |
|    time_elapsed         | 373        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.07724598 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.5      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 238        |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.0771    |
|    std                  | 0.367      |
|    value_loss           | 512        |
----------------------------------------
Num timesteps: 78000
Best mean reward: 791.71 - Last mean reward per episode: 640.18
----------------------------------------
| reward                  | -2.1       |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.5e-05    |
| reward_motion           | 9.24e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.81      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 632        |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 77         |
|    time_elapsed         | 377        |
|    total_timesteps      | 78848      |
| train/                  |            |
|    approx_kl            | 0.06728281 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.9      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | 156        |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.0718    |
|    std                  | 0.367      |
|    value_loss           | 617        |
----------------------------------------
-----------------------------------------
| reward                  | -2.09       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.46e-05    |
| reward_motion           | 9.16e-07    |
| reward_position         | 0.000137    |
| reward_torque           | -2.81       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 633         |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 78          |
|    time_elapsed         | 381         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.052631572 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.1       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 159         |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.0682     |
|    std                  | 0.367       |
|    value_loss           | 420         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.1        |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.65e-05    |
| reward_motion           | 9.47e-07    |
| reward_position         | 0.000142    |
| reward_torque           | -2.81       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 638         |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 79          |
|    time_elapsed         | 386         |
|    total_timesteps      | 80896       |
| train/                  |             |
|    approx_kl            | 0.091547176 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.7       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.4        |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.0856     |
|    std                  | 0.367       |
|    value_loss           | 432         |
-----------------------------------------
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.66e-05   |
| reward_motion           | 9.47e-07   |
| reward_position         | 0.000142   |
| reward_torque           | -2.82      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 636        |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 80         |
|    time_elapsed         | 390        |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.07504402 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.8      |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 103        |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.0823    |
|    std                  | 0.367      |
|    value_loss           | 492        |
----------------------------------------
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.62e-05   |
| reward_motion           | 9.39e-07   |
| reward_position         | 0.000141   |
| reward_torque           | -2.82      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 632        |
| time/                   |            |
|    fps                  | 210        |
|    iterations           | 81         |
|    time_elapsed         | 394        |
|    total_timesteps      | 82944      |
| train/                  |            |
|    approx_kl            | 0.05894314 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.7      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 60.3       |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.0641    |
|    std                  | 0.367      |
|    value_loss           | 404        |
----------------------------------------
-----------------------------------------
| reward                  | -2.1        |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4.58e-05    |
| reward_motion           | 9.34e-07    |
| reward_position         | 0.00014     |
| reward_torque           | -2.82       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 632         |
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 82          |
|    time_elapsed         | 399         |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.037166715 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.9       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 220         |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.0523     |
|    std                  | 0.367       |
|    value_loss           | 504         |
-----------------------------------------
Num timesteps: 84000
Best mean reward: 791.71 - Last mean reward per episode: 631.66
----------------------------------------
| reward                  | -2.11      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 4.55e-05   |
| reward_motion           | 9.28e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.82      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 628        |
| time/                   |            |
|    fps                  | 210        |
|    iterations           | 83         |
|    time_elapsed         | 403        |
|    total_timesteps      | 84992      |
| train/                  |            |
|    approx_kl            | 0.12127335 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.2      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 83.3       |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.0993    |
|    std                  | 0.367      |
|    value_loss           | 340        |
----------------------------------------
---------------------------------------
| reward                  | -2.1      |
| reward_contact          | -0.0471   |
| reward_ctrl             | 4.52e-05  |
| reward_motion           | 9.23e-07  |
| reward_position         | 0.000138  |
| reward_torque           | -2.82     |
| reward_velocity         | 0.761     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 634       |
| time/                   |           |
|    fps                  | 210       |
|    iterations           | 84        |
|    time_elapsed         | 408       |
|    total_timesteps      | 86016     |
| train/                  |           |
|    approx_kl            | 0.0711868 |
|    clip_fraction        | 0.16      |
|    clip_range           | 0.4       |
|    entropy_loss         | -15.5     |
|    explained_variance   | 0.974     |
|    learning_rate        | 0.0003    |
|    loss                 | 228       |
|    n_updates            | 1660      |
|    policy_gradient_loss | -0.0729   |
|    std                  | 0.367     |
|    value_loss           | 634       |
---------------------------------------
-----------------------------------------
| reward                  | -2.11       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.51e-05    |
| reward_motion           | 9.18e-07    |
| reward_position         | 0.000138    |
| reward_torque           | -2.83       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 631         |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 85          |
|    time_elapsed         | 412         |
|    total_timesteps      | 87040       |
| train/                  |             |
|    approx_kl            | 0.059059467 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.4         |
|    entropy_loss         | -15.9       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 472         |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.0646     |
|    std                  | 0.367       |
|    value_loss           | 605         |
-----------------------------------------
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.5e-05    |
| reward_motion           | 9.18e-07   |
| reward_position         | 0.000138   |
| reward_torque           | -2.84      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 637        |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 86         |
|    time_elapsed         | 416        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.10163675 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.9      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 487        |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.0836    |
|    std                  | 0.367      |
|    value_loss           | 497        |
----------------------------------------
-----------------------------------------
| reward                  | -2.13       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.47e-05    |
| reward_motion           | 9.15e-07    |
| reward_position         | 0.000137    |
| reward_torque           | -2.85       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 630         |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 87          |
|    time_elapsed         | 420         |
|    total_timesteps      | 89088       |
| train/                  |             |
|    approx_kl            | 0.051552266 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.9       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 138         |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.0379     |
|    std                  | 0.367       |
|    value_loss           | 485         |
-----------------------------------------
Num timesteps: 90000
Best mean reward: 791.71 - Last mean reward per episode: 630.24
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.46e-05   |
| reward_motion           | 9.1e-07    |
| reward_position         | 0.000136   |
| reward_torque           | -2.85      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 632        |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 88         |
|    time_elapsed         | 425        |
|    total_timesteps      | 90112      |
| train/                  |            |
|    approx_kl            | 0.04710511 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.5      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | 201        |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.0552    |
|    std                  | 0.367      |
|    value_loss           | 560        |
----------------------------------------
----------------------------------------
| reward                  | -2.13      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.44e-05   |
| reward_motion           | 9.08e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -2.85      |
| reward_velocity         | 0.761      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 625        |
| time/                   |            |
|    fps                  | 212        |
|    iterations           | 89         |
|    time_elapsed         | 429        |
|    total_timesteps      | 91136      |
| train/                  |            |
|    approx_kl            | 0.05163174 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.5      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 78         |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.0511    |
|    std                  | 0.367      |
|    value_loss           | 513        |
----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.44e-05    |
| reward_motion           | 9.09e-07    |
| reward_position         | 0.000136    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 621         |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 90          |
|    time_elapsed         | 434         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.062439263 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.1       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 524         |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.048      |
|    std                  | 0.367       |
|    value_loss           | 502         |
-----------------------------------------
----------------------------------------
| reward                  | -2.14      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.43e-05   |
| reward_motion           | 9.06e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -2.86      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 615        |
| time/                   |            |
|    fps                  | 212        |
|    iterations           | 91         |
|    time_elapsed         | 438        |
|    total_timesteps      | 93184      |
| train/                  |            |
|    approx_kl            | 0.07532377 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 149        |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.068     |
|    std                  | 0.367      |
|    value_loss           | 565        |
----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.42e-05    |
| reward_motion           | 9.06e-07    |
| reward_position         | 0.000136    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 611         |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 92          |
|    time_elapsed         | 442         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.038080342 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.7       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 95.1        |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.0483     |
|    std                  | 0.367       |
|    value_loss           | 469         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.38e-05    |
| reward_motion           | 8.99e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 612         |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 93          |
|    time_elapsed         | 447         |
|    total_timesteps      | 95232       |
| train/                  |             |
|    approx_kl            | 0.057437897 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.2       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 599         |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.0525     |
|    std                  | 0.367       |
|    value_loss           | 542         |
-----------------------------------------
Num timesteps: 96000
Best mean reward: 791.71 - Last mean reward per episode: 612.12
----------------------------------------
| reward                  | -2.14      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.37e-05   |
| reward_motion           | 8.96e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.85      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 607        |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 94         |
|    time_elapsed         | 451        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.06996028 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.9      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 107        |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.0799    |
|    std                  | 0.367      |
|    value_loss           | 526        |
----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.34e-05    |
| reward_motion           | 8.91e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -2.85       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 606         |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 95          |
|    time_elapsed         | 456         |
|    total_timesteps      | 97280       |
| train/                  |             |
|    approx_kl            | 0.056031484 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.5       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 217         |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.057      |
|    std                  | 0.367       |
|    value_loss           | 495         |
-----------------------------------------
----------------------------------------
| reward                  | -2.14      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.34e-05   |
| reward_motion           | 8.91e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.85      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 607        |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 96         |
|    time_elapsed         | 460        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.07183945 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 175        |
|    n_updates            | 1900       |
|    policy_gradient_loss | -0.0801    |
|    std                  | 0.367      |
|    value_loss           | 454        |
----------------------------------------
----------------------------------------
| reward                  | -2.15      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.36e-05   |
| reward_motion           | 8.97e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -2.86      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 603        |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 97         |
|    time_elapsed         | 464        |
|    total_timesteps      | 99328      |
| train/                  |            |
|    approx_kl            | 0.08660442 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -15.9      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 119        |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.0682    |
|    std                  | 0.367      |
|    value_loss           | 556        |
----------------------------------------
-----------------------------------------
| reward                  | -2.15       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.38e-05    |
| reward_motion           | 9.02e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 608         |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 98          |
|    time_elapsed         | 469         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.036415048 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.6       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 165         |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.0572     |
|    std                  | 0.367       |
|    value_loss           | 499         |
-----------------------------------------
---------------------------------------
| reward                  | -2.15     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.39e-05  |
| reward_motion           | 9.02e-07  |
| reward_position         | 0.000135  |
| reward_torque           | -2.87     |
| reward_velocity         | 0.762     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 606       |
| time/                   |           |
|    fps                  | 214       |
|    iterations           | 99        |
|    time_elapsed         | 473       |
|    total_timesteps      | 101376    |
| train/                  |           |
|    approx_kl            | 0.0719479 |
|    clip_fraction        | 0.14      |
|    clip_range           | 0.4       |
|    entropy_loss         | -16.8     |
|    explained_variance   | 0.974     |
|    learning_rate        | 0.0003    |
|    loss                 | 133       |
|    n_updates            | 1960      |
|    policy_gradient_loss | -0.0645   |
|    std                  | 0.367     |
|    value_loss           | 479       |
---------------------------------------
Num timesteps: 102000
Best mean reward: 791.71 - Last mean reward per episode: 605.58
-----------------------------------------
| reward                  | -2.15       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.36e-05    |
| reward_motion           | 8.98e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 602         |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 100         |
|    time_elapsed         | 477         |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.052123062 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.6       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 488         |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.0443     |
|    std                  | 0.367       |
|    value_loss           | 509         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.35e-05    |
| reward_motion           | 8.96e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 600         |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 101         |
|    time_elapsed         | 482         |
|    total_timesteps      | 103424      |
| train/                  |             |
|    approx_kl            | 0.060172085 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 173         |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.0666     |
|    std                  | 0.367       |
|    value_loss           | 449         |
-----------------------------------------
----------------------------------------
| reward                  | -2.14      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.34e-05   |
| reward_motion           | 8.99e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -2.86      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 599        |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 102        |
|    time_elapsed         | 485        |
|    total_timesteps      | 104448     |
| train/                  |            |
|    approx_kl            | 0.07174431 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.4      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 79.1       |
|    n_updates            | 2020       |
|    policy_gradient_loss | -0.0803    |
|    std                  | 0.367      |
|    value_loss           | 373        |
----------------------------------------
-----------------------------------------
| reward                  | -2.14       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.44e-05    |
| reward_motion           | 9.18e-07    |
| reward_position         | 0.000138    |
| reward_torque           | -2.86       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 595         |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 103         |
|    time_elapsed         | 489         |
|    total_timesteps      | 105472      |
| train/                  |             |
|    approx_kl            | 0.061719283 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.2       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 195         |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.0671     |
|    std                  | 0.366       |
|    value_loss           | 384         |
-----------------------------------------
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.46e-05   |
| reward_motion           | 9.2e-07    |
| reward_position         | 0.000138   |
| reward_torque           | -2.87      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 588        |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 104        |
|    time_elapsed         | 492        |
|    total_timesteps      | 106496     |
| train/                  |            |
|    approx_kl            | 0.09381586 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 106        |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.0782    |
|    std                  | 0.366      |
|    value_loss           | 346        |
----------------------------------------
-----------------------------------------
| reward                  | -2.16       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.48e-05    |
| reward_motion           | 9.24e-07    |
| reward_position         | 0.000139    |
| reward_torque           | -2.87       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 588         |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 105         |
|    time_elapsed         | 496         |
|    total_timesteps      | 107520      |
| train/                  |             |
|    approx_kl            | 0.056681145 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.5       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 169         |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.0553     |
|    std                  | 0.366       |
|    value_loss           | 358         |
-----------------------------------------
Num timesteps: 108000
Best mean reward: 791.71 - Last mean reward per episode: 587.83
----------------------------------------
| reward                  | -2.17      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.5e-05    |
| reward_motion           | 9.27e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.89      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 597        |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 106        |
|    time_elapsed         | 500        |
|    total_timesteps      | 108544     |
| train/                  |            |
|    approx_kl            | 0.06581798 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.6      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 83         |
|    n_updates            | 2100       |
|    policy_gradient_loss | -0.0689    |
|    std                  | 0.366      |
|    value_loss           | 276        |
----------------------------------------
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.5e-05    |
| reward_motion           | 9.28e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.88      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 596        |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 107        |
|    time_elapsed         | 504        |
|    total_timesteps      | 109568     |
| train/                  |            |
|    approx_kl            | 0.09682576 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.7      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 391        |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.0893    |
|    std                  | 0.366      |
|    value_loss           | 469        |
----------------------------------------
----------------------------------------
| reward                  | -2.15      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.49e-05   |
| reward_motion           | 9.27e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.87      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 585        |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 108        |
|    time_elapsed         | 508        |
|    total_timesteps      | 110592     |
| train/                  |            |
|    approx_kl            | 0.08974074 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.1      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 234        |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.0812    |
|    std                  | 0.366      |
|    value_loss           | 507        |
----------------------------------------
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.49e-05   |
| reward_motion           | 9.27e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.87      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 583        |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 109        |
|    time_elapsed         | 513        |
|    total_timesteps      | 111616     |
| train/                  |            |
|    approx_kl            | 0.07364488 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.4      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 216        |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.0762    |
|    std                  | 0.366      |
|    value_loss           | 431        |
----------------------------------------
-----------------------------------------
| reward                  | -2.15       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.5e-05     |
| reward_motion           | 9.3e-07     |
| reward_position         | 0.000139    |
| reward_torque           | -2.87       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 571         |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 110         |
|    time_elapsed         | 517         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.060291555 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 211         |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.0624     |
|    std                  | 0.366       |
|    value_loss           | 496         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.16       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.48e-05    |
| reward_motion           | 9.26e-07    |
| reward_position         | 0.000139    |
| reward_torque           | -2.87       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 560         |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 111         |
|    time_elapsed         | 521         |
|    total_timesteps      | 113664      |
| train/                  |             |
|    approx_kl            | 0.054138087 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 133         |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.0621     |
|    std                  | 0.366       |
|    value_loss           | 498         |
-----------------------------------------
Num timesteps: 114000
Best mean reward: 791.71 - Last mean reward per episode: 560.01
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.48e-05   |
| reward_motion           | 9.27e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -2.87      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 560        |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 112        |
|    time_elapsed         | 526        |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.06798768 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17        |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 119        |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.0686    |
|    std                  | 0.366      |
|    value_loss           | 359        |
----------------------------------------
-----------------------------------------
| reward                  | -2.16       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.39e-05    |
| reward_motion           | 9.09e-07    |
| reward_position         | 0.000136    |
| reward_torque           | -2.88       |
| reward_velocity         | 0.758       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 546         |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 113         |
|    time_elapsed         | 530         |
|    total_timesteps      | 115712      |
| train/                  |             |
|    approx_kl            | 0.046133902 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.1       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 431         |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.0542     |
|    std                  | 0.366       |
|    value_loss           | 599         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.18       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.38e-05    |
| reward_motion           | 9.08e-07    |
| reward_position         | 0.000136    |
| reward_torque           | -2.89       |
| reward_velocity         | 0.757       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 537         |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 114         |
|    time_elapsed         | 534         |
|    total_timesteps      | 116736      |
| train/                  |             |
|    approx_kl            | 0.068029776 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.2       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 134         |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.0635     |
|    std                  | 0.366       |
|    value_loss           | 456         |
-----------------------------------------
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 9.11e-07   |
| reward_position         | 0.000137   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.756      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 531        |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 115        |
|    time_elapsed         | 539        |
|    total_timesteps      | 117760     |
| train/                  |            |
|    approx_kl            | 0.09138459 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.9      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 140        |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.0855    |
|    std                  | 0.366      |
|    value_loss           | 305        |
----------------------------------------
-----------------------------------------
| reward                  | -2.19       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.33e-05    |
| reward_motion           | 8.97e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -2.9        |
| reward_velocity         | 0.754       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 522         |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 116         |
|    time_elapsed         | 542         |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.084602855 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.5       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 170         |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.0706     |
|    std                  | 0.366       |
|    value_loss           | 458         |
-----------------------------------------
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.3e-05    |
| reward_motion           | 8.93e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.752      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 517        |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 117        |
|    time_elapsed         | 546        |
|    total_timesteps      | 119808     |
| train/                  |            |
|    approx_kl            | 0.07564285 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.5      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 410        |
|    n_updates            | 2320       |
|    policy_gradient_loss | -0.0645    |
|    std                  | 0.366      |
|    value_loss           | 468        |
----------------------------------------
Num timesteps: 120000
Best mean reward: 791.71 - Last mean reward per episode: 516.74
-----------------------------------------
| reward                  | -2.21       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.23e-05    |
| reward_motion           | 8.83e-07    |
| reward_position         | 0.000132    |
| reward_torque           | -2.91       |
| reward_velocity         | 0.751       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 505         |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 118         |
|    time_elapsed         | 549         |
|    total_timesteps      | 120832      |
| train/                  |             |
|    approx_kl            | 0.093632296 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 74.1        |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.0891     |
|    std                  | 0.366       |
|    value_loss           | 297         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.21       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.19e-05    |
| reward_motion           | 8.75e-07    |
| reward_position         | 0.000131    |
| reward_torque           | -2.91       |
| reward_velocity         | 0.749       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 495         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 119         |
|    time_elapsed         | 553         |
|    total_timesteps      | 121856      |
| train/                  |             |
|    approx_kl            | 0.042702064 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 156         |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.0479     |
|    std                  | 0.366       |
|    value_loss           | 528         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.22       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.17e-05    |
| reward_motion           | 8.74e-07    |
| reward_position         | 0.000131    |
| reward_torque           | -2.92       |
| reward_velocity         | 0.749       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 496         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 120         |
|    time_elapsed         | 557         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.053215243 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.2       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 384         |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.0579     |
|    std                  | 0.366       |
|    value_loss           | 416         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.22       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.18e-05    |
| reward_motion           | 8.77e-07    |
| reward_position         | 0.000132    |
| reward_torque           | -2.92       |
| reward_velocity         | 0.75        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 491         |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 121         |
|    time_elapsed         | 561         |
|    total_timesteps      | 123904      |
| train/                  |             |
|    approx_kl            | 0.079917565 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.1       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 97.4        |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.0821     |
|    std                  | 0.366       |
|    value_loss           | 598         |
-----------------------------------------
----------------------------------------
| reward                  | -2.22      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.21e-05   |
| reward_motion           | 8.84e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.75       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 485        |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 122        |
|    time_elapsed         | 565        |
|    total_timesteps      | 124928     |
| train/                  |            |
|    approx_kl            | 0.09118663 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.1      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 265        |
|    n_updates            | 2420       |
|    policy_gradient_loss | -0.069     |
|    std                  | 0.366      |
|    value_loss           | 445        |
----------------------------------------
----------------------------------------
| reward                  | -2.21      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.24e-05   |
| reward_motion           | 8.85e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.75       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 477        |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 123        |
|    time_elapsed         | 570        |
|    total_timesteps      | 125952     |
| train/                  |            |
|    approx_kl            | 0.03758951 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.9      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 155        |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.0571    |
|    std                  | 0.366      |
|    value_loss           | 473        |
----------------------------------------
Num timesteps: 126000
Best mean reward: 791.71 - Last mean reward per episode: 476.83
---------------------------------------
| reward                  | -2.21     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 4.22e-05  |
| reward_motion           | 8.79e-07  |
| reward_position         | 0.000132  |
| reward_torque           | -2.91     |
| reward_velocity         | 0.748     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 478       |
| time/                   |           |
|    fps                  | 220       |
|    iterations           | 124       |
|    time_elapsed         | 574       |
|    total_timesteps      | 126976    |
| train/                  |           |
|    approx_kl            | 0.0718819 |
|    clip_fraction        | 0.186     |
|    clip_range           | 0.4       |
|    entropy_loss         | -17.4     |
|    explained_variance   | 0.983     |
|    learning_rate        | 0.0003    |
|    loss                 | 352       |
|    n_updates            | 2460      |
|    policy_gradient_loss | -0.069    |
|    std                  | 0.366     |
|    value_loss           | 473       |
---------------------------------------
----------------------------------------
| reward                  | -2.21      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.28e-05   |
| reward_motion           | 8.95e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.751      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 475        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 125        |
|    time_elapsed         | 579        |
|    total_timesteps      | 128000     |
| train/                  |            |
|    approx_kl            | 0.05870478 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.1      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 343        |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.0582    |
|    std                  | 0.366      |
|    value_loss           | 510        |
----------------------------------------
----------------------------------------
| reward                  | -2.22      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.28e-05   |
| reward_motion           | 8.96e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.93      |
| reward_velocity         | 0.752      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 467        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 126        |
|    time_elapsed         | 583        |
|    total_timesteps      | 129024     |
| train/                  |            |
|    approx_kl            | 0.09303458 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 149        |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.0826    |
|    std                  | 0.366      |
|    value_loss           | 333        |
----------------------------------------
-----------------------------------------
| reward                  | -2.22       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.27e-05    |
| reward_motion           | 8.94e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -2.93       |
| reward_velocity         | 0.751       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 457         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 127         |
|    time_elapsed         | 588         |
|    total_timesteps      | 130048      |
| train/                  |             |
|    approx_kl            | 0.075280145 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.3       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 2520        |
|    policy_gradient_loss | -0.064      |
|    std                  | 0.366       |
|    value_loss           | 423         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.21       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.25e-05    |
| reward_motion           | 8.91e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -2.91       |
| reward_velocity         | 0.752       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 466         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 128         |
|    time_elapsed         | 592         |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.089382544 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.4         |
|    entropy_loss         | -16.8       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 162         |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.0754     |
|    std                  | 0.366       |
|    value_loss           | 334         |
-----------------------------------------
Num timesteps: 132000
Best mean reward: 791.71 - Last mean reward per episode: 466.43
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.33e-05   |
| reward_motion           | 9.05e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.753      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 469        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 129        |
|    time_elapsed         | 596        |
|    total_timesteps      | 132096     |
| train/                  |            |
|    approx_kl            | 0.07682991 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.7      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 221        |
|    n_updates            | 2560       |
|    policy_gradient_loss | -0.0739    |
|    std                  | 0.366      |
|    value_loss           | 405        |
----------------------------------------
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.18e-05   |
| reward_motion           | 8.83e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.756      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 473        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 130        |
|    time_elapsed         | 601        |
|    total_timesteps      | 133120     |
| train/                  |            |
|    approx_kl            | 0.07019861 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 169        |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.067     |
|    std                  | 0.366      |
|    value_loss           | 317        |
----------------------------------------
----------------------------------------
| reward                  | -2.18      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.2e-05    |
| reward_motion           | 8.86e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -2.89      |
| reward_velocity         | 0.756      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 464        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 131        |
|    time_elapsed         | 605        |
|    total_timesteps      | 134144     |
| train/                  |            |
|    approx_kl            | 0.07635625 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.7      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 134        |
|    n_updates            | 2600       |
|    policy_gradient_loss | -0.0589    |
|    std                  | 0.366      |
|    value_loss           | 448        |
----------------------------------------
----------------------------------------
| reward                  | -2.17      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.22e-05   |
| reward_motion           | 8.86e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -2.88      |
| reward_velocity         | 0.757      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 452        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 132        |
|    time_elapsed         | 610        |
|    total_timesteps      | 135168     |
| train/                  |            |
|    approx_kl            | 0.06735976 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.4        |
|    entropy_loss         | -17        |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 92.3       |
|    n_updates            | 2620       |
|    policy_gradient_loss | -0.0696    |
|    std                  | 0.366      |
|    value_loss           | 360        |
----------------------------------------
----------------------------------------
| reward                  | -2.16      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.24e-05   |
| reward_motion           | 8.88e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -2.88      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 448        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 133        |
|    time_elapsed         | 614        |
|    total_timesteps      | 136192     |
| train/                  |            |
|    approx_kl            | 0.07325928 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.4      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 142        |
|    n_updates            | 2640       |
|    policy_gradient_loss | -0.0698    |
|    std                  | 0.366      |
|    value_loss           | 398        |
----------------------------------------
----------------------------------------
| reward                  | -2.17      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.18e-05   |
| reward_motion           | 8.8e-07    |
| reward_position         | 0.000132   |
| reward_torque           | -2.88      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 443        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 134        |
|    time_elapsed         | 618        |
|    total_timesteps      | 137216     |
| train/                  |            |
|    approx_kl            | 0.07183257 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.2      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 311        |
|    n_updates            | 2660       |
|    policy_gradient_loss | -0.0723    |
|    std                  | 0.366      |
|    value_loss           | 445        |
----------------------------------------
Num timesteps: 138000
Best mean reward: 791.71 - Last mean reward per episode: 443.06
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.26e-05   |
| reward_motion           | 8.96e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 444        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 135        |
|    time_elapsed         | 623        |
|    total_timesteps      | 138240     |
| train/                  |            |
|    approx_kl            | 0.09175141 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 168        |
|    n_updates            | 2680       |
|    policy_gradient_loss | -0.0898    |
|    std                  | 0.366      |
|    value_loss           | 425        |
----------------------------------------
---------------------------------------
| reward                  | -2.18     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 4.26e-05  |
| reward_motion           | 8.96e-07  |
| reward_position         | 0.000134  |
| reward_torque           | -2.89     |
| reward_velocity         | 0.763     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 445       |
| time/                   |           |
|    fps                  | 222       |
|    iterations           | 136       |
|    time_elapsed         | 626       |
|    total_timesteps      | 139264    |
| train/                  |           |
|    approx_kl            | 0.0662691 |
|    clip_fraction        | 0.21      |
|    clip_range           | 0.4       |
|    entropy_loss         | -18.1     |
|    explained_variance   | 0.974     |
|    learning_rate        | 0.0003    |
|    loss                 | 152       |
|    n_updates            | 2700      |
|    policy_gradient_loss | -0.0719   |
|    std                  | 0.366     |
|    value_loss           | 433       |
---------------------------------------
----------------------------------------
| reward                  | -2.2       |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.98e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 434        |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 137        |
|    time_elapsed         | 630        |
|    total_timesteps      | 140288     |
| train/                  |            |
|    approx_kl            | 0.05629784 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.3      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 57.8       |
|    n_updates            | 2720       |
|    policy_gradient_loss | -0.073     |
|    std                  | 0.366      |
|    value_loss           | 389        |
----------------------------------------
-----------------------------------------
| reward                  | -2.2        |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.13e-05    |
| reward_motion           | 8.71e-07    |
| reward_position         | 0.000131    |
| reward_torque           | -2.91       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 430         |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 138         |
|    time_elapsed         | 633         |
|    total_timesteps      | 141312      |
| train/                  |             |
|    approx_kl            | 0.063325554 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.3       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 362         |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.0723     |
|    std                  | 0.366       |
|    value_loss           | 416         |
-----------------------------------------
---------------------------------------
| reward                  | -2.19     |
| reward_contact          | -0.047    |
| reward_ctrl             | 3.98e-05  |
| reward_motion           | 8.44e-07  |
| reward_position         | 0.000127  |
| reward_torque           | -2.91     |
| reward_velocity         | 0.763     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 424       |
| time/                   |           |
|    fps                  | 223       |
|    iterations           | 139       |
|    time_elapsed         | 636       |
|    total_timesteps      | 142336    |
| train/                  |           |
|    approx_kl            | 0.1082619 |
|    clip_fraction        | 0.281     |
|    clip_range           | 0.4       |
|    entropy_loss         | -17.4     |
|    explained_variance   | 0.981     |
|    learning_rate        | 0.0003    |
|    loss                 | 39.6      |
|    n_updates            | 2760      |
|    policy_gradient_loss | -0.104    |
|    std                  | 0.366     |
|    value_loss           | 306       |
---------------------------------------
----------------------------------------
| reward                  | -2.2       |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.43e-07   |
| reward_position         | 0.000126   |
| reward_torque           | -2.91      |
| reward_velocity         | 0.761      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 424        |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 140        |
|    time_elapsed         | 640        |
|    total_timesteps      | 143360     |
| train/                  |            |
|    approx_kl            | 0.07125464 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.4      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 132        |
|    n_updates            | 2780       |
|    policy_gradient_loss | -0.0617    |
|    std                  | 0.366      |
|    value_loss           | 340        |
----------------------------------------
Num timesteps: 144000
Best mean reward: 791.71 - Last mean reward per episode: 424.38
----------------------------------------
| reward                  | -2.18      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.41e-07   |
| reward_position         | 0.000126   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 420        |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 141        |
|    time_elapsed         | 644        |
|    total_timesteps      | 144384     |
| train/                  |            |
|    approx_kl            | 0.08280164 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.5      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 347        |
|    n_updates            | 2800       |
|    policy_gradient_loss | -0.0521    |
|    std                  | 0.366      |
|    value_loss           | 378        |
----------------------------------------
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.99e-05   |
| reward_motion           | 8.47e-07   |
| reward_position         | 0.000127   |
| reward_torque           | -2.9       |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 415        |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 142        |
|    time_elapsed         | 649        |
|    total_timesteps      | 145408     |
| train/                  |            |
|    approx_kl            | 0.09441495 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.8      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 56.9       |
|    n_updates            | 2820       |
|    policy_gradient_loss | -0.0794    |
|    std                  | 0.366      |
|    value_loss           | 307        |
----------------------------------------
-----------------------------------------
| reward                  | -2.19       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.01e-05    |
| reward_motion           | 8.49e-07    |
| reward_position         | 0.000127    |
| reward_torque           | -2.91       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 410         |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 143         |
|    time_elapsed         | 654         |
|    total_timesteps      | 146432      |
| train/                  |             |
|    approx_kl            | 0.084700994 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.9       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 189         |
|    n_updates            | 2840        |
|    policy_gradient_loss | -0.0941     |
|    std                  | 0.366       |
|    value_loss           | 435         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.21       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.02e-05    |
| reward_motion           | 8.51e-07    |
| reward_position         | 0.000128    |
| reward_torque           | -2.92       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 410         |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 144         |
|    time_elapsed         | 658         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.078906074 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.4       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | 143         |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.0745     |
|    std                  | 0.366       |
|    value_loss           | 474         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.2        |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.93e-05    |
| reward_motion           | 8.34e-07    |
| reward_position         | 0.000125    |
| reward_torque           | -2.92       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 407         |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 145         |
|    time_elapsed         | 663         |
|    total_timesteps      | 148480      |
| train/                  |             |
|    approx_kl            | 0.079785354 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.8       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 135         |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.0749     |
|    std                  | 0.366       |
|    value_loss           | 373         |
-----------------------------------------
----------------------------------------
| reward                  | -2.21      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.91e-05   |
| reward_motion           | 8.31e-07   |
| reward_position         | 0.000125   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 399        |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 146        |
|    time_elapsed         | 667        |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.06602247 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.9      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 511        |
|    n_updates            | 2900       |
|    policy_gradient_loss | -0.079     |
|    std                  | 0.366      |
|    value_loss           | 481        |
----------------------------------------
Num timesteps: 150000
Best mean reward: 791.71 - Last mean reward per episode: 399.17
-----------------------------------------
| reward                  | -2.21       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.75e-05    |
| reward_motion           | 8.05e-07    |
| reward_position         | 0.000121    |
| reward_torque           | -2.93       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 394         |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 147         |
|    time_elapsed         | 672         |
|    total_timesteps      | 150528      |
| train/                  |             |
|    approx_kl            | 0.074376225 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.7       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 84.2        |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.0707     |
|    std                  | 0.366       |
|    value_loss           | 408         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.22       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.68e-05    |
| reward_motion           | 7.84e-07    |
| reward_position         | 0.000118    |
| reward_torque           | -2.93       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 393         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 148         |
|    time_elapsed         | 676         |
|    total_timesteps      | 151552      |
| train/                  |             |
|    approx_kl            | 0.064057305 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.1       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 210         |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.0699     |
|    std                  | 0.366       |
|    value_loss           | 445         |
-----------------------------------------
---------------------------------------
| reward                  | -2.22     |
| reward_contact          | -0.047    |
| reward_ctrl             | 3.67e-05  |
| reward_motion           | 7.81e-07  |
| reward_position         | 0.000117  |
| reward_torque           | -2.94     |
| reward_velocity         | 0.762     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 393       |
| time/                   |           |
|    fps                  | 224       |
|    iterations           | 149       |
|    time_elapsed         | 680       |
|    total_timesteps      | 152576    |
| train/                  |           |
|    approx_kl            | 0.0948907 |
|    clip_fraction        | 0.218     |
|    clip_range           | 0.4       |
|    entropy_loss         | -18.4     |
|    explained_variance   | 0.976     |
|    learning_rate        | 0.0003    |
|    loss                 | 112       |
|    n_updates            | 2960      |
|    policy_gradient_loss | -0.0826   |
|    std                  | 0.365     |
|    value_loss           | 441       |
---------------------------------------
-----------------------------------------
| reward                  | -2.25       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.61e-05    |
| reward_motion           | 7.67e-07    |
| reward_position         | 0.000115    |
| reward_torque           | -2.96       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 390         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 150         |
|    time_elapsed         | 685         |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.051997818 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.2       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 146         |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.0506     |
|    std                  | 0.365       |
|    value_loss           | 399         |
-----------------------------------------
----------------------------------------
| reward                  | -2.26      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.63e-05   |
| reward_motion           | 7.71e-07   |
| reward_position         | 0.000116   |
| reward_torque           | -2.98      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 398        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 151        |
|    time_elapsed         | 689        |
|    total_timesteps      | 154624     |
| train/                  |            |
|    approx_kl            | 0.08013227 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.8      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 206        |
|    n_updates            | 3000       |
|    policy_gradient_loss | -0.068     |
|    std                  | 0.365      |
|    value_loss           | 373        |
----------------------------------------
----------------------------------------
| reward                  | -2.28      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.7e-05    |
| reward_motion           | 7.83e-07   |
| reward_position         | 0.000118   |
| reward_torque           | -2.99      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 394        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 152        |
|    time_elapsed         | 693        |
|    total_timesteps      | 155648     |
| train/                  |            |
|    approx_kl            | 0.05597806 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.6      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 65         |
|    n_updates            | 3020       |
|    policy_gradient_loss | -0.0661    |
|    std                  | 0.365      |
|    value_loss           | 399        |
----------------------------------------
Num timesteps: 156000
Best mean reward: 791.71 - Last mean reward per episode: 393.96
----------------------------------------
| reward                  | -2.27      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.69e-05   |
| reward_motion           | 7.82e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -2.99      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 386        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 153        |
|    time_elapsed         | 698        |
|    total_timesteps      | 156672     |
| train/                  |            |
|    approx_kl            | 0.06945801 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.7      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 203        |
|    n_updates            | 3040       |
|    policy_gradient_loss | -0.0745    |
|    std                  | 0.365      |
|    value_loss           | 537        |
----------------------------------------
-----------------------------------------
| reward                  | -2.28       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.67e-05    |
| reward_motion           | 7.78e-07    |
| reward_position         | 0.000117    |
| reward_torque           | -3          |
| reward_velocity         | 0.765       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 378         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 154         |
|    time_elapsed         | 702         |
|    total_timesteps      | 157696      |
| train/                  |             |
|    approx_kl            | 0.075821295 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.7       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 79          |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.0732     |
|    std                  | 0.365       |
|    value_loss           | 319         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.3        |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.66e-05    |
| reward_motion           | 7.8e-07     |
| reward_position         | 0.000117    |
| reward_torque           | -3.01       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 372         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 155         |
|    time_elapsed         | 707         |
|    total_timesteps      | 158720      |
| train/                  |             |
|    approx_kl            | 0.065256454 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.5       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 81.4        |
|    n_updates            | 3080        |
|    policy_gradient_loss | -0.066      |
|    std                  | 0.365       |
|    value_loss           | 472         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.3        |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.61e-05    |
| reward_motion           | 7.66e-07    |
| reward_position         | 0.000115    |
| reward_torque           | -3.01       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 364         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 156         |
|    time_elapsed         | 711         |
|    total_timesteps      | 159744      |
| train/                  |             |
|    approx_kl            | 0.059843626 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.3       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 139         |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.0743     |
|    std                  | 0.365       |
|    value_loss           | 415         |
-----------------------------------------
----------------------------------------
| reward                  | -2.3       |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.58e-05   |
| reward_motion           | 7.62e-07   |
| reward_position         | 0.000114   |
| reward_torque           | -3.02      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 366        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 157        |
|    time_elapsed         | 716        |
|    total_timesteps      | 160768     |
| train/                  |            |
|    approx_kl            | 0.09143843 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.3      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 94.6       |
|    n_updates            | 3120       |
|    policy_gradient_loss | -0.0821    |
|    std                  | 0.365      |
|    value_loss           | 311        |
----------------------------------------
----------------------------------------
| reward                  | -2.32      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.57e-05   |
| reward_motion           | 7.61e-07   |
| reward_position         | 0.000114   |
| reward_torque           | -3.03      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 366        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 158        |
|    time_elapsed         | 720        |
|    total_timesteps      | 161792     |
| train/                  |            |
|    approx_kl            | 0.09160215 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.6      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 128        |
|    n_updates            | 3140       |
|    policy_gradient_loss | -0.0625    |
|    std                  | 0.365      |
|    value_loss           | 399        |
----------------------------------------
Num timesteps: 162000
Best mean reward: 791.71 - Last mean reward per episode: 365.98
-----------------------------------------
| reward                  | -2.32       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.58e-05    |
| reward_motion           | 7.65e-07    |
| reward_position         | 0.000115    |
| reward_torque           | -3.04       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 357         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 159         |
|    time_elapsed         | 725         |
|    total_timesteps      | 162816      |
| train/                  |             |
|    approx_kl            | 0.071382515 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.4         |
|    entropy_loss         | -18         |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 415         |
|    n_updates            | 3160        |
|    policy_gradient_loss | -0.0628     |
|    std                  | 0.365       |
|    value_loss           | 609         |
-----------------------------------------
----------------------------------------
| reward                  | -2.32      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.53e-05   |
| reward_motion           | 7.58e-07   |
| reward_position         | 0.000114   |
| reward_torque           | -3.04      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 350        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 160        |
|    time_elapsed         | 729        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.07973179 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.9      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 217        |
|    n_updates            | 3180       |
|    policy_gradient_loss | -0.0734    |
|    std                  | 0.365      |
|    value_loss           | 483        |
----------------------------------------
-----------------------------------------
| reward                  | -2.34       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.56e-05    |
| reward_motion           | 7.64e-07    |
| reward_position         | 0.000115    |
| reward_torque           | -3.05       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 345         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 161         |
|    time_elapsed         | 734         |
|    total_timesteps      | 164864      |
| train/                  |             |
|    approx_kl            | 0.062193085 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.3       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 70.8        |
|    n_updates            | 3200        |
|    policy_gradient_loss | -0.0636     |
|    std                  | 0.365       |
|    value_loss           | 465         |
-----------------------------------------
----------------------------------------
| reward                  | -2.35      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.58e-05   |
| reward_motion           | 7.67e-07   |
| reward_position         | 0.000115   |
| reward_torque           | -3.06      |
| reward_velocity         | 0.758      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 344        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 162        |
|    time_elapsed         | 738        |
|    total_timesteps      | 165888     |
| train/                  |            |
|    approx_kl            | 0.08855179 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18        |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 277        |
|    n_updates            | 3220       |
|    policy_gradient_loss | -0.0766    |
|    std                  | 0.365      |
|    value_loss           | 418        |
----------------------------------------
-----------------------------------------
| reward                  | -2.36       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.56e-05    |
| reward_motion           | 7.62e-07    |
| reward_position         | 0.000114    |
| reward_torque           | -3.07       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 342         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 163         |
|    time_elapsed         | 742         |
|    total_timesteps      | 166912      |
| train/                  |             |
|    approx_kl            | 0.090915054 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.5       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 447         |
|    n_updates            | 3240        |
|    policy_gradient_loss | -0.0813     |
|    std                  | 0.365       |
|    value_loss           | 441         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.36       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.53e-05    |
| reward_motion           | 7.55e-07    |
| reward_position         | 0.000113    |
| reward_torque           | -3.07       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 345         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 164         |
|    time_elapsed         | 746         |
|    total_timesteps      | 167936      |
| train/                  |             |
|    approx_kl            | 0.056085087 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.2       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | 256         |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.0711     |
|    std                  | 0.365       |
|    value_loss           | 590         |
-----------------------------------------
Num timesteps: 168000
Best mean reward: 791.71 - Last mean reward per episode: 345.20
----------------------------------------
| reward                  | -2.36      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.54e-05   |
| reward_motion           | 7.55e-07   |
| reward_position         | 0.000113   |
| reward_torque           | -3.07      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 347        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 165        |
|    time_elapsed         | 751        |
|    total_timesteps      | 168960     |
| train/                  |            |
|    approx_kl            | 0.07051596 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.7      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 58.4       |
|    n_updates            | 3280       |
|    policy_gradient_loss | -0.0793    |
|    std                  | 0.365      |
|    value_loss           | 540        |
----------------------------------------
----------------------------------------
| reward                  | -2.36      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.69e-05   |
| reward_motion           | 7.77e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -3.07      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 335        |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 166        |
|    time_elapsed         | 755        |
|    total_timesteps      | 169984     |
| train/                  |            |
|    approx_kl            | 0.07950926 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.3      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 60.2       |
|    n_updates            | 3300       |
|    policy_gradient_loss | -0.0713    |
|    std                  | 0.365      |
|    value_loss           | 432        |
----------------------------------------
----------------------------------------
| reward                  | -2.36      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.7e-05    |
| reward_motion           | 7.82e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -3.07      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 328        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 167        |
|    time_elapsed         | 759        |
|    total_timesteps      | 171008     |
| train/                  |            |
|    approx_kl            | 0.08645288 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.7      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 350        |
|    n_updates            | 3320       |
|    policy_gradient_loss | -0.0779    |
|    std                  | 0.365      |
|    value_loss           | 459        |
----------------------------------------
-----------------------------------------
| reward                  | -2.36       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.71e-05    |
| reward_motion           | 7.87e-07    |
| reward_position         | 0.000118    |
| reward_torque           | -3.08       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 326         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 168         |
|    time_elapsed         | 764         |
|    total_timesteps      | 172032      |
| train/                  |             |
|    approx_kl            | 0.062871024 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.7       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 406         |
|    n_updates            | 3340        |
|    policy_gradient_loss | -0.0655     |
|    std                  | 0.365       |
|    value_loss           | 491         |
-----------------------------------------
----------------------------------------
| reward                  | -2.37      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.72e-05   |
| reward_motion           | 7.9e-07    |
| reward_position         | 0.000119   |
| reward_torque           | -3.08      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 318        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 169        |
|    time_elapsed         | 768        |
|    total_timesteps      | 173056     |
| train/                  |            |
|    approx_kl            | 0.07585441 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.6      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 227        |
|    n_updates            | 3360       |
|    policy_gradient_loss | -0.0525    |
|    std                  | 0.365      |
|    value_loss           | 359        |
----------------------------------------
Num timesteps: 174000
Best mean reward: 791.71 - Last mean reward per episode: 318.08
----------------------------------------
| reward                  | -2.38      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.61e-05   |
| reward_motion           | 7.78e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.761      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 318        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 170        |
|    time_elapsed         | 773        |
|    total_timesteps      | 174080     |
| train/                  |            |
|    approx_kl            | 0.06525008 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.7      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | 156        |
|    n_updates            | 3380       |
|    policy_gradient_loss | -0.0433    |
|    std                  | 0.365      |
|    value_loss           | 378        |
----------------------------------------
---------------------------------------
| reward                  | -2.37     |
| reward_contact          | -0.047    |
| reward_ctrl             | 3.69e-05  |
| reward_motion           | 7.89e-07  |
| reward_position         | 0.000118  |
| reward_torque           | -3.09     |
| reward_velocity         | 0.761     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 320       |
| time/                   |           |
|    fps                  | 225       |
|    iterations           | 171       |
|    time_elapsed         | 777       |
|    total_timesteps      | 175104    |
| train/                  |           |
|    approx_kl            | 0.0965157 |
|    clip_fraction        | 0.265     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.3     |
|    explained_variance   | 0.981     |
|    learning_rate        | 0.0003    |
|    loss                 | 290       |
|    n_updates            | 3400      |
|    policy_gradient_loss | -0.0908   |
|    std                  | 0.365     |
|    value_loss           | 494       |
---------------------------------------
----------------------------------------
| reward                  | -2.38      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.72e-05   |
| reward_motion           | 7.95e-07   |
| reward_position         | 0.000119   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 320        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 172        |
|    time_elapsed         | 781        |
|    total_timesteps      | 176128     |
| train/                  |            |
|    approx_kl            | 0.06046278 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.9      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 172        |
|    n_updates            | 3420       |
|    policy_gradient_loss | -0.0625    |
|    std                  | 0.365      |
|    value_loss           | 522        |
----------------------------------------
-----------------------------------------
| reward                  | -2.38       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.74e-05    |
| reward_motion           | 7.98e-07    |
| reward_position         | 0.00012     |
| reward_torque           | -3.1        |
| reward_velocity         | 0.763       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 317         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 173         |
|    time_elapsed         | 786         |
|    total_timesteps      | 177152      |
| train/                  |             |
|    approx_kl            | 0.076122925 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.1       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 284         |
|    n_updates            | 3440        |
|    policy_gradient_loss | -0.0637     |
|    std                  | 0.365       |
|    value_loss           | 425         |
-----------------------------------------
----------------------------------------
| reward                  | -2.38      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.78e-05   |
| reward_motion           | 8.02e-07   |
| reward_position         | 0.00012    |
| reward_torque           | -3.09      |
| reward_velocity         | 0.762      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 317        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 174        |
|    time_elapsed         | 790        |
|    total_timesteps      | 178176     |
| train/                  |            |
|    approx_kl            | 0.08470478 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 47.4       |
|    n_updates            | 3460       |
|    policy_gradient_loss | -0.0741    |
|    std                  | 0.365      |
|    value_loss           | 270        |
----------------------------------------
-----------------------------------------
| reward                  | -2.4        |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.77e-05    |
| reward_motion           | 8.02e-07    |
| reward_position         | 0.00012     |
| reward_torque           | -3.12       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 310         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 175         |
|    time_elapsed         | 795         |
|    total_timesteps      | 179200      |
| train/                  |             |
|    approx_kl            | 0.071893886 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 3480        |
|    policy_gradient_loss | -0.0697     |
|    std                  | 0.365       |
|    value_loss           | 433         |
-----------------------------------------
Num timesteps: 180000
Best mean reward: 791.71 - Last mean reward per episode: 310.27
----------------------------------------
| reward                  | -2.41      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.75e-05   |
| reward_motion           | 7.97e-07   |
| reward_position         | 0.00012    |
| reward_torque           | -3.12      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 300        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 176        |
|    time_elapsed         | 799        |
|    total_timesteps      | 180224     |
| train/                  |            |
|    approx_kl            | 0.08430763 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.7      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 346        |
|    n_updates            | 3500       |
|    policy_gradient_loss | -0.0841    |
|    std                  | 0.365      |
|    value_loss           | 462        |
----------------------------------------
-----------------------------------------
| reward                  | -2.41       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.73e-05    |
| reward_motion           | 7.94e-07    |
| reward_position         | 0.000119    |
| reward_torque           | -3.13       |
| reward_velocity         | 0.762       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 299         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 177         |
|    time_elapsed         | 803         |
|    total_timesteps      | 181248      |
| train/                  |             |
|    approx_kl            | 0.082156904 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.1       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 88.1        |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.0705     |
|    std                  | 0.365       |
|    value_loss           | 398         |
-----------------------------------------
----------------------------------------
| reward                  | -2.43      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.73e-05   |
| reward_motion           | 7.96e-07   |
| reward_position         | 0.000119   |
| reward_torque           | -3.14      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 291        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 178        |
|    time_elapsed         | 808        |
|    total_timesteps      | 182272     |
| train/                  |            |
|    approx_kl            | 0.07415432 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.3      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 173        |
|    n_updates            | 3540       |
|    policy_gradient_loss | -0.066     |
|    std                  | 0.365      |
|    value_loss           | 450        |
----------------------------------------
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.62e-05   |
| reward_motion           | 7.75e-07   |
| reward_position         | 0.000116   |
| reward_torque           | -3.15      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 283        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 179        |
|    time_elapsed         | 812        |
|    total_timesteps      | 183296     |
| train/                  |            |
|    approx_kl            | 0.08652607 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.7      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 188        |
|    n_updates            | 3560       |
|    policy_gradient_loss | -0.0736    |
|    std                  | 0.365      |
|    value_loss           | 445        |
----------------------------------------
----------------------------------------
| reward                  | -2.43      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.72e-05   |
| reward_motion           | 7.92e-07   |
| reward_position         | 0.000119   |
| reward_torque           | -3.14      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 284        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 180        |
|    time_elapsed         | 816        |
|    total_timesteps      | 184320     |
| train/                  |            |
|    approx_kl            | 0.09836302 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 271        |
|    n_updates            | 3580       |
|    policy_gradient_loss | -0.0782    |
|    std                  | 0.365      |
|    value_loss           | 437        |
----------------------------------------
----------------------------------------
| reward                  | -2.43      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.83e-05   |
| reward_motion           | 8.1e-07    |
| reward_position         | 0.000122   |
| reward_torque           | -3.14      |
| reward_velocity         | 0.758      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 281        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 181        |
|    time_elapsed         | 821        |
|    total_timesteps      | 185344     |
| train/                  |            |
|    approx_kl            | 0.10941856 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.3      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | 85.2       |
|    n_updates            | 3600       |
|    policy_gradient_loss | -0.072     |
|    std                  | 0.365      |
|    value_loss           | 280        |
----------------------------------------
Num timesteps: 186000
Best mean reward: 791.71 - Last mean reward per episode: 281.13
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.85e-05   |
| reward_motion           | 8.12e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.15      |
| reward_velocity         | 0.757      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 273        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 182        |
|    time_elapsed         | 825        |
|    total_timesteps      | 186368     |
| train/                  |            |
|    approx_kl            | 0.08115048 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.8      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 67.7       |
|    n_updates            | 3620       |
|    policy_gradient_loss | -0.073     |
|    std                  | 0.365      |
|    value_loss           | 420        |
----------------------------------------
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.89e-05   |
| reward_motion           | 8.18e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.15      |
| reward_velocity         | 0.757      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 272        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 183        |
|    time_elapsed         | 829        |
|    total_timesteps      | 187392     |
| train/                  |            |
|    approx_kl            | 0.06963568 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.6      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 153        |
|    n_updates            | 3640       |
|    policy_gradient_loss | -0.0629    |
|    std                  | 0.365      |
|    value_loss           | 281        |
----------------------------------------
----------------------------------------
| reward                  | -2.45      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.88e-05   |
| reward_motion           | 8.17e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.16      |
| reward_velocity         | 0.76       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 262        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 184        |
|    time_elapsed         | 834        |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.08650813 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 89.9       |
|    n_updates            | 3660       |
|    policy_gradient_loss | -0.0588    |
|    std                  | 0.365      |
|    value_loss           | 342        |
----------------------------------------
-----------------------------------------
| reward                  | -2.45       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.89e-05    |
| reward_motion           | 8.18e-07    |
| reward_position         | 0.000123    |
| reward_torque           | -3.16       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 261         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 185         |
|    time_elapsed         | 838         |
|    total_timesteps      | 189440      |
| train/                  |             |
|    approx_kl            | 0.059413694 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.5       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 123         |
|    n_updates            | 3680        |
|    policy_gradient_loss | -0.0513     |
|    std                  | 0.365       |
|    value_loss           | 307         |
-----------------------------------------
----------------------------------------
| reward                  | -2.45      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.86e-05   |
| reward_motion           | 8.13e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.16      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 255        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 186        |
|    time_elapsed         | 842        |
|    total_timesteps      | 190464     |
| train/                  |            |
|    approx_kl            | 0.08431653 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.2      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 143        |
|    n_updates            | 3700       |
|    policy_gradient_loss | -0.0713    |
|    std                  | 0.365      |
|    value_loss           | 379        |
----------------------------------------
----------------------------------------
| reward                  | -2.45      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.87e-05   |
| reward_motion           | 8.13e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.759      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 255        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 187        |
|    time_elapsed         | 847        |
|    total_timesteps      | 191488     |
| train/                  |            |
|    approx_kl            | 0.08574592 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.1      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 74.4       |
|    n_updates            | 3720       |
|    policy_gradient_loss | -0.0862    |
|    std                  | 0.365      |
|    value_loss           | 314        |
----------------------------------------
Num timesteps: 192000
Best mean reward: 791.71 - Last mean reward per episode: 255.42
-----------------------------------------
| reward                  | -2.46       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.86e-05    |
| reward_motion           | 8.13e-07    |
| reward_position         | 0.000122    |
| reward_torque           | -3.17       |
| reward_velocity         | 0.76        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 247         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 188         |
|    time_elapsed         | 851         |
|    total_timesteps      | 192512      |
| train/                  |             |
|    approx_kl            | 0.098878115 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.1       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 93.8        |
|    n_updates            | 3740        |
|    policy_gradient_loss | -0.0883     |
|    std                  | 0.365       |
|    value_loss           | 382         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.47       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.88e-05    |
| reward_motion           | 8.23e-07    |
| reward_position         | 0.000123    |
| reward_torque           | -3.19       |
| reward_velocity         | 0.761       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 245         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 189         |
|    time_elapsed         | 856         |
|    total_timesteps      | 193536      |
| train/                  |             |
|    approx_kl            | 0.031861797 |
|    clip_fraction        | 0.0913      |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.8       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 188         |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.0382     |
|    std                  | 0.365       |
|    value_loss           | 399         |
-----------------------------------------
----------------------------------------
| reward                  | -2.47      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.85e-05   |
| reward_motion           | 8.17e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.19      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 245        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 190        |
|    time_elapsed         | 860        |
|    total_timesteps      | 194560     |
| train/                  |            |
|    approx_kl            | 0.05897156 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.8      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 134        |
|    n_updates            | 3780       |
|    policy_gradient_loss | -0.0541    |
|    std                  | 0.365      |
|    value_loss           | 388        |
----------------------------------------
----------------------------------------
| reward                  | -2.48      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.84e-05   |
| reward_motion           | 8.16e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.19      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 245        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 191        |
|    time_elapsed         | 865        |
|    total_timesteps      | 195584     |
| train/                  |            |
|    approx_kl            | 0.09375177 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.1      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 23.1       |
|    n_updates            | 3800       |
|    policy_gradient_loss | -0.0871    |
|    std                  | 0.365      |
|    value_loss           | 395        |
----------------------------------------
----------------------------------------
| reward                  | -2.48      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.83e-05   |
| reward_motion           | 8.11e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.2       |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 248        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 192        |
|    time_elapsed         | 870        |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.09083923 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.8      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 300        |
|    n_updates            | 3820       |
|    policy_gradient_loss | -0.0716    |
|    std                  | 0.365      |
|    value_loss           | 479        |
----------------------------------------
----------------------------------------
| reward                  | -2.47      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.95e-05   |
| reward_motion           | 8.31e-07   |
| reward_position         | 0.000125   |
| reward_torque           | -3.19      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 241        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 193        |
|    time_elapsed         | 874        |
|    total_timesteps      | 197632     |
| train/                  |            |
|    approx_kl            | 0.08830097 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.9      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 80.5       |
|    n_updates            | 3840       |
|    policy_gradient_loss | -0.0854    |
|    std                  | 0.365      |
|    value_loss           | 393        |
----------------------------------------
Num timesteps: 198000
Best mean reward: 791.71 - Last mean reward per episode: 241.05
-----------------------------------------
| reward                  | -2.49       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.95e-05    |
| reward_motion           | 8.29e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -3.2        |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 238         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 194         |
|    time_elapsed         | 879         |
|    total_timesteps      | 198656      |
| train/                  |             |
|    approx_kl            | 0.045449574 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20         |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | 66          |
|    n_updates            | 3860        |
|    policy_gradient_loss | -0.0453     |
|    std                  | 0.365       |
|    value_loss           | 317         |
-----------------------------------------
----------------------------------------
| reward                  | -2.49      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.94e-05   |
| reward_motion           | 8.28e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.21      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 231        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 195        |
|    time_elapsed         | 884        |
|    total_timesteps      | 199680     |
| train/                  |            |
|    approx_kl            | 0.09413753 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.9      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 332        |
|    n_updates            | 3880       |
|    policy_gradient_loss | -0.0859    |
|    std                  | 0.365      |
|    value_loss           | 472        |
----------------------------------------
----------------------------------------
| reward                  | -2.49      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.91e-05   |
| reward_motion           | 8.22e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.21      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 231        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 196        |
|    time_elapsed         | 888        |
|    total_timesteps      | 200704     |
| train/                  |            |
|    approx_kl            | 0.08775077 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.7      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 56.5       |
|    n_updates            | 3900       |
|    policy_gradient_loss | -0.0775    |
|    std                  | 0.365      |
|    value_loss           | 360        |
----------------------------------------
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.01e-05   |
| reward_motion           | 8.35e-07   |
| reward_position         | 0.000125   |
| reward_torque           | -3.22      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 231        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 197        |
|    time_elapsed         | 893        |
|    total_timesteps      | 201728     |
| train/                  |            |
|    approx_kl            | 0.08543901 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.2      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 120        |
|    n_updates            | 3920       |
|    policy_gradient_loss | -0.0776    |
|    std                  | 0.365      |
|    value_loss           | 441        |
----------------------------------------
-----------------------------------------
| reward                  | -2.51       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.96e-05    |
| reward_motion           | 8.26e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -3.23       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 220         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 198         |
|    time_elapsed         | 898         |
|    total_timesteps      | 202752      |
| train/                  |             |
|    approx_kl            | 0.101773694 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.6       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 74.5        |
|    n_updates            | 3940        |
|    policy_gradient_loss | -0.093      |
|    std                  | 0.365       |
|    value_loss           | 291         |
-----------------------------------------
----------------------------------------
| reward                  | -2.51      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.92e-05   |
| reward_motion           | 8.21e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.22      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 219        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 199        |
|    time_elapsed         | 902        |
|    total_timesteps      | 203776     |
| train/                  |            |
|    approx_kl            | 0.08246793 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.5      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 46.6       |
|    n_updates            | 3960       |
|    policy_gradient_loss | -0.0725    |
|    std                  | 0.365      |
|    value_loss           | 366        |
----------------------------------------
Num timesteps: 204000
Best mean reward: 791.71 - Last mean reward per episode: 218.92
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.99e-05   |
| reward_motion           | 8.31e-07   |
| reward_position         | 0.000125   |
| reward_torque           | -3.24      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 220        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 200        |
|    time_elapsed         | 907        |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.07519507 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 156        |
|    n_updates            | 3980       |
|    policy_gradient_loss | -0.0767    |
|    std                  | 0.365      |
|    value_loss           | 446        |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.29e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.24      |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 217        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 201        |
|    time_elapsed         | 912        |
|    total_timesteps      | 205824     |
| train/                  |            |
|    approx_kl            | 0.07404611 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.8      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 200        |
|    n_updates            | 4000       |
|    policy_gradient_loss | -0.0743    |
|    std                  | 0.365      |
|    value_loss           | 417        |
----------------------------------------
---------------------------------------
| reward                  | -2.53     |
| reward_contact          | -0.047    |
| reward_ctrl             | 3.93e-05  |
| reward_motion           | 8.14e-07  |
| reward_position         | 0.000122  |
| reward_torque           | -3.24     |
| reward_velocity         | 0.764     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 215       |
| time/                   |           |
|    fps                  | 225       |
|    iterations           | 202       |
|    time_elapsed         | 916       |
|    total_timesteps      | 206848    |
| train/                  |           |
|    approx_kl            | 0.0682548 |
|    clip_fraction        | 0.173     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.7     |
|    explained_variance   | 0.983     |
|    learning_rate        | 0.0003    |
|    loss                 | 275       |
|    n_updates            | 4020      |
|    policy_gradient_loss | -0.0635   |
|    std                  | 0.365     |
|    value_loss           | 386       |
---------------------------------------
-----------------------------------------
| reward                  | -2.53       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.85e-05    |
| reward_motion           | 7.96e-07    |
| reward_position         | 0.000119    |
| reward_torque           | -3.25       |
| reward_velocity         | 0.764       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 215         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 203         |
|    time_elapsed         | 921         |
|    total_timesteps      | 207872      |
| train/                  |             |
|    approx_kl            | 0.058709525 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.9       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 45.7        |
|    n_updates            | 4040        |
|    policy_gradient_loss | -0.0601     |
|    std                  | 0.365       |
|    value_loss           | 348         |
-----------------------------------------
---------------------------------------
| reward                  | -2.53     |
| reward_contact          | -0.047    |
| reward_ctrl             | 3.84e-05  |
| reward_motion           | 7.97e-07  |
| reward_position         | 0.00012   |
| reward_torque           | -3.25     |
| reward_velocity         | 0.765     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 210       |
| time/                   |           |
|    fps                  | 225       |
|    iterations           | 204       |
|    time_elapsed         | 925       |
|    total_timesteps      | 208896    |
| train/                  |           |
|    approx_kl            | 0.0821961 |
|    clip_fraction        | 0.2       |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.4     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 244       |
|    n_updates            | 4060      |
|    policy_gradient_loss | -0.0733   |
|    std                  | 0.365     |
|    value_loss           | 486       |
---------------------------------------
----------------------------------------
| reward                  | -2.54      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.86e-05   |
| reward_motion           | 8e-07      |
| reward_position         | 0.00012    |
| reward_torque           | -3.26      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 209        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 205        |
|    time_elapsed         | 930        |
|    total_timesteps      | 209920     |
| train/                  |            |
|    approx_kl            | 0.09615535 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 118        |
|    n_updates            | 4080       |
|    policy_gradient_loss | -0.0759    |
|    std                  | 0.365      |
|    value_loss           | 424        |
----------------------------------------
Num timesteps: 210000
Best mean reward: 791.71 - Last mean reward per episode: 209.18
----------------------------------------
| reward                  | -2.55      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.89e-05   |
| reward_motion           | 8.04e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -3.27      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 200        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 206        |
|    time_elapsed         | 935        |
|    total_timesteps      | 210944     |
| train/                  |            |
|    approx_kl            | 0.10412278 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.5      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | 58.5       |
|    n_updates            | 4100       |
|    policy_gradient_loss | -0.0897    |
|    std                  | 0.365      |
|    value_loss           | 422        |
----------------------------------------
-----------------------------------------
| reward                  | -2.57       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.89e-05    |
| reward_motion           | 8.05e-07    |
| reward_position         | 0.000121    |
| reward_torque           | -3.29       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 197         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 207         |
|    time_elapsed         | 939         |
|    total_timesteps      | 211968      |
| train/                  |             |
|    approx_kl            | 0.078166276 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.2       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 126         |
|    n_updates            | 4120        |
|    policy_gradient_loss | -0.0763     |
|    std                  | 0.365       |
|    value_loss           | 390         |
-----------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.92e-05   |
| reward_motion           | 8.09e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -3.3       |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 199        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 208        |
|    time_elapsed         | 944        |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.09233539 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19        |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 62.5       |
|    n_updates            | 4140       |
|    policy_gradient_loss | -0.0837    |
|    std                  | 0.365      |
|    value_loss           | 307        |
----------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.97e-05   |
| reward_motion           | 8.16e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.31      |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 198        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 209        |
|    time_elapsed         | 948        |
|    total_timesteps      | 214016     |
| train/                  |            |
|    approx_kl            | 0.07892518 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.1      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 74.1       |
|    n_updates            | 4160       |
|    policy_gradient_loss | -0.0726    |
|    std                  | 0.365      |
|    value_loss           | 370        |
----------------------------------------
----------------------------------------
| reward                  | -2.59      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.97e-05   |
| reward_motion           | 8.13e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.31      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 203        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 210        |
|    time_elapsed         | 952        |
|    total_timesteps      | 215040     |
| train/                  |            |
|    approx_kl            | 0.09467095 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 209        |
|    n_updates            | 4180       |
|    policy_gradient_loss | -0.0849    |
|    std                  | 0.365      |
|    value_loss           | 370        |
----------------------------------------
Num timesteps: 216000
Best mean reward: 791.71 - Last mean reward per episode: 203.38
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.16e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.33      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 209        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 211        |
|    time_elapsed         | 957        |
|    total_timesteps      | 216064     |
| train/                  |            |
|    approx_kl            | 0.09579381 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.3      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 510        |
|    n_updates            | 4200       |
|    policy_gradient_loss | -0.0742    |
|    std                  | 0.365      |
|    value_loss           | 494        |
----------------------------------------
-----------------------------------------
| reward                  | -2.6        |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4e-05       |
| reward_motion           | 8.18e-07    |
| reward_position         | 0.000123    |
| reward_torque           | -3.32       |
| reward_velocity         | 0.769       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 202         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 212         |
|    time_elapsed         | 961         |
|    total_timesteps      | 217088      |
| train/                  |             |
|    approx_kl            | 0.118106246 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.5       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 227         |
|    n_updates            | 4220        |
|    policy_gradient_loss | -0.0861     |
|    std                  | 0.364       |
|    value_loss           | 381         |
-----------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.99e-05   |
| reward_motion           | 8.19e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.33      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 210        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 213        |
|    time_elapsed         | 966        |
|    total_timesteps      | 218112     |
| train/                  |            |
|    approx_kl            | 0.08759506 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.7      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 97.6       |
|    n_updates            | 4240       |
|    policy_gradient_loss | -0.0588    |
|    std                  | 0.364      |
|    value_loss           | 310        |
----------------------------------------
----------------------------------------
| reward                  | -2.6       |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.97e-05   |
| reward_motion           | 8.14e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.33      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 210        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 214        |
|    time_elapsed         | 970        |
|    total_timesteps      | 219136     |
| train/                  |            |
|    approx_kl            | 0.08177506 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.3      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 250        |
|    n_updates            | 4260       |
|    policy_gradient_loss | -0.0723    |
|    std                  | 0.364      |
|    value_loss           | 445        |
----------------------------------------
---------------------------------------
| reward                  | -2.59     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 3.92e-05  |
| reward_motion           | 8.03e-07  |
| reward_position         | 0.00012   |
| reward_torque           | -3.32     |
| reward_velocity         | 0.773     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 200       |
| time/                   |           |
|    fps                  | 225       |
|    iterations           | 215       |
|    time_elapsed         | 975       |
|    total_timesteps      | 220160    |
| train/                  |           |
|    approx_kl            | 0.0946173 |
|    clip_fraction        | 0.233     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.1     |
|    explained_variance   | 0.982     |
|    learning_rate        | 0.0003    |
|    loss                 | 43.8      |
|    n_updates            | 4280      |
|    policy_gradient_loss | -0.0792   |
|    std                  | 0.364     |
|    value_loss           | 362       |
---------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.94e-05   |
| reward_motion           | 8.1e-07    |
| reward_position         | 0.000121   |
| reward_torque           | -3.33      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 198        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 216        |
|    time_elapsed         | 979        |
|    total_timesteps      | 221184     |
| train/                  |            |
|    approx_kl            | 0.06670367 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 116        |
|    n_updates            | 4300       |
|    policy_gradient_loss | -0.0598    |
|    std                  | 0.364      |
|    value_loss           | 426        |
----------------------------------------
Num timesteps: 222000
Best mean reward: 791.71 - Last mean reward per episode: 197.53
-----------------------------------------
| reward                  | -2.62       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.96e-05    |
| reward_motion           | 8.1e-07     |
| reward_position         | 0.000122    |
| reward_torque           | -3.34       |
| reward_velocity         | 0.772       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 190         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 217         |
|    time_elapsed         | 984         |
|    total_timesteps      | 222208      |
| train/                  |             |
|    approx_kl            | 0.061054613 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.1       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 147         |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.0641     |
|    std                  | 0.364       |
|    value_loss           | 424         |
-----------------------------------------
----------------------------------------
| reward                  | -2.62      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.96e-05   |
| reward_motion           | 8.12e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.35      |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 190        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 218        |
|    time_elapsed         | 988        |
|    total_timesteps      | 223232     |
| train/                  |            |
|    approx_kl            | 0.10701846 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.1      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 192        |
|    n_updates            | 4340       |
|    policy_gradient_loss | -0.0829    |
|    std                  | 0.364      |
|    value_loss           | 423        |
----------------------------------------
-----------------------------------------
| reward                  | -2.62       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.98e-05    |
| reward_motion           | 8.14e-07    |
| reward_position         | 0.000122    |
| reward_torque           | -3.35       |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 191         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 219         |
|    time_elapsed         | 992         |
|    total_timesteps      | 224256      |
| train/                  |             |
|    approx_kl            | 0.083152995 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.4       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 102         |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.0553     |
|    std                  | 0.364       |
|    value_loss           | 344         |
-----------------------------------------
----------------------------------------
| reward                  | -2.62      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.13e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.34      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 184        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 220        |
|    time_elapsed         | 997        |
|    total_timesteps      | 225280     |
| train/                  |            |
|    approx_kl            | 0.07975471 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.4      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 93.1       |
|    n_updates            | 4380       |
|    policy_gradient_loss | -0.0786    |
|    std                  | 0.364      |
|    value_loss           | 461        |
----------------------------------------
----------------------------------------
| reward                  | -2.63      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.02e-05   |
| reward_motion           | 8.19e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.35      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 182        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 221        |
|    time_elapsed         | 1001       |
|    total_timesteps      | 226304     |
| train/                  |            |
|    approx_kl            | 0.06731045 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.6      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 434        |
|    n_updates            | 4400       |
|    policy_gradient_loss | -0.0634    |
|    std                  | 0.364      |
|    value_loss           | 550        |
----------------------------------------
----------------------------------------
| reward                  | -2.63      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.14e-05   |
| reward_motion           | 8.38e-07   |
| reward_position         | 0.000126   |
| reward_torque           | -3.35      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 182        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 222        |
|    time_elapsed         | 1005       |
|    total_timesteps      | 227328     |
| train/                  |            |
|    approx_kl            | 0.06061294 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 151        |
|    n_updates            | 4420       |
|    policy_gradient_loss | -0.0688    |
|    std                  | 0.364      |
|    value_loss           | 555        |
----------------------------------------
Num timesteps: 228000
Best mean reward: 791.71 - Last mean reward per episode: 181.53
-----------------------------------------
| reward                  | -2.64       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.06e-05    |
| reward_motion           | 8.26e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -3.36       |
| reward_velocity         | 0.769       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 183         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 223         |
|    time_elapsed         | 1010        |
|    total_timesteps      | 228352      |
| train/                  |             |
|    approx_kl            | 0.090596244 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20         |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.5        |
|    n_updates            | 4440        |
|    policy_gradient_loss | -0.0788     |
|    std                  | 0.364       |
|    value_loss           | 346         |
-----------------------------------------
----------------------------------------
| reward                  | -2.64      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.06e-05   |
| reward_motion           | 8.27e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.37      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 182        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 224        |
|    time_elapsed         | 1014       |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.07922794 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 188        |
|    n_updates            | 4460       |
|    policy_gradient_loss | -0.0688    |
|    std                  | 0.364      |
|    value_loss           | 574        |
----------------------------------------
----------------------------------------
| reward                  | -2.65      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4e-05      |
| reward_motion           | 8.13e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.38      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 173        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 225        |
|    time_elapsed         | 1018       |
|    total_timesteps      | 230400     |
| train/                  |            |
|    approx_kl            | 0.08420156 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.7      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | 140        |
|    n_updates            | 4480       |
|    policy_gradient_loss | -0.0767    |
|    std                  | 0.364      |
|    value_loss           | 407        |
----------------------------------------
----------------------------------------
| reward                  | -2.65      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4e-05      |
| reward_motion           | 8.12e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.38      |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 173        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 226        |
|    time_elapsed         | 1023       |
|    total_timesteps      | 231424     |
| train/                  |            |
|    approx_kl            | 0.11329693 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.6      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 255        |
|    n_updates            | 4500       |
|    policy_gradient_loss | -0.0819    |
|    std                  | 0.364      |
|    value_loss           | 496        |
----------------------------------------
----------------------------------------
| reward                  | -2.66      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4e-05      |
| reward_motion           | 8.11e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.39      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 176        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 227        |
|    time_elapsed         | 1027       |
|    total_timesteps      | 232448     |
| train/                  |            |
|    approx_kl            | 0.08218557 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 47.8       |
|    n_updates            | 4520       |
|    policy_gradient_loss | -0.0796    |
|    std                  | 0.364      |
|    value_loss           | 421        |
----------------------------------------
----------------------------------------
| reward                  | -2.67      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.05e-05   |
| reward_motion           | 8.17e-07   |
| reward_position         | 0.000123   |
| reward_torque           | -3.4       |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 168        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 228        |
|    time_elapsed         | 1031       |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.09102081 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 115        |
|    n_updates            | 4540       |
|    policy_gradient_loss | -0.0842    |
|    std                  | 0.364      |
|    value_loss           | 399        |
----------------------------------------
Num timesteps: 234000
Best mean reward: 791.71 - Last mean reward per episode: 167.55
----------------------------------------
| reward                  | -2.69      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.99e-05   |
| reward_motion           | 8.08e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -3.41      |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 159        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 229        |
|    time_elapsed         | 1036       |
|    total_timesteps      | 234496     |
| train/                  |            |
|    approx_kl            | 0.06711652 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 186        |
|    n_updates            | 4560       |
|    policy_gradient_loss | -0.0594    |
|    std                  | 0.364      |
|    value_loss           | 313        |
----------------------------------------
----------------------------------------
| reward                  | -2.69      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.97e-05   |
| reward_motion           | 8.03e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -3.41      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 149        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 230        |
|    time_elapsed         | 1040       |
|    total_timesteps      | 235520     |
| train/                  |            |
|    approx_kl            | 0.08161515 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20        |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | 247        |
|    n_updates            | 4580       |
|    policy_gradient_loss | -0.0668    |
|    std                  | 0.364      |
|    value_loss           | 621        |
----------------------------------------
---------------------------------------
| reward                  | -2.7      |
| reward_contact          | -0.0469   |
| reward_ctrl             | 3.96e-05  |
| reward_motion           | 8.01e-07  |
| reward_position         | 0.00012   |
| reward_torque           | -3.42     |
| reward_velocity         | 0.77      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 155       |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 231       |
|    time_elapsed         | 1044      |
|    total_timesteps      | 236544    |
| train/                  |           |
|    approx_kl            | 0.0906602 |
|    clip_fraction        | 0.214     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.8     |
|    explained_variance   | 0.975     |
|    learning_rate        | 0.0003    |
|    loss                 | 103       |
|    n_updates            | 4600      |
|    policy_gradient_loss | -0.0795   |
|    std                  | 0.364     |
|    value_loss           | 404       |
---------------------------------------
----------------------------------------
| reward                  | -2.71      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.09e-05   |
| reward_motion           | 8.26e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.43      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 157        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 232        |
|    time_elapsed         | 1049       |
|    total_timesteps      | 237568     |
| train/                  |            |
|    approx_kl            | 0.08881818 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.4      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 223        |
|    n_updates            | 4620       |
|    policy_gradient_loss | -0.0793    |
|    std                  | 0.364      |
|    value_loss           | 533        |
----------------------------------------
---------------------------------------
| reward                  | -2.72     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 4.06e-05  |
| reward_motion           | 8.24e-07  |
| reward_position         | 0.000124  |
| reward_torque           | -3.44     |
| reward_velocity         | 0.77      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 150       |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 233       |
|    time_elapsed         | 1053      |
|    total_timesteps      | 238592    |
| train/                  |           |
|    approx_kl            | 0.0750297 |
|    clip_fraction        | 0.18      |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.6     |
|    explained_variance   | 0.977     |
|    learning_rate        | 0.0003    |
|    loss                 | 224       |
|    n_updates            | 4640      |
|    policy_gradient_loss | -0.0521   |
|    std                  | 0.364     |
|    value_loss           | 468       |
---------------------------------------
----------------------------------------
| reward                  | -2.73      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.07e-05   |
| reward_motion           | 8.23e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.45      |
| reward_velocity         | 0.767      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 141        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 234        |
|    time_elapsed         | 1057       |
|    total_timesteps      | 239616     |
| train/                  |            |
|    approx_kl            | 0.07345635 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.3      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 215        |
|    n_updates            | 4660       |
|    policy_gradient_loss | -0.0686    |
|    std                  | 0.364      |
|    value_loss           | 490        |
----------------------------------------
Num timesteps: 240000
Best mean reward: 791.71 - Last mean reward per episode: 141.28
----------------------------------------
| reward                  | -2.73      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.14e-05   |
| reward_motion           | 8.34e-07   |
| reward_position         | 0.000125   |
| reward_torque           | -3.45      |
| reward_velocity         | 0.767      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 143        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 235        |
|    time_elapsed         | 1062       |
|    total_timesteps      | 240640     |
| train/                  |            |
|    approx_kl            | 0.06890334 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 67.4       |
|    n_updates            | 4680       |
|    policy_gradient_loss | -0.0613    |
|    std                  | 0.364      |
|    value_loss           | 407        |
----------------------------------------
----------------------------------------
| reward                  | -2.74      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.25e-05   |
| reward_motion           | 8.53e-07   |
| reward_position         | 0.000128   |
| reward_torque           | -3.46      |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 137        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 236        |
|    time_elapsed         | 1066       |
|    total_timesteps      | 241664     |
| train/                  |            |
|    approx_kl            | 0.07793336 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.3      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 92.2       |
|    n_updates            | 4700       |
|    policy_gradient_loss | -0.07      |
|    std                  | 0.364      |
|    value_loss           | 611        |
----------------------------------------
----------------------------------------
| reward                  | -2.74      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.25e-05   |
| reward_motion           | 8.52e-07   |
| reward_position         | 0.000128   |
| reward_torque           | -3.46      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 142        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 237        |
|    time_elapsed         | 1071       |
|    total_timesteps      | 242688     |
| train/                  |            |
|    approx_kl            | 0.08569087 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.2      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 81.3       |
|    n_updates            | 4720       |
|    policy_gradient_loss | -0.0705    |
|    std                  | 0.364      |
|    value_loss           | 371        |
----------------------------------------
----------------------------------------
| reward                  | -2.76      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.55e-07   |
| reward_position         | 0.000128   |
| reward_torque           | -3.48      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 137        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 238        |
|    time_elapsed         | 1075       |
|    total_timesteps      | 243712     |
| train/                  |            |
|    approx_kl            | 0.09417659 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.2      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 378        |
|    n_updates            | 4740       |
|    policy_gradient_loss | -0.0716    |
|    std                  | 0.364      |
|    value_loss           | 548        |
----------------------------------------
-----------------------------------------
| reward                  | -2.77       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.29e-05    |
| reward_motion           | 8.59e-07    |
| reward_position         | 0.000129    |
| reward_torque           | -3.49       |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 133         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 239         |
|    time_elapsed         | 1079        |
|    total_timesteps      | 244736      |
| train/                  |             |
|    approx_kl            | 0.076811194 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.2       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.3        |
|    n_updates            | 4760        |
|    policy_gradient_loss | -0.0673     |
|    std                  | 0.364       |
|    value_loss           | 519         |
-----------------------------------------
----------------------------------------
| reward                  | -2.77      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.39e-05   |
| reward_motion           | 8.76e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.49      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 240        |
|    time_elapsed         | 1084       |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.06444687 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 147        |
|    n_updates            | 4780       |
|    policy_gradient_loss | -0.0532    |
|    std                  | 0.364      |
|    value_loss           | 506        |
----------------------------------------
Num timesteps: 246000
Best mean reward: 791.71 - Last mean reward per episode: 127.14
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.41e-05   |
| reward_motion           | 8.79e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -3.5       |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 123        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 241        |
|    time_elapsed         | 1088       |
|    total_timesteps      | 246784     |
| train/                  |            |
|    approx_kl            | 0.07253139 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.6      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 113        |
|    n_updates            | 4800       |
|    policy_gradient_loss | -0.0679    |
|    std                  | 0.364      |
|    value_loss           | 497        |
----------------------------------------
-----------------------------------------
| reward                  | -2.77       |
| reward_contact          | -0.0468     |
| reward_ctrl             | 4.39e-05    |
| reward_motion           | 8.71e-07    |
| reward_position         | 0.000131    |
| reward_torque           | -3.49       |
| reward_velocity         | 0.768       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 114         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 242         |
|    time_elapsed         | 1092        |
|    total_timesteps      | 247808      |
| train/                  |             |
|    approx_kl            | 0.081109695 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.1       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 180         |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.0731     |
|    std                  | 0.364       |
|    value_loss           | 514         |
-----------------------------------------
----------------------------------------
| reward                  | -2.77      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.37e-05   |
| reward_motion           | 8.7e-07    |
| reward_position         | 0.00013    |
| reward_torque           | -3.49      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 115        |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 243        |
|    time_elapsed         | 1097       |
|    total_timesteps      | 248832     |
| train/                  |            |
|    approx_kl            | 0.07144873 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 108        |
|    n_updates            | 4840       |
|    policy_gradient_loss | -0.0479    |
|    std                  | 0.364      |
|    value_loss           | 353        |
----------------------------------------
---------------------------------------
| reward                  | -2.77     |
| reward_contact          | -0.0468   |
| reward_ctrl             | 4.38e-05  |
| reward_motion           | 8.66e-07  |
| reward_position         | 0.00013   |
| reward_torque           | -3.49     |
| reward_velocity         | 0.77      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 105       |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 244       |
|    time_elapsed         | 1102      |
|    total_timesteps      | 249856    |
| train/                  |           |
|    approx_kl            | 0.0925653 |
|    clip_fraction        | 0.234     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.6     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 230       |
|    n_updates            | 4860      |
|    policy_gradient_loss | -0.0768   |
|    std                  | 0.364     |
|    value_loss           | 403       |
---------------------------------------
---------------------------------------
| reward                  | -2.78     |
| reward_contact          | -0.0468   |
| reward_ctrl             | 4.36e-05  |
| reward_motion           | 8.63e-07  |
| reward_position         | 0.00013   |
| reward_torque           | -3.5      |
| reward_velocity         | 0.769     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 99.4      |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 245       |
|    time_elapsed         | 1106      |
|    total_timesteps      | 250880    |
| train/                  |           |
|    approx_kl            | 0.0684265 |
|    clip_fraction        | 0.167     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.7     |
|    explained_variance   | 0.978     |
|    learning_rate        | 0.0003    |
|    loss                 | 381       |
|    n_updates            | 4880      |
|    policy_gradient_loss | -0.0741   |
|    std                  | 0.364     |
|    value_loss           | 536       |
---------------------------------------
-----------------------------------------
| reward                  | -2.78       |
| reward_contact          | -0.0468     |
| reward_ctrl             | 4.37e-05    |
| reward_motion           | 8.66e-07    |
| reward_position         | 0.00013     |
| reward_torque           | -3.5        |
| reward_velocity         | 0.769       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 97.4        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 246         |
|    time_elapsed         | 1111        |
|    total_timesteps      | 251904      |
| train/                  |             |
|    approx_kl            | 0.057845965 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.7       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 227         |
|    n_updates            | 4900        |
|    policy_gradient_loss | -0.0739     |
|    std                  | 0.364       |
|    value_loss           | 432         |
-----------------------------------------
Num timesteps: 252000
Best mean reward: 791.71 - Last mean reward per episode: 97.37
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.66e-07   |
| reward_position         | 0.00013    |
| reward_torque           | -3.51      |
| reward_velocity         | 0.767      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 97.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 247        |
|    time_elapsed         | 1116       |
|    total_timesteps      | 252928     |
| train/                  |            |
|    approx_kl            | 0.06736165 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 200        |
|    n_updates            | 4920       |
|    policy_gradient_loss | -0.0606    |
|    std                  | 0.364      |
|    value_loss           | 564        |
----------------------------------------
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.66e-07   |
| reward_position         | 0.00013    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 93.7       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 248        |
|    time_elapsed         | 1120       |
|    total_timesteps      | 253952     |
| train/                  |            |
|    approx_kl            | 0.08807652 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20        |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 307        |
|    n_updates            | 4940       |
|    policy_gradient_loss | -0.0796    |
|    std                  | 0.364      |
|    value_loss           | 580        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.41e-05   |
| reward_motion           | 8.7e-07    |
| reward_position         | 0.000131   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 90.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 249        |
|    time_elapsed         | 1125       |
|    total_timesteps      | 254976     |
| train/                  |            |
|    approx_kl            | 0.06194989 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 112        |
|    n_updates            | 4960       |
|    policy_gradient_loss | -0.0573    |
|    std                  | 0.364      |
|    value_loss           | 427        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.48e-05   |
| reward_motion           | 8.81e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 97         |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 250        |
|    time_elapsed         | 1130       |
|    total_timesteps      | 256000     |
| train/                  |            |
|    approx_kl            | 0.06726232 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 165        |
|    n_updates            | 4980       |
|    policy_gradient_loss | -0.0606    |
|    std                  | 0.364      |
|    value_loss           | 454        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.56e-05   |
| reward_motion           | 8.9e-07    |
| reward_position         | 0.000133   |
| reward_torque           | -3.5       |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 91.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 251        |
|    time_elapsed         | 1134       |
|    total_timesteps      | 257024     |
| train/                  |            |
|    approx_kl            | 0.09167318 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.2      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 295        |
|    n_updates            | 5000       |
|    policy_gradient_loss | -0.0873    |
|    std                  | 0.364      |
|    value_loss           | 558        |
----------------------------------------
Num timesteps: 258000
Best mean reward: 791.71 - Last mean reward per episode: 91.10
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.46e-05   |
| reward_motion           | 8.75e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.5       |
| reward_velocity         | 0.765      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 83         |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 252        |
|    time_elapsed         | 1139       |
|    total_timesteps      | 258048     |
| train/                  |            |
|    approx_kl            | 0.08047421 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 205        |
|    n_updates            | 5020       |
|    policy_gradient_loss | -0.0812    |
|    std                  | 0.364      |
|    value_loss           | 417        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.48e-05   |
| reward_motion           | 8.77e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 84.5       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 253        |
|    time_elapsed         | 1143       |
|    total_timesteps      | 259072     |
| train/                  |            |
|    approx_kl            | 0.06514632 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 136        |
|    n_updates            | 5040       |
|    policy_gradient_loss | -0.0661    |
|    std                  | 0.364      |
|    value_loss           | 485        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.53e-05   |
| reward_motion           | 8.9e-07    |
| reward_position         | 0.000133   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 83.6       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 254        |
|    time_elapsed         | 1148       |
|    total_timesteps      | 260096     |
| train/                  |            |
|    approx_kl            | 0.07208417 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.4      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 284        |
|    n_updates            | 5060       |
|    policy_gradient_loss | -0.0785    |
|    std                  | 0.364      |
|    value_loss           | 568        |
----------------------------------------
-----------------------------------------
| reward                  | -2.78       |
| reward_contact          | -0.0467     |
| reward_ctrl             | 4.52e-05    |
| reward_motion           | 8.89e-07    |
| reward_position         | 0.000133    |
| reward_torque           | -3.5        |
| reward_velocity         | 0.767       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 78.3        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 255         |
|    time_elapsed         | 1152        |
|    total_timesteps      | 261120      |
| train/                  |             |
|    approx_kl            | 0.068733916 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.2       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 70.3        |
|    n_updates            | 5080        |
|    policy_gradient_loss | -0.0525     |
|    std                  | 0.364       |
|    value_loss           | 279         |
-----------------------------------------
---------------------------------------
| reward                  | -2.79     |
| reward_contact          | -0.0467   |
| reward_ctrl             | 4.56e-05  |
| reward_motion           | 8.96e-07  |
| reward_position         | 0.000134  |
| reward_torque           | -3.51     |
| reward_velocity         | 0.768     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 73.8      |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 256       |
|    time_elapsed         | 1157      |
|    total_timesteps      | 262144    |
| train/                  |           |
|    approx_kl            | 0.0644877 |
|    clip_fraction        | 0.138     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.5     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 312       |
|    n_updates            | 5100      |
|    policy_gradient_loss | -0.0623   |
|    std                  | 0.364     |
|    value_loss           | 532       |
---------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.59e-05   |
| reward_motion           | 8.99e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 69.5       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 257        |
|    time_elapsed         | 1162       |
|    total_timesteps      | 263168     |
| train/                  |            |
|    approx_kl            | 0.09269175 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 208        |
|    n_updates            | 5120       |
|    policy_gradient_loss | -0.0786    |
|    std                  | 0.364      |
|    value_loss           | 426        |
----------------------------------------
Num timesteps: 264000
Best mean reward: 791.71 - Last mean reward per episode: 69.46
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.61e-05   |
| reward_motion           | 9.05e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 71         |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 258        |
|    time_elapsed         | 1166       |
|    total_timesteps      | 264192     |
| train/                  |            |
|    approx_kl            | 0.08261158 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 84.6       |
|    n_updates            | 5140       |
|    policy_gradient_loss | -0.0697    |
|    std                  | 0.364      |
|    value_loss           | 444        |
----------------------------------------
----------------------------------------
| reward                  | -2.8       |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.63e-05   |
| reward_motion           | 9.05e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 73         |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 259        |
|    time_elapsed         | 1171       |
|    total_timesteps      | 265216     |
| train/                  |            |
|    approx_kl            | 0.15402758 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.6      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 48         |
|    n_updates            | 5160       |
|    policy_gradient_loss | -0.0891    |
|    std                  | 0.364      |
|    value_loss           | 459        |
----------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.64e-05   |
| reward_motion           | 9.09e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.768      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 68.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 260        |
|    time_elapsed         | 1175       |
|    total_timesteps      | 266240     |
| train/                  |            |
|    approx_kl            | 0.09338033 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 225        |
|    n_updates            | 5180       |
|    policy_gradient_loss | -0.0844    |
|    std                  | 0.363      |
|    value_loss           | 488        |
----------------------------------------
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.63e-05   |
| reward_motion           | 9.07e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.5       |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 65.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 261        |
|    time_elapsed         | 1180       |
|    total_timesteps      | 267264     |
| train/                  |            |
|    approx_kl            | 0.07121593 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 156        |
|    n_updates            | 5200       |
|    policy_gradient_loss | -0.0671    |
|    std                  | 0.363      |
|    value_loss           | 475        |
----------------------------------------
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.64e-05   |
| reward_motion           | 9.14e-07   |
| reward_position         | 0.000137   |
| reward_torque           | -3.5       |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 63.4       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 262        |
|    time_elapsed         | 1185       |
|    total_timesteps      | 268288     |
| train/                  |            |
|    approx_kl            | 0.08495103 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.2      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 149        |
|    n_updates            | 5220       |
|    policy_gradient_loss | -0.0736    |
|    std                  | 0.363      |
|    value_loss           | 465        |
----------------------------------------
----------------------------------------
| reward                  | -2.78      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.64e-05   |
| reward_motion           | 9.15e-07   |
| reward_position         | 0.000137   |
| reward_torque           | -3.51      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 66.9       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 263        |
|    time_elapsed         | 1189       |
|    total_timesteps      | 269312     |
| train/                  |            |
|    approx_kl            | 0.11406556 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.3      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 200        |
|    n_updates            | 5240       |
|    policy_gradient_loss | -0.0798    |
|    std                  | 0.363      |
|    value_loss           | 663        |
----------------------------------------
Num timesteps: 270000
Best mean reward: 791.71 - Last mean reward per episode: 66.95
---------------------------------------
| reward                  | -2.78     |
| reward_contact          | -0.0467   |
| reward_ctrl             | 4.69e-05  |
| reward_motion           | 9.23e-07  |
| reward_position         | 0.000138  |
| reward_torque           | -3.5      |
| reward_velocity         | 0.772     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 58.3      |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 264       |
|    time_elapsed         | 1194      |
|    total_timesteps      | 270336    |
| train/                  |           |
|    approx_kl            | 0.0768143 |
|    clip_fraction        | 0.209     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.972     |
|    learning_rate        | 0.0003    |
|    loss                 | 58.3      |
|    n_updates            | 5260      |
|    policy_gradient_loss | -0.0761   |
|    std                  | 0.363     |
|    value_loss           | 505       |
---------------------------------------
----------------------------------------
| reward                  | -2.79      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.68e-05   |
| reward_motion           | 9.21e-07   |
| reward_position         | 0.000138   |
| reward_torque           | -3.52      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 59.3       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 265        |
|    time_elapsed         | 1199       |
|    total_timesteps      | 271360     |
| train/                  |            |
|    approx_kl            | 0.08713607 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 206        |
|    n_updates            | 5280       |
|    policy_gradient_loss | -0.0601    |
|    std                  | 0.363      |
|    value_loss           | 467        |
----------------------------------------
-----------------------------------------
| reward                  | -2.8        |
| reward_contact          | -0.0467     |
| reward_ctrl             | 4.62e-05    |
| reward_motion           | 9.17e-07    |
| reward_position         | 0.00014     |
| reward_torque           | -3.53       |
| reward_velocity         | 0.772       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 64.9        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 266         |
|    time_elapsed         | 1203        |
|    total_timesteps      | 272384      |
| train/                  |             |
|    approx_kl            | 0.088511154 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.5       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 327         |
|    n_updates            | 5300        |
|    policy_gradient_loss | -0.0809     |
|    std                  | 0.363       |
|    value_loss           | 423         |
-----------------------------------------
----------------------------------------
| reward                  | -2.81      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.58e-05   |
| reward_motion           | 9.09e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -3.53      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 67.2       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 267        |
|    time_elapsed         | 1208       |
|    total_timesteps      | 273408     |
| train/                  |            |
|    approx_kl            | 0.08538949 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.3      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 62.1       |
|    n_updates            | 5320       |
|    policy_gradient_loss | -0.0803    |
|    std                  | 0.363      |
|    value_loss           | 611        |
----------------------------------------
----------------------------------------
| reward                  | -2.82      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.71e-05   |
| reward_motion           | 9.28e-07   |
| reward_position         | 0.000142   |
| reward_torque           | -3.54      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 62.3       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 268        |
|    time_elapsed         | 1212       |
|    total_timesteps      | 274432     |
| train/                  |            |
|    approx_kl            | 0.10191356 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 124        |
|    n_updates            | 5340       |
|    policy_gradient_loss | -0.0903    |
|    std                  | 0.363      |
|    value_loss           | 570        |
----------------------------------------
----------------------------------------
| reward                  | -2.81      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.72e-05   |
| reward_motion           | 9.27e-07   |
| reward_position         | 0.000142   |
| reward_torque           | -3.54      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 64.6       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 269        |
|    time_elapsed         | 1216       |
|    total_timesteps      | 275456     |
| train/                  |            |
|    approx_kl            | 0.06791703 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 153        |
|    n_updates            | 5360       |
|    policy_gradient_loss | -0.0707    |
|    std                  | 0.363      |
|    value_loss           | 403        |
----------------------------------------
Num timesteps: 276000
Best mean reward: 791.71 - Last mean reward per episode: 64.63
----------------------------------------
| reward                  | -2.81      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.71e-05   |
| reward_motion           | 9.23e-07   |
| reward_position         | 0.000141   |
| reward_torque           | -3.53      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 56.2       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 270        |
|    time_elapsed         | 1221       |
|    total_timesteps      | 276480     |
| train/                  |            |
|    approx_kl            | 0.09127189 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 251        |
|    n_updates            | 5380       |
|    policy_gradient_loss | -0.0753    |
|    std                  | 0.363      |
|    value_loss           | 497        |
----------------------------------------
-----------------------------------------
| reward                  | -2.82       |
| reward_contact          | -0.0467     |
| reward_ctrl             | 4.63e-05    |
| reward_motion           | 9.11e-07    |
| reward_position         | 0.000139    |
| reward_torque           | -3.55       |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 48.2        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 271         |
|    time_elapsed         | 1225        |
|    total_timesteps      | 277504      |
| train/                  |             |
|    approx_kl            | 0.053911544 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.9       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 122         |
|    n_updates            | 5400        |
|    policy_gradient_loss | -0.0572     |
|    std                  | 0.363       |
|    value_loss           | 436         |
-----------------------------------------
----------------------------------------
| reward                  | -2.82      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.62e-05   |
| reward_motion           | 9.12e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -3.55      |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 45.3       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 272        |
|    time_elapsed         | 1230       |
|    total_timesteps      | 278528     |
| train/                  |            |
|    approx_kl            | 0.08128683 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 72         |
|    n_updates            | 5420       |
|    policy_gradient_loss | -0.0645    |
|    std                  | 0.363      |
|    value_loss           | 369        |
----------------------------------------
----------------------------------------
| reward                  | -2.82      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.62e-05   |
| reward_motion           | 9.11e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -3.55      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 41         |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 273        |
|    time_elapsed         | 1234       |
|    total_timesteps      | 279552     |
| train/                  |            |
|    approx_kl            | 0.07408503 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 269        |
|    n_updates            | 5440       |
|    policy_gradient_loss | -0.0741    |
|    std                  | 0.363      |
|    value_loss           | 479        |
----------------------------------------
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.58e-05   |
| reward_motion           | 9.07e-07   |
| reward_position         | 0.000138   |
| reward_torque           | -3.55      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 34.3       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 274        |
|    time_elapsed         | 1238       |
|    total_timesteps      | 280576     |
| train/                  |            |
|    approx_kl            | 0.06615147 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 213        |
|    n_updates            | 5460       |
|    policy_gradient_loss | -0.0655    |
|    std                  | 0.363      |
|    value_loss           | 421        |
----------------------------------------
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.57e-05   |
| reward_motion           | 9.09e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -3.56      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 36.9       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 275        |
|    time_elapsed         | 1243       |
|    total_timesteps      | 281600     |
| train/                  |            |
|    approx_kl            | 0.07603255 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | 166        |
|    n_updates            | 5480       |
|    policy_gradient_loss | -0.0649    |
|    std                  | 0.363      |
|    value_loss           | 395        |
----------------------------------------
Num timesteps: 282000
Best mean reward: 791.71 - Last mean reward per episode: 36.90
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.58e-05   |
| reward_motion           | 9.1e-07    |
| reward_position         | 0.000139   |
| reward_torque           | -3.56      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 35.2       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 276        |
|    time_elapsed         | 1247       |
|    total_timesteps      | 282624     |
| train/                  |            |
|    approx_kl            | 0.08953834 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 119        |
|    n_updates            | 5500       |
|    policy_gradient_loss | -0.092     |
|    std                  | 0.363      |
|    value_loss           | 422        |
----------------------------------------
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.58e-05   |
| reward_motion           | 9.08e-07   |
| reward_position         | 0.000139   |
| reward_torque           | -3.55      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 36.1       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 277        |
|    time_elapsed         | 1251       |
|    total_timesteps      | 283648     |
| train/                  |            |
|    approx_kl            | 0.09077023 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.5      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 81.7       |
|    n_updates            | 5520       |
|    policy_gradient_loss | -0.0753    |
|    std                  | 0.363      |
|    value_loss           | 421        |
----------------------------------------
-----------------------------------------
| reward                  | -2.82       |
| reward_contact          | -0.0467     |
| reward_ctrl             | 4.6e-05     |
| reward_motion           | 9.11e-07    |
| reward_position         | 0.000139    |
| reward_torque           | -3.55       |
| reward_velocity         | 0.774       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 37.4        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 278         |
|    time_elapsed         | 1255        |
|    total_timesteps      | 284672      |
| train/                  |             |
|    approx_kl            | 0.098774806 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.6       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 441         |
|    n_updates            | 5540        |
|    policy_gradient_loss | -0.0813     |
|    std                  | 0.363       |
|    value_loss           | 492         |
-----------------------------------------
----------------------------------------
| reward                  | -2.81      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.55e-05   |
| reward_motion           | 9.03e-07   |
| reward_position         | 0.000138   |
| reward_torque           | -3.54      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 29.7       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 279        |
|    time_elapsed         | 1260       |
|    total_timesteps      | 285696     |
| train/                  |            |
|    approx_kl            | 0.10155187 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.4      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 123        |
|    n_updates            | 5560       |
|    policy_gradient_loss | -0.0703    |
|    std                  | 0.363      |
|    value_loss           | 481        |
----------------------------------------
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.41e-05   |
| reward_motion           | 8.82e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.56      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 22.9       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 280        |
|    time_elapsed         | 1264       |
|    total_timesteps      | 286720     |
| train/                  |            |
|    approx_kl            | 0.07861796 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 108        |
|    n_updates            | 5580       |
|    policy_gradient_loss | -0.0738    |
|    std                  | 0.363      |
|    value_loss           | 411        |
----------------------------------------
-----------------------------------------
| reward                  | -2.84       |
| reward_contact          | -0.0468     |
| reward_ctrl             | 4.29e-05    |
| reward_motion           | 8.64e-07    |
| reward_position         | 0.000132    |
| reward_torque           | -3.57       |
| reward_velocity         | 0.778       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 19.5        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 281         |
|    time_elapsed         | 1268        |
|    total_timesteps      | 287744      |
| train/                  |             |
|    approx_kl            | 0.121506184 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 5600        |
|    policy_gradient_loss | -0.0625     |
|    std                  | 0.363       |
|    value_loss           | 327         |
-----------------------------------------
Num timesteps: 288000
Best mean reward: 791.71 - Last mean reward per episode: 19.47
----------------------------------------
| reward                  | -2.83      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.29e-05   |
| reward_motion           | 8.63e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 26.8       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 282        |
|    time_elapsed         | 1273       |
|    total_timesteps      | 288768     |
| train/                  |            |
|    approx_kl            | 0.06990592 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 136        |
|    n_updates            | 5620       |
|    policy_gradient_loss | -0.0652    |
|    std                  | 0.363      |
|    value_loss           | 336        |
----------------------------------------
-----------------------------------------
| reward                  | -2.84       |
| reward_contact          | -0.0468     |
| reward_ctrl             | 4.29e-05    |
| reward_motion           | 8.6e-07     |
| reward_position         | 0.000131    |
| reward_torque           | -3.58       |
| reward_velocity         | 0.78        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 19.2        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 283         |
|    time_elapsed         | 1277        |
|    total_timesteps      | 289792      |
| train/                  |             |
|    approx_kl            | 0.100373924 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 179         |
|    n_updates            | 5640        |
|    policy_gradient_loss | -0.099      |
|    std                  | 0.363       |
|    value_loss           | 367         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.84       |
| reward_contact          | -0.0468     |
| reward_ctrl             | 4.29e-05    |
| reward_motion           | 8.59e-07    |
| reward_position         | 0.000131    |
| reward_torque           | -3.57       |
| reward_velocity         | 0.778       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 22.7        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 284         |
|    time_elapsed         | 1281        |
|    total_timesteps      | 290816      |
| train/                  |             |
|    approx_kl            | 0.100580186 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.8       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 236         |
|    n_updates            | 5660        |
|    policy_gradient_loss | -0.0729     |
|    std                  | 0.363       |
|    value_loss           | 416         |
-----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.36e-05   |
| reward_motion           | 8.73e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 18.5       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 285        |
|    time_elapsed         | 1286       |
|    total_timesteps      | 291840     |
| train/                  |            |
|    approx_kl            | 0.09491367 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 177        |
|    n_updates            | 5680       |
|    policy_gradient_loss | -0.082     |
|    std                  | 0.363      |
|    value_loss           | 404        |
----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.36e-05   |
| reward_motion           | 8.74e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 6.95       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 286        |
|    time_elapsed         | 1290       |
|    total_timesteps      | 292864     |
| train/                  |            |
|    approx_kl            | 0.05984002 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 144        |
|    n_updates            | 5700       |
|    policy_gradient_loss | -0.0675    |
|    std                  | 0.363      |
|    value_loss           | 372        |
----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 4.42e-05   |
| reward_motion           | 8.86e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 2.41       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 287        |
|    time_elapsed         | 1295       |
|    total_timesteps      | 293888     |
| train/                  |            |
|    approx_kl            | 0.10393738 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 155        |
|    n_updates            | 5720       |
|    policy_gradient_loss | -0.0829    |
|    std                  | 0.363      |
|    value_loss           | 446        |
----------------------------------------
Num timesteps: 294000
Best mean reward: 791.71 - Last mean reward per episode: 2.41
---------------------------------------
| reward                  | -2.83     |
| reward_contact          | -0.0468   |
| reward_ctrl             | 4.41e-05  |
| reward_motion           | 8.84e-07  |
| reward_position         | 0.000135  |
| reward_torque           | -3.56     |
| reward_velocity         | 0.777     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | 4.24      |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 288       |
|    time_elapsed         | 1299      |
|    total_timesteps      | 294912    |
| train/                  |           |
|    approx_kl            | 0.0791928 |
|    clip_fraction        | 0.199     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.9     |
|    explained_variance   | 0.972     |
|    learning_rate        | 0.0003    |
|    loss                 | 197       |
|    n_updates            | 5740      |
|    policy_gradient_loss | -0.0775   |
|    std                  | 0.363     |
|    value_loss           | 445       |
---------------------------------------
----------------------------------------
| reward                  | -2.82      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.42e-05   |
| reward_motion           | 8.79e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.55      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | 7.74       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 289        |
|    time_elapsed         | 1303       |
|    total_timesteps      | 295936     |
| train/                  |            |
|    approx_kl            | 0.11338747 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 102        |
|    n_updates            | 5760       |
|    policy_gradient_loss | -0.0832    |
|    std                  | 0.363      |
|    value_loss           | 453        |
----------------------------------------
-----------------------------------------
| reward                  | -2.82       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.42e-05    |
| reward_motion           | 8.77e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -3.55       |
| reward_velocity         | 0.776       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 4.88        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 290         |
|    time_elapsed         | 1308        |
|    total_timesteps      | 296960      |
| train/                  |             |
|    approx_kl            | 0.085854016 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.7       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 264         |
|    n_updates            | 5780        |
|    policy_gradient_loss | -0.0722     |
|    std                  | 0.363       |
|    value_loss           | 467         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.83       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.45e-05    |
| reward_motion           | 8.82e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -3.56       |
| reward_velocity         | 0.776       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | 5.61        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 291         |
|    time_elapsed         | 1312        |
|    total_timesteps      | 297984      |
| train/                  |             |
|    approx_kl            | 0.122409046 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.8       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 84.2        |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.0768     |
|    std                  | 0.363       |
|    value_loss           | 324         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.83       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.58e-05    |
| reward_motion           | 9.01e-07    |
| reward_position         | 0.000138    |
| reward_torque           | -3.56       |
| reward_velocity         | 0.775       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -4.49       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 292         |
|    time_elapsed         | 1316        |
|    total_timesteps      | 299008      |
| train/                  |             |
|    approx_kl            | 0.106792346 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21         |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 74.4        |
|    n_updates            | 5820        |
|    policy_gradient_loss | -0.0909     |
|    std                  | 0.363       |
|    value_loss           | 537         |
-----------------------------------------
Num timesteps: 300000
Best mean reward: 791.71 - Last mean reward per episode: -4.49
-----------------------------------------
| reward                  | -2.84       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 4.47e-05    |
| reward_motion           | 8.85e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -3.57       |
| reward_velocity         | 0.775       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -5.45       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 293         |
|    time_elapsed         | 1321        |
|    total_timesteps      | 300032      |
| train/                  |             |
|    approx_kl            | 0.081342936 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 87.6        |
|    n_updates            | 5840        |
|    policy_gradient_loss | -0.0754     |
|    std                  | 0.363       |
|    value_loss           | 442         |
-----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.48e-05   |
| reward_motion           | 8.88e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -5.69      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 294        |
|    time_elapsed         | 1325       |
|    total_timesteps      | 301056     |
| train/                  |            |
|    approx_kl            | 0.08617334 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 110        |
|    n_updates            | 5860       |
|    policy_gradient_loss | -0.0753    |
|    std                  | 0.363      |
|    value_loss           | 528        |
----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.49e-05   |
| reward_motion           | 8.89e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -4.42      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 295        |
|    time_elapsed         | 1330       |
|    total_timesteps      | 302080     |
| train/                  |            |
|    approx_kl            | 0.05853964 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 181        |
|    n_updates            | 5880       |
|    policy_gradient_loss | -0.0607    |
|    std                  | 0.363      |
|    value_loss           | 525        |
----------------------------------------
----------------------------------------
| reward                  | -2.85      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.49e-05   |
| reward_motion           | 8.91e-07   |
| reward_position         | 0.000136   |
| reward_torque           | -3.58      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -9.75      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 296        |
|    time_elapsed         | 1334       |
|    total_timesteps      | 303104     |
| train/                  |            |
|    approx_kl            | 0.08167342 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 59.6       |
|    n_updates            | 5900       |
|    policy_gradient_loss | -0.067     |
|    std                  | 0.363      |
|    value_loss           | 267        |
----------------------------------------
-----------------------------------------
| reward                  | -2.85       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.35e-05    |
| reward_motion           | 8.68e-07    |
| reward_position         | 0.000133    |
| reward_torque           | -3.58       |
| reward_velocity         | 0.775       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -4.24       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 297         |
|    time_elapsed         | 1339        |
|    total_timesteps      | 304128      |
| train/                  |             |
|    approx_kl            | 0.079787806 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.8       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 163         |
|    n_updates            | 5920        |
|    policy_gradient_loss | -0.0805     |
|    std                  | 0.363       |
|    value_loss           | 554         |
-----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.46e-05   |
| reward_motion           | 8.83e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.777      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -8.79      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 298        |
|    time_elapsed         | 1343       |
|    total_timesteps      | 305152     |
| train/                  |            |
|    approx_kl            | 0.10891131 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 320        |
|    n_updates            | 5940       |
|    policy_gradient_loss | -0.0834    |
|    std                  | 0.363      |
|    value_loss           | 422        |
----------------------------------------
Num timesteps: 306000
Best mean reward: 791.71 - Last mean reward per episode: -8.79
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.45e-05   |
| reward_motion           | 8.81e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -11.2      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 299        |
|    time_elapsed         | 1348       |
|    total_timesteps      | 306176     |
| train/                  |            |
|    approx_kl            | 0.09185974 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 243        |
|    n_updates            | 5960       |
|    policy_gradient_loss | -0.0714    |
|    std                  | 0.363      |
|    value_loss           | 389        |
----------------------------------------
----------------------------------------
| reward                  | -2.84      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.39e-05   |
| reward_motion           | 8.73e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -3.57      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -17.8      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 300        |
|    time_elapsed         | 1352       |
|    total_timesteps      | 307200     |
| train/                  |            |
|    approx_kl            | 0.07487345 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 120        |
|    n_updates            | 5980       |
|    policy_gradient_loss | -0.0701    |
|    std                  | 0.363      |
|    value_loss           | 362        |
----------------------------------------
---------------------------------------
| reward                  | -2.85     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.39e-05  |
| reward_motion           | 8.75e-07  |
| reward_position         | 0.000134  |
| reward_torque           | -3.58     |
| reward_velocity         | 0.773     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -22.2     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 301       |
|    time_elapsed         | 1357      |
|    total_timesteps      | 308224    |
| train/                  |           |
|    approx_kl            | 0.0672723 |
|    clip_fraction        | 0.161     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.973     |
|    learning_rate        | 0.0003    |
|    loss                 | 94.2      |
|    n_updates            | 6000      |
|    policy_gradient_loss | -0.0552   |
|    std                  | 0.363     |
|    value_loss           | 406       |
---------------------------------------
----------------------------------------
| reward                  | -2.86      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.4e-05    |
| reward_motion           | 8.76e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.59      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -22.7      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 302        |
|    time_elapsed         | 1361       |
|    total_timesteps      | 309248     |
| train/                  |            |
|    approx_kl            | 0.07330081 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 71.3       |
|    n_updates            | 6020       |
|    policy_gradient_loss | -0.058     |
|    std                  | 0.363      |
|    value_loss           | 404        |
----------------------------------------
-----------------------------------------
| reward                  | -2.87       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.4e-05     |
| reward_motion           | 8.76e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -3.59       |
| reward_velocity         | 0.773       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -20.3       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 303         |
|    time_elapsed         | 1365        |
|    total_timesteps      | 310272      |
| train/                  |             |
|    approx_kl            | 0.060410976 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | 290         |
|    n_updates            | 6040        |
|    policy_gradient_loss | -0.0609     |
|    std                  | 0.363       |
|    value_loss           | 535         |
-----------------------------------------
----------------------------------------
| reward                  | -2.86      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.71e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -3.59      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -19        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 304        |
|    time_elapsed         | 1370       |
|    total_timesteps      | 311296     |
| train/                  |            |
|    approx_kl            | 0.09256604 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 165        |
|    n_updates            | 6060       |
|    policy_gradient_loss | -0.0847    |
|    std                  | 0.363      |
|    value_loss           | 359        |
----------------------------------------
Num timesteps: 312000
Best mean reward: 791.71 - Last mean reward per episode: -18.98
---------------------------------------
| reward                  | -2.86     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.42e-05  |
| reward_motion           | 8.81e-07  |
| reward_position         | 0.000135  |
| reward_torque           | -3.59     |
| reward_velocity         | 0.778     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -28.5     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 305       |
|    time_elapsed         | 1374      |
|    total_timesteps      | 312320    |
| train/                  |           |
|    approx_kl            | 0.0849249 |
|    clip_fraction        | 0.216     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.9     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 33.1      |
|    n_updates            | 6080      |
|    policy_gradient_loss | -0.0641   |
|    std                  | 0.362     |
|    value_loss           | 464       |
---------------------------------------
-----------------------------------------
| reward                  | -2.86       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.36e-05    |
| reward_motion           | 8.73e-07    |
| reward_position         | 0.000133    |
| reward_torque           | -3.59       |
| reward_velocity         | 0.775       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -31.9       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 306         |
|    time_elapsed         | 1378        |
|    total_timesteps      | 313344      |
| train/                  |             |
|    approx_kl            | 0.056493938 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.2       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 223         |
|    n_updates            | 6100        |
|    policy_gradient_loss | -0.0561     |
|    std                  | 0.362       |
|    value_loss           | 455         |
-----------------------------------------
----------------------------------------
| reward                  | -2.86      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.78e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.59      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -29.3      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 307        |
|    time_elapsed         | 1383       |
|    total_timesteps      | 314368     |
| train/                  |            |
|    approx_kl            | 0.09777212 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 142        |
|    n_updates            | 6120       |
|    policy_gradient_loss | -0.0864    |
|    std                  | 0.362      |
|    value_loss           | 448        |
----------------------------------------
-----------------------------------------
| reward                  | -2.86       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.35e-05    |
| reward_motion           | 8.73e-07    |
| reward_position         | 0.000133    |
| reward_torque           | -3.59       |
| reward_velocity         | 0.775       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -39.7       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 308         |
|    time_elapsed         | 1387        |
|    total_timesteps      | 315392      |
| train/                  |             |
|    approx_kl            | 0.090832666 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.2       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 323         |
|    n_updates            | 6140        |
|    policy_gradient_loss | -0.0817     |
|    std                  | 0.362       |
|    value_loss           | 542         |
-----------------------------------------
---------------------------------------
| reward                  | -2.87     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.31e-05  |
| reward_motion           | 8.68e-07  |
| reward_position         | 0.000133  |
| reward_torque           | -3.6      |
| reward_velocity         | 0.771     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -46.8     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 309       |
|    time_elapsed         | 1392      |
|    total_timesteps      | 316416    |
| train/                  |           |
|    approx_kl            | 0.0758764 |
|    clip_fraction        | 0.27      |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.3     |
|    explained_variance   | 0.971     |
|    learning_rate        | 0.0003    |
|    loss                 | 92.8      |
|    n_updates            | 6160      |
|    policy_gradient_loss | -0.0626   |
|    std                  | 0.362     |
|    value_loss           | 457       |
---------------------------------------
-----------------------------------------
| reward                  | -2.88       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.3e-05     |
| reward_motion           | 8.66e-07    |
| reward_position         | 0.000132    |
| reward_torque           | -3.6        |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -49.1       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 310         |
|    time_elapsed         | 1396        |
|    total_timesteps      | 317440      |
| train/                  |             |
|    approx_kl            | 0.074833825 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | 140         |
|    n_updates            | 6180        |
|    policy_gradient_loss | -0.0625     |
|    std                  | 0.362       |
|    value_loss           | 283         |
-----------------------------------------
Num timesteps: 318000
Best mean reward: 791.71 - Last mean reward per episode: -49.09
----------------------------------------
| reward                  | -2.88      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.8e-07    |
| reward_position         | 0.000134   |
| reward_torque           | -3.6       |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -52.4      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 311        |
|    time_elapsed         | 1400       |
|    total_timesteps      | 318464     |
| train/                  |            |
|    approx_kl            | 0.07202639 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 63.3       |
|    n_updates            | 6200       |
|    policy_gradient_loss | -0.0701    |
|    std                  | 0.362      |
|    value_loss           | 443        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.36e-05   |
| reward_motion           | 8.78e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -49.1      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 312        |
|    time_elapsed         | 1405       |
|    total_timesteps      | 319488     |
| train/                  |            |
|    approx_kl            | 0.07316707 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 176        |
|    n_updates            | 6220       |
|    policy_gradient_loss | -0.0767    |
|    std                  | 0.362      |
|    value_loss           | 487        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.36e-05   |
| reward_motion           | 8.78e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -53.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 313        |
|    time_elapsed         | 1409       |
|    total_timesteps      | 320512     |
| train/                  |            |
|    approx_kl            | 0.08623876 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 42.6       |
|    n_updates            | 6240       |
|    policy_gradient_loss | -0.0746    |
|    std                  | 0.362      |
|    value_loss           | 419        |
----------------------------------------
-----------------------------------------
| reward                  | -2.89       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.39e-05    |
| reward_motion           | 8.83e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -3.61       |
| reward_velocity         | 0.774       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -58         |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 314         |
|    time_elapsed         | 1414        |
|    total_timesteps      | 321536      |
| train/                  |             |
|    approx_kl            | 0.054401636 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.2       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 103         |
|    n_updates            | 6260        |
|    policy_gradient_loss | -0.0629     |
|    std                  | 0.362       |
|    value_loss           | 473         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.9        |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.4e-05     |
| reward_motion           | 8.84e-07    |
| reward_position         | 0.000135    |
| reward_torque           | -3.62       |
| reward_velocity         | 0.773       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -58.7       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 315         |
|    time_elapsed         | 1419        |
|    total_timesteps      | 322560      |
| train/                  |             |
|    approx_kl            | 0.097741075 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.8       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 174         |
|    n_updates            | 6280        |
|    policy_gradient_loss | -0.0815     |
|    std                  | 0.362       |
|    value_loss           | 487         |
-----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.79e-07   |
| reward_position         | 0.000134   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -60        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 316        |
|    time_elapsed         | 1423       |
|    total_timesteps      | 323584     |
| train/                  |            |
|    approx_kl            | 0.08981447 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | 178        |
|    n_updates            | 6300       |
|    policy_gradient_loss | -0.0756    |
|    std                  | 0.362      |
|    value_loss           | 421        |
----------------------------------------
Num timesteps: 324000
Best mean reward: 791.71 - Last mean reward per episode: -59.96
----------------------------------------
| reward                  | -2.88      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.83e-07   |
| reward_position         | 0.000135   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -64.2      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 317        |
|    time_elapsed         | 1428       |
|    total_timesteps      | 324608     |
| train/                  |            |
|    approx_kl            | 0.07262496 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 192        |
|    n_updates            | 6320       |
|    policy_gradient_loss | -0.0625    |
|    std                  | 0.362      |
|    value_loss           | 495        |
----------------------------------------
----------------------------------------
| reward                  | -2.88      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.37e-05   |
| reward_motion           | 8.8e-07    |
| reward_position         | 0.000134   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -58.4      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 318        |
|    time_elapsed         | 1433       |
|    total_timesteps      | 325632     |
| train/                  |            |
|    approx_kl            | 0.08142903 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 228        |
|    n_updates            | 6340       |
|    policy_gradient_loss | -0.073     |
|    std                  | 0.362      |
|    value_loss           | 393        |
----------------------------------------
-----------------------------------------
| reward                  | -2.88       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.36e-05    |
| reward_motion           | 8.78e-07    |
| reward_position         | 0.000134    |
| reward_torque           | -3.61       |
| reward_velocity         | 0.777       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -57.9       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 319         |
|    time_elapsed         | 1437        |
|    total_timesteps      | 326656      |
| train/                  |             |
|    approx_kl            | 0.057203952 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.4       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | 149         |
|    n_updates            | 6360        |
|    policy_gradient_loss | -0.0562     |
|    std                  | 0.362       |
|    value_loss           | 307         |
-----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.38e-05   |
| reward_motion           | 8.8e-07    |
| reward_position         | 0.000134   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.777      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -61.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 320        |
|    time_elapsed         | 1442       |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.07968338 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 106        |
|    n_updates            | 6380       |
|    policy_gradient_loss | -0.0626    |
|    std                  | 0.362      |
|    value_loss           | 355        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.33e-05   |
| reward_motion           | 8.71e-07   |
| reward_position         | 0.000133   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.779      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -63.8      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 321        |
|    time_elapsed         | 1447       |
|    total_timesteps      | 328704     |
| train/                  |            |
|    approx_kl            | 0.07895845 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 251        |
|    n_updates            | 6400       |
|    policy_gradient_loss | -0.064     |
|    std                  | 0.362      |
|    value_loss           | 415        |
----------------------------------------
-----------------------------------------
| reward                  | -2.88       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.2e-05     |
| reward_motion           | 8.5e-07     |
| reward_position         | 0.00013     |
| reward_torque           | -3.62       |
| reward_velocity         | 0.779       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -62.5       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 322         |
|    time_elapsed         | 1451        |
|    total_timesteps      | 329728      |
| train/                  |             |
|    approx_kl            | 0.082525745 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | 76.1        |
|    n_updates            | 6420        |
|    policy_gradient_loss | -0.0713     |
|    std                  | 0.362       |
|    value_loss           | 509         |
-----------------------------------------
Num timesteps: 330000
Best mean reward: 791.71 - Last mean reward per episode: -62.45
----------------------------------------
| reward                  | -2.88      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.25e-05   |
| reward_motion           | 8.56e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.779      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -66.3      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 323        |
|    time_elapsed         | 1456       |
|    total_timesteps      | 330752     |
| train/                  |            |
|    approx_kl            | 0.08526311 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 186        |
|    n_updates            | 6440       |
|    policy_gradient_loss | -0.0714    |
|    std                  | 0.362      |
|    value_loss           | 503        |
----------------------------------------
---------------------------------------
| reward                  | -2.88     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.24e-05  |
| reward_motion           | 8.57e-07  |
| reward_position         | 0.000131  |
| reward_torque           | -3.61     |
| reward_velocity         | 0.776     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -65.6     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 324       |
|    time_elapsed         | 1460      |
|    total_timesteps      | 331776    |
| train/                  |           |
|    approx_kl            | 0.0881749 |
|    clip_fraction        | 0.222     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.98      |
|    learning_rate        | 0.0003    |
|    loss                 | 281       |
|    n_updates            | 6460      |
|    policy_gradient_loss | -0.0783   |
|    std                  | 0.362     |
|    value_loss           | 405       |
---------------------------------------
----------------------------------------
| reward                  | -2.88      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.61e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -68.3      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 325        |
|    time_elapsed         | 1465       |
|    total_timesteps      | 332800     |
| train/                  |            |
|    approx_kl            | 0.08563089 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 46.2       |
|    n_updates            | 6480       |
|    policy_gradient_loss | -0.0872    |
|    std                  | 0.362      |
|    value_loss           | 386        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.28e-05   |
| reward_motion           | 8.6e-07    |
| reward_position         | 0.000131   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -68.1      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 326        |
|    time_elapsed         | 1469       |
|    total_timesteps      | 333824     |
| train/                  |            |
|    approx_kl            | 0.06256071 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.9      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 85.2       |
|    n_updates            | 6500       |
|    policy_gradient_loss | -0.0566    |
|    std                  | 0.362      |
|    value_loss           | 392        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.28e-05   |
| reward_motion           | 8.6e-07    |
| reward_position         | 0.000131   |
| reward_torque           | -3.61      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -72.1      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 327        |
|    time_elapsed         | 1473       |
|    total_timesteps      | 334848     |
| train/                  |            |
|    approx_kl            | 0.08924619 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 51.8       |
|    n_updates            | 6520       |
|    policy_gradient_loss | -0.0898    |
|    std                  | 0.362      |
|    value_loss           | 366        |
----------------------------------------
----------------------------------------
| reward                  | -2.89      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.26e-05   |
| reward_motion           | 8.59e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.772      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -74.9      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 328        |
|    time_elapsed         | 1478       |
|    total_timesteps      | 335872     |
| train/                  |            |
|    approx_kl            | 0.09193449 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 96.4       |
|    n_updates            | 6540       |
|    policy_gradient_loss | -0.0824    |
|    std                  | 0.362      |
|    value_loss           | 319        |
----------------------------------------
Num timesteps: 336000
Best mean reward: 791.71 - Last mean reward per episode: -74.94
----------------------------------------
| reward                  | -2.9       |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.59e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -74        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 329        |
|    time_elapsed         | 1482       |
|    total_timesteps      | 336896     |
| train/                  |            |
|    approx_kl            | 0.08618444 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | 68.5       |
|    n_updates            | 6560       |
|    policy_gradient_loss | -0.0702    |
|    std                  | 0.362      |
|    value_loss           | 396        |
----------------------------------------
----------------------------------------
| reward                  | -2.9       |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.28e-05   |
| reward_motion           | 8.61e-07   |
| reward_position         | 0.000132   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -67.4      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 330        |
|    time_elapsed         | 1486       |
|    total_timesteps      | 337920     |
| train/                  |            |
|    approx_kl            | 0.05716681 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 128        |
|    n_updates            | 6580       |
|    policy_gradient_loss | -0.0634    |
|    std                  | 0.362      |
|    value_loss           | 581        |
----------------------------------------
----------------------------------------
| reward                  | -2.9       |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.27e-05   |
| reward_motion           | 8.61e-07   |
| reward_position         | 0.000131   |
| reward_torque           | -3.62      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -70.8      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 331        |
|    time_elapsed         | 1491       |
|    total_timesteps      | 338944     |
| train/                  |            |
|    approx_kl            | 0.09605715 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 164        |
|    n_updates            | 6600       |
|    policy_gradient_loss | -0.0876    |
|    std                  | 0.362      |
|    value_loss           | 537        |
----------------------------------------
----------------------------------------
| reward                  | -2.9       |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.15e-05   |
| reward_motion           | 8.39e-07   |
| reward_position         | 0.000128   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.769      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -68.9      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 332        |
|    time_elapsed         | 1495       |
|    total_timesteps      | 339968     |
| train/                  |            |
|    approx_kl            | 0.08115013 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 228        |
|    n_updates            | 6620       |
|    policy_gradient_loss | -0.071     |
|    std                  | 0.362      |
|    value_loss           | 578        |
----------------------------------------
---------------------------------------
| reward                  | -2.91     |
| reward_contact          | -0.047    |
| reward_ctrl             | 4.17e-05  |
| reward_motion           | 8.39e-07  |
| reward_position         | 0.000128  |
| reward_torque           | -3.63     |
| reward_velocity         | 0.767     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -70.2     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 333       |
|    time_elapsed         | 1500      |
|    total_timesteps      | 340992    |
| train/                  |           |
|    approx_kl            | 0.0719579 |
|    clip_fraction        | 0.162     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | 112       |
|    n_updates            | 6640      |
|    policy_gradient_loss | -0.0657   |
|    std                  | 0.362     |
|    value_loss           | 449       |
---------------------------------------
Num timesteps: 342000
Best mean reward: 791.71 - Last mean reward per episode: -70.20
----------------------------------------
| reward                  | -2.91      |
| reward_contact          | -0.047     |
| reward_ctrl             | 4.18e-05   |
| reward_motion           | 8.44e-07   |
| reward_position         | 0.000129   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.767      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -72.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 334        |
|    time_elapsed         | 1504       |
|    total_timesteps      | 342016     |
| train/                  |            |
|    approx_kl            | 0.07477051 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 134        |
|    n_updates            | 6660       |
|    policy_gradient_loss | -0.0692    |
|    std                  | 0.362      |
|    value_loss           | 357        |
----------------------------------------
-----------------------------------------
| reward                  | -2.91       |
| reward_contact          | -0.047      |
| reward_ctrl             | 4.05e-05    |
| reward_motion           | 8.18e-07    |
| reward_position         | 0.000125    |
| reward_torque           | -3.63       |
| reward_velocity         | 0.767       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -82.2       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 335         |
|    time_elapsed         | 1509        |
|    total_timesteps      | 343040      |
| train/                  |             |
|    approx_kl            | 0.038849957 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.7       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 166         |
|    n_updates            | 6680        |
|    policy_gradient_loss | -0.0445     |
|    std                  | 0.362       |
|    value_loss           | 342         |
-----------------------------------------
----------------------------------------
| reward                  | -2.92      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.94e-05   |
| reward_motion           | 8e-07      |
| reward_position         | 0.000122   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.764      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -83.1      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 336        |
|    time_elapsed         | 1513       |
|    total_timesteps      | 344064     |
| train/                  |            |
|    approx_kl            | 0.08295447 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | 358        |
|    n_updates            | 6700       |
|    policy_gradient_loss | -0.0803    |
|    std                  | 0.362      |
|    value_loss           | 511        |
----------------------------------------
----------------------------------------
| reward                  | -2.91      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.93e-05   |
| reward_motion           | 7.99e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -85.1      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 337        |
|    time_elapsed         | 1518       |
|    total_timesteps      | 345088     |
| train/                  |            |
|    approx_kl            | 0.10497835 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | 307        |
|    n_updates            | 6720       |
|    policy_gradient_loss | -0.0865    |
|    std                  | 0.362      |
|    value_loss           | 490        |
----------------------------------------
----------------------------------------
| reward                  | -2.91      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.94e-05   |
| reward_motion           | 7.99e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.761      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -89.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 338        |
|    time_elapsed         | 1522       |
|    total_timesteps      | 346112     |
| train/                  |            |
|    approx_kl            | 0.11712283 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | 89.1       |
|    n_updates            | 6740       |
|    policy_gradient_loss | -0.0746    |
|    std                  | 0.362      |
|    value_loss           | 385        |
----------------------------------------
----------------------------------------
| reward                  | -2.91      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.96e-05   |
| reward_motion           | 8.08e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.63      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -81.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 339        |
|    time_elapsed         | 1527       |
|    total_timesteps      | 347136     |
| train/                  |            |
|    approx_kl            | 0.10852885 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | 129        |
|    n_updates            | 6760       |
|    policy_gradient_loss | -0.0829    |
|    std                  | 0.362      |
|    value_loss           | 504        |
----------------------------------------
Num timesteps: 348000
Best mean reward: 791.71 - Last mean reward per episode: -81.56
----------------------------------------
| reward                  | -2.92      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.85e-05   |
| reward_motion           | 7.89e-07   |
| reward_position         | 0.000121   |
| reward_torque           | -3.64      |
| reward_velocity         | 0.763      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -75.5      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 340        |
|    time_elapsed         | 1531       |
|    total_timesteps      | 348160     |
| train/                  |            |
|    approx_kl            | 0.06358122 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | 376        |
|    n_updates            | 6780       |
|    policy_gradient_loss | -0.0604    |
|    std                  | 0.362      |
|    value_loss           | 440        |
----------------------------------------
----------------------------------------
| reward                  | -2.92      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.88e-05   |
| reward_motion           | 7.97e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.64      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -74.5      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 341        |
|    time_elapsed         | 1536       |
|    total_timesteps      | 349184     |
| train/                  |            |
|    approx_kl            | 0.08850199 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 166        |
|    n_updates            | 6800       |
|    policy_gradient_loss | -0.0753    |
|    std                  | 0.362      |
|    value_loss           | 465        |
----------------------------------------
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.047      |
| reward_ctrl             | 3.9e-05     |
| reward_motion           | 7.97e-07    |
| reward_position         | 0.000122    |
| reward_torque           | -3.65       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -74.4       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 342         |
|    time_elapsed         | 1540        |
|    total_timesteps      | 350208      |
| train/                  |             |
|    approx_kl            | 0.098224916 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 117         |
|    n_updates            | 6820        |
|    policy_gradient_loss | -0.0764     |
|    std                  | 0.362       |
|    value_loss           | 438         |
-----------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.93e-05   |
| reward_motion           | 7.99e-07   |
| reward_position         | 0.000122   |
| reward_torque           | -3.66      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -77.6      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 343        |
|    time_elapsed         | 1544       |
|    total_timesteps      | 351232     |
| train/                  |            |
|    approx_kl            | 0.10913843 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 98         |
|    n_updates            | 6840       |
|    policy_gradient_loss | -0.0914    |
|    std                  | 0.362      |
|    value_loss           | 389        |
----------------------------------------
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.92e-05    |
| reward_motion           | 7.98e-07    |
| reward_position         | 0.000122    |
| reward_torque           | -3.65       |
| reward_velocity         | 0.766       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -75.3       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 344         |
|    time_elapsed         | 1549        |
|    total_timesteps      | 352256      |
| train/                  |             |
|    approx_kl            | 0.086836606 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 214         |
|    n_updates            | 6860        |
|    policy_gradient_loss | -0.0805     |
|    std                  | 0.362       |
|    value_loss           | 413         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.92e-05    |
| reward_motion           | 7.99e-07    |
| reward_position         | 0.000122    |
| reward_torque           | -3.65       |
| reward_velocity         | 0.767       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -69.6       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 345         |
|    time_elapsed         | 1553        |
|    total_timesteps      | 353280      |
| train/                  |             |
|    approx_kl            | 0.060340073 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 77.9        |
|    n_updates            | 6880        |
|    policy_gradient_loss | -0.0478     |
|    std                  | 0.362       |
|    value_loss           | 399         |
-----------------------------------------
Num timesteps: 354000
Best mean reward: 791.71 - Last mean reward per episode: -69.57
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.94e-05    |
| reward_motion           | 8.01e-07    |
| reward_position         | 0.000123    |
| reward_torque           | -3.65       |
| reward_velocity         | 0.77        |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -76.1       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 346         |
|    time_elapsed         | 1557        |
|    total_timesteps      | 354304      |
| train/                  |             |
|    approx_kl            | 0.095809594 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 147         |
|    n_updates            | 6900        |
|    policy_gradient_loss | -0.0825     |
|    std                  | 0.362       |
|    value_loss           | 411         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 4e-05       |
| reward_motion           | 8.13e-07    |
| reward_position         | 0.000124    |
| reward_torque           | -3.66       |
| reward_velocity         | 0.772       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -77.8       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 347         |
|    time_elapsed         | 1562        |
|    total_timesteps      | 355328      |
| train/                  |             |
|    approx_kl            | 0.097013265 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 72.5        |
|    n_updates            | 6920        |
|    policy_gradient_loss | -0.0865     |
|    std                  | 0.362       |
|    value_loss           | 374         |
-----------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.98e-05   |
| reward_motion           | 8.12e-07   |
| reward_position         | 0.000124   |
| reward_torque           | -3.66      |
| reward_velocity         | 0.773      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -85.3      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 348        |
|    time_elapsed         | 1566       |
|    total_timesteps      | 356352     |
| train/                  |            |
|    approx_kl            | 0.06848779 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | 94.4       |
|    n_updates            | 6940       |
|    policy_gradient_loss | -0.0646    |
|    std                  | 0.362      |
|    value_loss           | 434        |
----------------------------------------
---------------------------------------
| reward                  | -2.93     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 3.97e-05  |
| reward_motion           | 8.1e-07   |
| reward_position         | 0.000124  |
| reward_torque           | -3.66     |
| reward_velocity         | 0.772     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -85       |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 349       |
|    time_elapsed         | 1571      |
|    total_timesteps      | 357376    |
| train/                  |           |
|    approx_kl            | 0.0612828 |
|    clip_fraction        | 0.178     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.978     |
|    learning_rate        | 0.0003    |
|    loss                 | 118       |
|    n_updates            | 6960      |
|    policy_gradient_loss | -0.0699   |
|    std                  | 0.362     |
|    value_loss           | 382       |
---------------------------------------
---------------------------------------
| reward                  | -2.93     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 3.9e-05   |
| reward_motion           | 7.98e-07  |
| reward_position         | 0.000122  |
| reward_torque           | -3.66     |
| reward_velocity         | 0.775     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -90.5     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 350       |
|    time_elapsed         | 1575      |
|    total_timesteps      | 358400    |
| train/                  |           |
|    approx_kl            | 0.0758909 |
|    clip_fraction        | 0.174     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.1     |
|    explained_variance   | 0.976     |
|    learning_rate        | 0.0003    |
|    loss                 | 124       |
|    n_updates            | 6980      |
|    policy_gradient_loss | -0.0733   |
|    std                  | 0.362     |
|    value_loss           | 488       |
---------------------------------------
-----------------------------------------
| reward                  | -2.93       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.8e-05     |
| reward_motion           | 7.85e-07    |
| reward_position         | 0.00012     |
| reward_torque           | -3.66       |
| reward_velocity         | 0.777       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -94.4       |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 351         |
|    time_elapsed         | 1579        |
|    total_timesteps      | 359424      |
| train/                  |             |
|    approx_kl            | 0.101735264 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.2       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 55.7        |
|    n_updates            | 7000        |
|    policy_gradient_loss | -0.0952     |
|    std                  | 0.362       |
|    value_loss           | 491         |
-----------------------------------------
Num timesteps: 360000
Best mean reward: 791.71 - Last mean reward per episode: -94.37
---------------------------------------
| reward                  | -2.93     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 3.77e-05  |
| reward_motion           | 7.8e-07   |
| reward_position         | 0.000119  |
| reward_torque           | -3.66     |
| reward_velocity         | 0.778     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -89.2     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 352       |
|    time_elapsed         | 1584      |
|    total_timesteps      | 360448    |
| train/                  |           |
|    approx_kl            | 0.1101183 |
|    clip_fraction        | 0.254     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.1     |
|    explained_variance   | 0.981     |
|    learning_rate        | 0.0003    |
|    loss                 | 89.5      |
|    n_updates            | 7020      |
|    policy_gradient_loss | -0.0816   |
|    std                  | 0.362     |
|    value_loss           | 406       |
---------------------------------------
----------------------------------------
| reward                  | -2.93      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.77e-05   |
| reward_motion           | 7.78e-07   |
| reward_position         | 0.000119   |
| reward_torque           | -3.66      |
| reward_velocity         | 0.78       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -91.3      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 353        |
|    time_elapsed         | 1588       |
|    total_timesteps      | 361472     |
| train/                  |            |
|    approx_kl            | 0.07463523 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 59.8       |
|    n_updates            | 7040       |
|    policy_gradient_loss | -0.0775    |
|    std                  | 0.362      |
|    value_loss           | 331        |
----------------------------------------
----------------------------------------
| reward                  | -2.93      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 3.72e-05   |
| reward_motion           | 7.66e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -3.67      |
| reward_velocity         | 0.783      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -92        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 354        |
|    time_elapsed         | 1593       |
|    total_timesteps      | 362496     |
| train/                  |            |
|    approx_kl            | 0.09484478 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 41.7       |
|    n_updates            | 7060       |
|    policy_gradient_loss | -0.0837    |
|    std                  | 0.362      |
|    value_loss           | 385        |
----------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.73e-05   |
| reward_motion           | 7.69e-07   |
| reward_position         | 0.000118   |
| reward_torque           | -3.67      |
| reward_velocity         | 0.78       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -96        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 355        |
|    time_elapsed         | 1598       |
|    total_timesteps      | 363520     |
| train/                  |            |
|    approx_kl            | 0.07948056 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.7      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | 230        |
|    n_updates            | 7080       |
|    policy_gradient_loss | -0.085     |
|    std                  | 0.362      |
|    value_loss           | 342        |
----------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.68e-05   |
| reward_motion           | 7.62e-07   |
| reward_position         | 0.000117   |
| reward_torque           | -3.67      |
| reward_velocity         | 0.778      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -92.5      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 356        |
|    time_elapsed         | 1602       |
|    total_timesteps      | 364544     |
| train/                  |            |
|    approx_kl            | 0.09544092 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 29.2       |
|    n_updates            | 7100       |
|    policy_gradient_loss | -0.0886    |
|    std                  | 0.362      |
|    value_loss           | 335        |
----------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.66e-05   |
| reward_motion           | 7.58e-07   |
| reward_position         | 0.000116   |
| reward_torque           | -3.67      |
| reward_velocity         | 0.779      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -97.9      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 357        |
|    time_elapsed         | 1607       |
|    total_timesteps      | 365568     |
| train/                  |            |
|    approx_kl            | 0.07863753 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | 57.6       |
|    n_updates            | 7120       |
|    policy_gradient_loss | -0.0727    |
|    std                  | 0.362      |
|    value_loss           | 478        |
----------------------------------------
Num timesteps: 366000
Best mean reward: 791.71 - Last mean reward per episode: -97.89
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0471    |
| reward_ctrl             | 3.66e-05   |
| reward_motion           | 7.57e-07   |
| reward_position         | 0.000116   |
| reward_torque           | -3.68      |
| reward_velocity         | 0.78       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -105       |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 358        |
|    time_elapsed         | 1611       |
|    total_timesteps      | 366592     |
| train/                  |            |
|    approx_kl            | 0.09423084 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | 226        |
|    n_updates            | 7140       |
|    policy_gradient_loss | -0.0858    |
|    std                  | 0.362      |
|    value_loss           | 437        |
----------------------------------------
---------------------------------------
| reward                  | -2.94     |
| reward_contact          | -0.0472   |
| reward_ctrl             | 3.63e-05  |
| reward_motion           | 7.54e-07  |
| reward_position         | 0.000116  |
| reward_torque           | -3.67     |
| reward_velocity         | 0.781     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -108      |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 359       |
|    time_elapsed         | 1616      |
|    total_timesteps      | 367616    |
| train/                  |           |
|    approx_kl            | 0.1212948 |
|    clip_fraction        | 0.262     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21       |
|    explained_variance   | 0.972     |
|    learning_rate        | 0.0003    |
|    loss                 | 97.7      |
|    n_updates            | 7160      |
|    policy_gradient_loss | -0.0898   |
|    std                  | 0.362     |
|    value_loss           | 416       |
---------------------------------------
----------------------------------------
| reward                  | -2.94      |
| reward_contact          | -0.0472    |
| reward_ctrl             | 3.65e-05   |
| reward_motion           | 7.55e-07   |
| reward_position         | 0.000116   |
| reward_torque           | -3.67      |
| reward_velocity         | 0.78       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -99        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 360        |
|    time_elapsed         | 1620       |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.10194968 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | 238        |
|    n_updates            | 7180       |
|    policy_gradient_loss | -0.0818    |
|    std                  | 0.362      |
|    value_loss           | 470        |
----------------------------------------
-----------------------------------------
| reward                  | -2.95       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.7e-05     |
| reward_motion           | 7.61e-07    |
| reward_position         | 0.000117    |
| reward_torque           | -3.68       |
| reward_velocity         | 0.781       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -100        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 361         |
|    time_elapsed         | 1624        |
|    total_timesteps      | 369664      |
| train/                  |             |
|    approx_kl            | 0.096999794 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 41          |
|    n_updates            | 7200        |
|    policy_gradient_loss | -0.0912     |
|    std                  | 0.362       |
|    value_loss           | 318         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.94       |
| reward_contact          | -0.0472     |
| reward_ctrl             | 3.68e-05    |
| reward_motion           | 7.53e-07    |
| reward_position         | 0.000115    |
| reward_torque           | -3.68       |
| reward_velocity         | 0.778       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -104        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 362         |
|    time_elapsed         | 1629        |
|    total_timesteps      | 370688      |
| train/                  |             |
|    approx_kl            | 0.081590526 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 51          |
|    n_updates            | 7220        |
|    policy_gradient_loss | -0.071      |
|    std                  | 0.362       |
|    value_loss           | 373         |
-----------------------------------------
---------------------------------------
| reward                  | -2.95     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 3.76e-05  |
| reward_motion           | 7.68e-07  |
| reward_position         | 0.000118  |
| reward_torque           | -3.68     |
| reward_velocity         | 0.778     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -112      |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 363       |
|    time_elapsed         | 1633      |
|    total_timesteps      | 371712    |
| train/                  |           |
|    approx_kl            | 0.0668627 |
|    clip_fraction        | 0.179     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.7     |
|    explained_variance   | 0.985     |
|    learning_rate        | 0.0003    |
|    loss                 | 44.1      |
|    n_updates            | 7240      |
|    policy_gradient_loss | -0.0582   |
|    std                  | 0.362     |
|    value_loss           | 240       |
---------------------------------------
Num timesteps: 372000
Best mean reward: 791.71 - Last mean reward per episode: -112.40
-----------------------------------------
| reward                  | -2.96       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.72e-05    |
| reward_motion           | 7.62e-07    |
| reward_position         | 0.000117    |
| reward_torque           | -3.69       |
| reward_velocity         | 0.778       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -113        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 364         |
|    time_elapsed         | 1637        |
|    total_timesteps      | 372736      |
| train/                  |             |
|    approx_kl            | 0.094104856 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 69.6        |
|    n_updates            | 7260        |
|    policy_gradient_loss | -0.0816     |
|    std                  | 0.361       |
|    value_loss           | 385         |
-----------------------------------------
-----------------------------------------
| reward                  | -2.96       |
| reward_contact          | -0.0471     |
| reward_ctrl             | 3.71e-05    |
| reward_motion           | 7.59e-07    |
| reward_position         | 0.000116    |
| reward_torque           | -3.69       |
| reward_velocity         | 0.776       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -123        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 365         |
|    time_elapsed         | 1642        |
|    total_timesteps      | 373760      |
| train/                  |             |
|    approx_kl            | 0.049591344 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.8       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | 182         |
|    n_updates            | 7280        |
|    policy_gradient_loss | -0.0509     |
|    std                  | 0.361       |
|    value_loss           | 347         |
-----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Traceback (most recent call last):
  File "ddpg.py", line 224, in <module>
    'info_keywords' : info_kwargs
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/env_util.py", line 105, in make_vec_env
    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/env_util.py", line 80, in _init
    env = gym.make(env_id, **env_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 145, in make
    return registry.make(id, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 90, in make
    env = spec.make(**kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 60, in make
    env = cls(**_kwargs)
  File "/home/ubuntu/AntController/src/simulations/gym/ant.py", line 977, in __init__
    super(AntEnvV8, self).__init__('ant.xml')
  File "/home/ubuntu/AntController/src/simulations/gym/ant.py", line 467, in __init__
    super(AntEnvV4, self).__init__(path)
  File "/home/ubuntu/AntController/src/simulations/gym/ant.py", line 84, in __init__
    mujoco_env.MujocoEnv.__init__(self, path, 5)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/mujoco/mujoco_env.py", line 64, in __init__
    observation, _reward, done, _info = self.step(action)
  File "/home/ubuntu/AntController/src/simulations/gym/ant.py", line 1086, in step
    reward = info['reward_ctrl'] + info['reward_torque'] + info['reward_motion'] + info['reward_position'] + info['reward_velocity'] + info['reward_contact']
KeyError: 'reward_position'
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_4
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.02e+03  |
|    ep_rew_mean     | -1.68e+03 |
| time/              |           |
|    fps             | 501       |
|    iterations      | 1         |
|    time_elapsed    | 2         |
|    total_timesteps | 1024      |
----------------------------------
----------------------------------------
| reward                  | -1.59      |
| reward_contact          | -0.0475    |
| reward_ctrl             | 1.48e-06   |
| reward_motion           | 3.8e-07    |
| reward_torque           | -2.33      |
| reward_velocity         | 0.787      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -1.68e+03  |
| time/                   |            |
|    fps                  | 408        |
|    iterations           | 2          |
|    time_elapsed         | 5          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.36854783 |
|    clip_fraction        | 0.502      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.6      |
|    explained_variance   | -0.00194   |
|    learning_rate        | 0.0003     |
|    loss                 | 1.23       |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.364      |
|    value_loss           | 27.3       |
----------------------------------------
---------------------------------------
| reward                  | -1.72     |
| reward_contact          | -0.0474   |
| reward_ctrl             | 5.05e-06  |
| reward_motion           | 1.15e-06  |
| reward_torque           | -2.54     |
| reward_velocity         | 0.869     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -1.97e+03 |
| time/                   |           |
|    fps                  | 384       |
|    iterations           | 3         |
|    time_elapsed         | 7         |
|    total_timesteps      | 3072      |
| train/                  |           |
|    approx_kl            | 0.3725308 |
|    clip_fraction        | 0.329     |
|    clip_range           | 0.4       |
|    entropy_loss         | -14.8     |
|    explained_variance   | 0.0732    |
|    learning_rate        | 0.0003    |
|    loss                 | 2.83      |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0945   |
|    std                  | 0.362     |
|    value_loss           | 27.4      |
---------------------------------------
---------------------------------------
| reward                  | -2.26     |
| reward_contact          | -0.0471   |
| reward_ctrl             | 3.95e-06  |
| reward_motion           | 9.14e-07  |
| reward_torque           | -3.03     |
| reward_velocity         | 0.814     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.03e+03 |
| time/                   |           |
|    fps                  | 373       |
|    iterations           | 4         |
|    time_elapsed         | 10        |
|    total_timesteps      | 4096      |
| train/                  |           |
|    approx_kl            | 0.133129  |
|    clip_fraction        | 0.281     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19       |
|    explained_variance   | 0.3       |
|    learning_rate        | 0.0003    |
|    loss                 | 2.01      |
|    n_updates            | 60        |
|    policy_gradient_loss | -2.98e-05 |
|    std                  | 0.362     |
|    value_loss           | 126       |
---------------------------------------
----------------------------------------
| reward                  | -2.3       |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.5e-06    |
| reward_motion           | 8.15e-07   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.839      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.06e+03  |
| time/                   |            |
|    fps                  | 367        |
|    iterations           | 5          |
|    time_elapsed         | 13         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.20465791 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.8      |
|    explained_variance   | 0.322      |
|    learning_rate        | 0.0003     |
|    loss                 | 19.7       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0417    |
|    std                  | 0.361      |
|    value_loss           | 66.6       |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -2059.00
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -2.12      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 4.93e-06   |
| reward_motion           | 1.06e-06   |
| reward_torque           | -2.92      |
| reward_velocity         | 0.849      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.13e+03  |
| time/                   |            |
|    fps                  | 362        |
|    iterations           | 6          |
|    time_elapsed         | 16         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.11940461 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.7      |
|    explained_variance   | 0.379      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.92       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0845    |
|    std                  | 0.36       |
|    value_loss           | 29.1       |
----------------------------------------
----------------------------------------
| reward                  | -2.19      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.4e-06    |
| reward_motion           | 9.54e-07   |
| reward_torque           | -2.98      |
| reward_velocity         | 0.837      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.21e+03  |
| time/                   |            |
|    fps                  | 359        |
|    iterations           | 7          |
|    time_elapsed         | 19         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.09918121 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.3      |
|    explained_variance   | 0.224      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.8        |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0694    |
|    std                  | 0.359      |
|    value_loss           | 40.8       |
----------------------------------------
----------------------------------------
| reward                  | -2.26      |
| reward_contact          | -0.0466    |
| reward_ctrl             | 3.99e-06   |
| reward_motion           | 8.61e-07   |
| reward_torque           | -3.02      |
| reward_velocity         | 0.806      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.24e+03  |
| time/                   |            |
|    fps                  | 357        |
|    iterations           | 8          |
|    time_elapsed         | 22         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.04906732 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.3      |
|    explained_variance   | -1.03      |
|    learning_rate        | 0.0003     |
|    loss                 | 23.1       |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0508    |
|    std                  | 0.359      |
|    value_loss           | 50.8       |
----------------------------------------
----------------------------------------
| reward                  | -2.32      |
| reward_contact          | -0.0465    |
| reward_ctrl             | 3.78e-06   |
| reward_motion           | 8.16e-07   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.815      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.19e+03  |
| time/                   |            |
|    fps                  | 356        |
|    iterations           | 9          |
|    time_elapsed         | 25         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.09477489 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | 0.219      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.31       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0674    |
|    std                  | 0.358      |
|    value_loss           | 39.8       |
----------------------------------------
----------------------------------------
| reward                  | -2.32      |
| reward_contact          | -0.0467    |
| reward_ctrl             | 3.46e-06   |
| reward_motion           | 7.53e-07   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.822      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.18e+03  |
| time/                   |            |
|    fps                  | 354        |
|    iterations           | 10         |
|    time_elapsed         | 28         |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.64199543 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.4        |
|    entropy_loss         | -14.6      |
|    explained_variance   | 0.0344     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.3        |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0534    |
|    std                  | 0.357      |
|    value_loss           | 47.2       |
----------------------------------------
----------------------------------------
| reward                  | -2.24      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 3.2e-06    |
| reward_motion           | 7.02e-07   |
| reward_torque           | -3.02      |
| reward_velocity         | 0.822      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.2e+03   |
| time/                   |            |
|    fps                  | 353        |
|    iterations           | 11         |
|    time_elapsed         | 31         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.22036704 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.8      |
|    explained_variance   | 0.164      |
|    learning_rate        | 0.0003     |
|    loss                 | 35         |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0688    |
|    std                  | 0.355      |
|    value_loss           | 61.6       |
----------------------------------------
Num timesteps: 12000
Best mean reward: -2059.00 - Last mean reward per episode: -2203.10
----------------------------------------
| reward                  | -2.34      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.1e-06    |
| reward_motion           | 6.88e-07   |
| reward_torque           | -3.11      |
| reward_velocity         | 0.812      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.21e+03  |
| time/                   |            |
|    fps                  | 353        |
|    iterations           | 12         |
|    time_elapsed         | 34         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.16806449 |
|    clip_fraction        | 0.382      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.6      |
|    explained_variance   | 0.0587     |
|    learning_rate        | 0.0003     |
|    loss                 | 11.6       |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0739    |
|    std                  | 0.355      |
|    value_loss           | 56.9       |
----------------------------------------
----------------------------------------
| reward                  | -2.33      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 3.13e-06   |
| reward_motion           | 7.12e-07   |
| reward_torque           | -3.09      |
| reward_velocity         | 0.8        |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.2e+03   |
| time/                   |            |
|    fps                  | 352        |
|    iterations           | 13         |
|    time_elapsed         | 37         |
|    total_timesteps      | 13312      |
| train/                  |            |
|    approx_kl            | 0.22779572 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.2      |
|    explained_variance   | -0.00966   |
|    learning_rate        | 0.0003     |
|    loss                 | 30         |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.066     |
|    std                  | 0.355      |
|    value_loss           | 70.9       |
----------------------------------------
----------------------------------------
| reward                  | -2.29      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 2.98e-06   |
| reward_motion           | 6.79e-07   |
| reward_torque           | -3.06      |
| reward_velocity         | 0.813      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.19e+03  |
| time/                   |            |
|    fps                  | 351        |
|    iterations           | 14         |
|    time_elapsed         | 40         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.25915498 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.7      |
|    explained_variance   | 0.0405     |
|    learning_rate        | 0.0003     |
|    loss                 | 5.39       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.043     |
|    std                  | 0.354      |
|    value_loss           | 75.4       |
----------------------------------------
---------------------------------------
| reward                  | -2.26     |
| reward_contact          | -0.047    |
| reward_ctrl             | 2.97e-06  |
| reward_motion           | 6.71e-07  |
| reward_torque           | -3.04     |
| reward_velocity         | 0.82      |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.2e+03  |
| time/                   |           |
|    fps                  | 351       |
|    iterations           | 15        |
|    time_elapsed         | 43        |
|    total_timesteps      | 15360     |
| train/                  |           |
|    approx_kl            | 0.2859522 |
|    clip_fraction        | 0.461     |
|    clip_range           | 0.4       |
|    entropy_loss         | -17.6     |
|    explained_variance   | -0.0266   |
|    learning_rate        | 0.0003    |
|    loss                 | 5.41      |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.0304   |
|    std                  | 0.353     |
|    value_loss           | 74.5      |
---------------------------------------
----------------------------------------
| reward                  | -2.29      |
| reward_contact          | -0.047     |
| reward_ctrl             | 2.85e-06   |
| reward_motion           | 6.45e-07   |
| reward_torque           | -3.05      |
| reward_velocity         | 0.809      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.22e+03  |
| time/                   |            |
|    fps                  | 350        |
|    iterations           | 16         |
|    time_elapsed         | 46         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.18760654 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.4        |
|    entropy_loss         | -16.9      |
|    explained_variance   | -0.237     |
|    learning_rate        | 0.0003     |
|    loss                 | 21.8       |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0598    |
|    std                  | 0.352      |
|    value_loss           | 92.7       |
----------------------------------------
----------------------------------------
| reward                  | -2.3       |
| reward_contact          | -0.047     |
| reward_ctrl             | 2.8e-06    |
| reward_motion           | 6.31e-07   |
| reward_torque           | -3.06      |
| reward_velocity         | 0.806      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.22e+03  |
| time/                   |            |
|    fps                  | 350        |
|    iterations           | 17         |
|    time_elapsed         | 49         |
|    total_timesteps      | 17408      |
| train/                  |            |
|    approx_kl            | 0.29463354 |
|    clip_fraction        | 0.494      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.2      |
|    explained_variance   | 0.0503     |
|    learning_rate        | 0.0003     |
|    loss                 | 63.3       |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.049     |
|    std                  | 0.352      |
|    value_loss           | 102        |
----------------------------------------
Num timesteps: 18000
Best mean reward: -2059.00 - Last mean reward per episode: -2224.99
----------------------------------------
| reward                  | -2.32      |
| reward_contact          | -0.047     |
| reward_ctrl             | 2.81e-06   |
| reward_motion           | 6.25e-07   |
| reward_torque           | -3.07      |
| reward_velocity         | 0.802      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.24e+03  |
| time/                   |            |
|    fps                  | 349        |
|    iterations           | 18         |
|    time_elapsed         | 52         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.36696294 |
|    clip_fraction        | 0.429      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17        |
|    explained_variance   | 0.032      |
|    learning_rate        | 0.0003     |
|    loss                 | 63.1       |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0541    |
|    std                  | 0.351      |
|    value_loss           | 107        |
----------------------------------------
---------------------------------------
| reward                  | -2.37     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 3.16e-06  |
| reward_motion           | 6.89e-07  |
| reward_torque           | -3.12     |
| reward_velocity         | 0.8       |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.25e+03 |
| time/                   |           |
|    fps                  | 349       |
|    iterations           | 19        |
|    time_elapsed         | 55        |
|    total_timesteps      | 19456     |
| train/                  |           |
|    approx_kl            | 0.0930747 |
|    clip_fraction        | 0.264     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.3     |
|    explained_variance   | -0.0214   |
|    learning_rate        | 0.0003    |
|    loss                 | 75.1      |
|    n_updates            | 360       |
|    policy_gradient_loss | -0.0604   |
|    std                  | 0.351     |
|    value_loss           | 114       |
---------------------------------------
----------------------------------------
| reward                  | -2.38      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.11e-06   |
| reward_motion           | 6.82e-07   |
| reward_torque           | -3.13      |
| reward_velocity         | 0.794      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.26e+03  |
| time/                   |            |
|    fps                  | 349        |
|    iterations           | 20         |
|    time_elapsed         | 58         |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.21897617 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.4        |
|    entropy_loss         | -17.9      |
|    explained_variance   | 0.0717     |
|    learning_rate        | 0.0003     |
|    loss                 | 145        |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0625    |
|    std                  | 0.35       |
|    value_loss           | 123        |
----------------------------------------
----------------------------------------
| reward                  | -2.42      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.01e-06   |
| reward_motion           | 6.64e-07   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.796      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.28e+03  |
| time/                   |            |
|    fps                  | 348        |
|    iterations           | 21         |
|    time_elapsed         | 61         |
|    total_timesteps      | 21504      |
| train/                  |            |
|    approx_kl            | 0.44544753 |
|    clip_fraction        | 0.489      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.4      |
|    explained_variance   | 0.0266     |
|    learning_rate        | 0.0003     |
|    loss                 | 75.7       |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0516    |
|    std                  | 0.349      |
|    value_loss           | 129        |
----------------------------------------
----------------------------------------
| reward                  | -2.43      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.05e-06   |
| reward_motion           | 6.65e-07   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.792      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.3e+03   |
| time/                   |            |
|    fps                  | 348        |
|    iterations           | 22         |
|    time_elapsed         | 64         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.09976232 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.0835     |
|    learning_rate        | 0.0003     |
|    loss                 | 25.2       |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.0709    |
|    std                  | 0.348      |
|    value_loss           | 130        |
----------------------------------------
----------------------------------------
| reward                  | -2.42      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 2.96e-06   |
| reward_motion           | 6.5e-07    |
| reward_torque           | -3.17      |
| reward_velocity         | 0.791      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.31e+03  |
| time/                   |            |
|    fps                  | 348        |
|    iterations           | 23         |
|    time_elapsed         | 67         |
|    total_timesteps      | 23552      |
| train/                  |            |
|    approx_kl            | 0.23673081 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.0369     |
|    learning_rate        | 0.0003     |
|    loss                 | 120        |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0577    |
|    std                  | 0.348      |
|    value_loss           | 152        |
----------------------------------------
Num timesteps: 24000
Best mean reward: -2059.00 - Last mean reward per episode: -2307.34
---------------------------------------
| reward                  | -2.46     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 2.98e-06  |
| reward_motion           | 6.58e-07  |
| reward_torque           | -3.2      |
| reward_velocity         | 0.785     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.32e+03 |
| time/                   |           |
|    fps                  | 347       |
|    iterations           | 24        |
|    time_elapsed         | 70        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.8325364 |
|    clip_fraction        | 0.595     |
|    clip_range           | 0.4       |
|    entropy_loss         | -18.1     |
|    explained_variance   | 0.0506    |
|    learning_rate        | 0.0003    |
|    loss                 | 2.49      |
|    n_updates            | 460       |
|    policy_gradient_loss | -0.0374   |
|    std                  | 0.347     |
|    value_loss           | 157       |
---------------------------------------
---------------------------------------
| reward                  | -2.46     |
| reward_contact          | -0.047    |
| reward_ctrl             | 2.95e-06  |
| reward_motion           | 6.54e-07  |
| reward_torque           | -3.19     |
| reward_velocity         | 0.781     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.34e+03 |
| time/                   |           |
|    fps                  | 347       |
|    iterations           | 25        |
|    time_elapsed         | 73        |
|    total_timesteps      | 25600     |
| train/                  |           |
|    approx_kl            | 0.169721  |
|    clip_fraction        | 0.294     |
|    clip_range           | 0.4       |
|    entropy_loss         | -18.4     |
|    explained_variance   | 0.035     |
|    learning_rate        | 0.0003    |
|    loss                 | 21.3      |
|    n_updates            | 480       |
|    policy_gradient_loss | -0.0566   |
|    std                  | 0.347     |
|    value_loss           | 166       |
---------------------------------------
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.047     |
| reward_ctrl             | 2.95e-06   |
| reward_motion           | 6.46e-07   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.34e+03  |
| time/                   |            |
|    fps                  | 347        |
|    iterations           | 26         |
|    time_elapsed         | 76         |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.18979153 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.8      |
|    explained_variance   | 0.0987     |
|    learning_rate        | 0.0003     |
|    loss                 | 37.5       |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.06      |
|    std                  | 0.347      |
|    value_loss           | 120        |
----------------------------------------
----------------------------------------
| reward                  | -2.45      |
| reward_contact          | -0.047     |
| reward_ctrl             | 2.96e-06   |
| reward_motion           | 6.49e-07   |
| reward_torque           | -3.18      |
| reward_velocity         | 0.781      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.35e+03  |
| time/                   |            |
|    fps                  | 347        |
|    iterations           | 27         |
|    time_elapsed         | 79         |
|    total_timesteps      | 27648      |
| train/                  |            |
|    approx_kl            | 0.11660235 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.9      |
|    explained_variance   | -0.46      |
|    learning_rate        | 0.0003     |
|    loss                 | 67.9       |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.0506    |
|    std                  | 0.346      |
|    value_loss           | 184        |
----------------------------------------
----------------------------------------
| reward                  | -2.41      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.1e-06    |
| reward_motion           | 6.64e-07   |
| reward_torque           | -3.14      |
| reward_velocity         | 0.779      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.36e+03  |
| time/                   |            |
|    fps                  | 346        |
|    iterations           | 28         |
|    time_elapsed         | 82         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.11507568 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.6      |
|    explained_variance   | 0.0308     |
|    learning_rate        | 0.0003     |
|    loss                 | 28.1       |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.041     |
|    std                  | 0.346      |
|    value_loss           | 185        |
----------------------------------------
-----------------------------------------
| reward                  | -2.44       |
| reward_contact          | -0.0469     |
| reward_ctrl             | 3.24e-06    |
| reward_motion           | 6.88e-07    |
| reward_torque           | -3.17       |
| reward_velocity         | 0.773       |
| rollout/                |             |
|    ep_len_mean          | 1.02e+03    |
|    ep_rew_mean          | -2.38e+03   |
| time/                   |             |
|    fps                  | 346         |
|    iterations           | 29          |
|    time_elapsed         | 85          |
|    total_timesteps      | 29696       |
| train/                  |             |
|    approx_kl            | 0.068516396 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | 0.0659      |
|    learning_rate        | 0.0003      |
|    loss                 | 142         |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0653     |
|    std                  | 0.345       |
|    value_loss           | 198         |
-----------------------------------------
Num timesteps: 30000
Best mean reward: -2059.00 - Last mean reward per episode: -2376.28
----------------------------------------
| reward                  | -2.46      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 3.33e-06   |
| reward_motion           | 7.15e-07   |
| reward_torque           | -3.18      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.38e+03  |
| time/                   |            |
|    fps                  | 346        |
|    iterations           | 30         |
|    time_elapsed         | 88         |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.11165166 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.0535     |
|    learning_rate        | 0.0003     |
|    loss                 | 86.8       |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.0514    |
|    std                  | 0.345      |
|    value_loss           | 195        |
----------------------------------------
----------------------------------------
| reward                  | -2.47      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 3.75e-06   |
| reward_motion           | 7.84e-07   |
| reward_torque           | -3.19      |
| reward_velocity         | 0.774      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.38e+03  |
| time/                   |            |
|    fps                  | 346        |
|    iterations           | 31         |
|    time_elapsed         | 91         |
|    total_timesteps      | 31744      |
| train/                  |            |
|    approx_kl            | 0.16336218 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.7      |
|    explained_variance   | 0.0889     |
|    learning_rate        | 0.0003     |
|    loss                 | 182        |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.0694    |
|    std                  | 0.345      |
|    value_loss           | 194        |
----------------------------------------
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.0468    |
| reward_ctrl             | 3.74e-06   |
| reward_motion           | 7.83e-07   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.779      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.39e+03  |
| time/                   |            |
|    fps                  | 346        |
|    iterations           | 32         |
|    time_elapsed         | 94         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.20484138 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.9      |
|    explained_variance   | 0.0319     |
|    learning_rate        | 0.0003     |
|    loss                 | 41.6       |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0573    |
|    std                  | 0.344      |
|    value_loss           | 210        |
----------------------------------------
----------------------------------------
| reward                  | -2.44      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.87e-06   |
| reward_motion           | 8.28e-07   |
| reward_torque           | -3.17      |
| reward_velocity         | 0.776      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.4e+03   |
| time/                   |            |
|    fps                  | 346        |
|    iterations           | 33         |
|    time_elapsed         | 97         |
|    total_timesteps      | 33792      |
| train/                  |            |
|    approx_kl            | 0.38690743 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.1      |
|    explained_variance   | 0.0346     |
|    learning_rate        | 0.0003     |
|    loss                 | 150        |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0492    |
|    std                  | 0.344      |
|    value_loss           | 228        |
----------------------------------------
----------------------------------------
| reward                  | -2.46      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.79e-06   |
| reward_motion           | 8.12e-07   |
| reward_torque           | -3.19      |
| reward_velocity         | 0.777      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.4e+03   |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 34         |
|    time_elapsed         | 100        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.11785532 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20        |
|    explained_variance   | 0.0255     |
|    learning_rate        | 0.0003     |
|    loss                 | 116        |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.0671    |
|    std                  | 0.344      |
|    value_loss           | 214        |
----------------------------------------
----------------------------------------
| reward                  | -2.48      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.71e-06   |
| reward_motion           | 7.97e-07   |
| reward_torque           | -3.21      |
| reward_velocity         | 0.775      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.42e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 35         |
|    time_elapsed         | 103        |
|    total_timesteps      | 35840      |
| train/                  |            |
|    approx_kl            | 0.13725826 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.4      |
|    explained_variance   | 0.0618     |
|    learning_rate        | 0.0003     |
|    loss                 | 6.58       |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.0687    |
|    std                  | 0.344      |
|    value_loss           | 214        |
----------------------------------------
Num timesteps: 36000
Best mean reward: -2059.00 - Last mean reward per episode: -2416.80
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | -0.0469    |
| reward_ctrl             | 3.64e-06   |
| reward_motion           | 7.83e-07   |
| reward_torque           | -3.22      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.42e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 36         |
|    time_elapsed         | 106        |
|    total_timesteps      | 36864      |
| train/                  |            |
|    approx_kl            | 0.12666398 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.0903     |
|    learning_rate        | 0.0003     |
|    loss                 | 40         |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0596    |
|    std                  | 0.344      |
|    value_loss           | 225        |
----------------------------------------
----------------------------------------
| reward                  | -2.49      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.18e-06   |
| reward_motion           | 8.75e-07   |
| reward_torque           | -3.22      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.43e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 37         |
|    time_elapsed         | 109        |
|    total_timesteps      | 37888      |
| train/                  |            |
|    approx_kl            | 0.19215783 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.5      |
|    explained_variance   | 0.0698     |
|    learning_rate        | 0.0003     |
|    loss                 | 104        |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.067     |
|    std                  | 0.344      |
|    value_loss           | 221        |
----------------------------------------
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.1e-06    |
| reward_motion           | 8.6e-07    |
| reward_torque           | -3.22      |
| reward_velocity         | 0.767      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.44e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 38         |
|    time_elapsed         | 112        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.13736585 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.5      |
|    explained_variance   | 0.0917     |
|    learning_rate        | 0.0003     |
|    loss                 | 84.8       |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0518    |
|    std                  | 0.343      |
|    value_loss           | 219        |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.09e-06   |
| reward_motion           | 8.66e-07   |
| reward_torque           | -3.24      |
| reward_velocity         | 0.766      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.44e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 39         |
|    time_elapsed         | 115        |
|    total_timesteps      | 39936      |
| train/                  |            |
|    approx_kl            | 0.37177145 |
|    clip_fraction        | 0.456      |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.5      |
|    explained_variance   | 0.0291     |
|    learning_rate        | 0.0003     |
|    loss                 | 3          |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.0629    |
|    std                  | 0.343      |
|    value_loss           | 244        |
----------------------------------------
---------------------------------------
| reward                  | -2.53     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 4.06e-06  |
| reward_motion           | 8.59e-07  |
| reward_torque           | -3.25     |
| reward_velocity         | 0.771     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.44e+03 |
| time/                   |           |
|    fps                  | 345       |
|    iterations           | 40        |
|    time_elapsed         | 118       |
|    total_timesteps      | 40960     |
| train/                  |           |
|    approx_kl            | 0.2808153 |
|    clip_fraction        | 0.423     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.1     |
|    explained_variance   | 0.0506    |
|    learning_rate        | 0.0003    |
|    loss                 | 4.05      |
|    n_updates            | 780       |
|    policy_gradient_loss | -0.063    |
|    std                  | 0.343     |
|    value_loss           | 236       |
---------------------------------------
----------------------------------------
| reward                  | -2.53      |
| reward_contact          | -0.0469    |
| reward_ctrl             | 4.09e-06   |
| reward_motion           | 8.64e-07   |
| reward_torque           | -3.26      |
| reward_velocity         | 0.771      |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.44e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 41         |
|    time_elapsed         | 121        |
|    total_timesteps      | 41984      |
| train/                  |            |
|    approx_kl            | 0.17382684 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.7      |
|    explained_variance   | 0.0568     |
|    learning_rate        | 0.0003     |
|    loss                 | 131        |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0536    |
|    std                  | 0.343      |
|    value_loss           | 235        |
----------------------------------------
Num timesteps: 42000
Best mean reward: -2059.00 - Last mean reward per episode: -2441.78
---------------------------------------
| reward                  | -2.52     |
| reward_contact          | -0.0469   |
| reward_ctrl             | 4.04e-06  |
| reward_motion           | 8.57e-07  |
| reward_torque           | -3.25     |
| reward_velocity         | 0.773     |
| rollout/                |           |
|    ep_len_mean          | 1.02e+03  |
|    ep_rew_mean          | -2.45e+03 |
| time/                   |           |
|    fps                  | 345       |
|    iterations           | 42        |
|    time_elapsed         | 124       |
|    total_timesteps      | 43008     |
| train/                  |           |
|    approx_kl            | 0.1686695 |
|    clip_fraction        | 0.343     |
|    clip_range           | 0.4       |
|    entropy_loss         | -19.6     |
|    explained_variance   | 0.102     |
|    learning_rate        | 0.0003    |
|    loss                 | 14.7      |
|    n_updates            | 820       |
|    policy_gradient_loss | -0.0672   |
|    std                  | 0.342     |
|    value_loss           | 233       |
---------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | -0.047     |
| reward_ctrl             | 3.98e-06   |
| reward_motion           | 8.45e-07   |
| reward_torque           | -3.24      |
| reward_velocity         | 0.77       |
| rollout/                |            |
|    ep_len_mean          | 1.02e+03   |
|    ep_rew_mean          | -2.46e+03  |
| time/                   |            |
|    fps                  | 345        |
|    iterations           | 43         |
|    time_elapsed         | 127        |
|    total_timesteps      | 44032      |
| train/                  |            |
|    approx_kl            | 0.18749896 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20        |
|    explained_variance   | 0.0552     |
|    learning_rate        | 0.0003     |
|    loss                 | 255        |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.0655    |
|    std                  | 0.342      |
|    value_loss           | 236        |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_5
---------------------------------
| reward             | 9.74     |
| reward_contact     | 0        |
| reward_ctrl        | 10.5     |
| reward_motion      | 2.47     |
| reward_torque      | -3.27    |
| reward_velocity    | 0.0114   |
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    fps             | 468      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | 12.7       |
| reward_contact          | 0          |
| reward_ctrl             | 13.5       |
| reward_motion           | 2.5        |
| reward_torque           | -3.34      |
| reward_velocity         | 0.0349     |
| rollout/                |            |
|    ep_len_mean          | 86.9       |
|    ep_rew_mean          | 864        |
| time/                   |            |
|    fps                  | 382        |
|    iterations           | 2          |
|    time_elapsed         | 5          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.08909861 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | -0.0042    |
|    learning_rate        | 0.0003     |
|    loss                 | 1.6e+03    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.368      |
|    value_loss           | 5.3e+03    |
----------------------------------------
----------------------------------------
| reward                  | 12.1       |
| reward_contact          | -0.000819  |
| reward_ctrl             | 13.1       |
| reward_motion           | 2.39       |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0387     |
| rollout/                |            |
|    ep_len_mean          | 104        |
|    ep_rew_mean          | 987        |
| time/                   |            |
|    fps                  | 351        |
|    iterations           | 3          |
|    time_elapsed         | 8          |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.08520674 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.101      |
|    learning_rate        | 0.0003     |
|    loss                 | 813        |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0771    |
|    std                  | 0.368      |
|    value_loss           | 3.24e+03   |
----------------------------------------
----------------------------------------
| reward                  | 11.9       |
| reward_contact          | -0.0012    |
| reward_ctrl             | 12.9       |
| reward_motion           | 2.39       |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0384     |
| rollout/                |            |
|    ep_len_mean          | 110        |
|    ep_rew_mean          | 1.05e+03   |
| time/                   |            |
|    fps                  | 335        |
|    iterations           | 4          |
|    time_elapsed         | 12         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.06514679 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.0102     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.02e+03   |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0947    |
|    std                  | 0.368      |
|    value_loss           | 3.44e+03   |
----------------------------------------
----------------------------------------
| reward                  | 12         |
| reward_contact          | -0.00108   |
| reward_ctrl             | 13.1       |
| reward_motion           | 2.38       |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0377     |
| rollout/                |            |
|    ep_len_mean          | 127        |
|    ep_rew_mean          | 1.21e+03   |
| time/                   |            |
|    fps                  | 325        |
|    iterations           | 5          |
|    time_elapsed         | 15         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.07207671 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.232      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.34e+03   |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.075     |
|    std                  | 0.368      |
|    value_loss           | 3.91e+03   |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 878.37
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | 11.8        |
| reward_contact          | -0.000959   |
| reward_ctrl             | 12.7        |
| reward_motion           | 2.41        |
| reward_torque           | -3.42       |
| reward_velocity         | 0.0303      |
| rollout/                |             |
|    ep_len_mean          | 86.8        |
|    ep_rew_mean          | 856         |
| time/                   |             |
|    fps                  | 316         |
|    iterations           | 6           |
|    time_elapsed         | 19          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.028427092 |
|    clip_fraction        | 0.0353      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.493       |
|    learning_rate        | 0.0003      |
|    loss                 | 449         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0339     |
|    std                  | 0.368       |
|    value_loss           | 1.61e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 11.8       |
| reward_contact          | -0.000907  |
| reward_ctrl             | 12.8       |
| reward_motion           | 2.44       |
| reward_torque           | -3.44      |
| reward_velocity         | 0.0302     |
| rollout/                |            |
|    ep_len_mean          | 94.9       |
|    ep_rew_mean          | 921        |
| time/                   |            |
|    fps                  | 311        |
|    iterations           | 7          |
|    time_elapsed         | 22         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.04576248 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.0659     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.28e+03   |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0643    |
|    std                  | 0.368      |
|    value_loss           | 3.73e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 11.7        |
| reward_contact          | -0.001      |
| reward_ctrl             | 12.7        |
| reward_motion           | 2.51        |
| reward_torque           | -3.46       |
| reward_velocity         | 0.0268      |
| rollout/                |             |
|    ep_len_mean          | 86.1        |
|    ep_rew_mean          | 841         |
| time/                   |             |
|    fps                  | 308         |
|    iterations           | 8           |
|    time_elapsed         | 26          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.055026103 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.643       |
|    learning_rate        | 0.0003      |
|    loss                 | 332         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0649     |
|    std                  | 0.368       |
|    value_loss           | 1.43e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 11.6       |
| reward_contact          | -0.00103   |
| reward_ctrl             | 12.5       |
| reward_motion           | 2.48       |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0311     |
| rollout/                |            |
|    ep_len_mean          | 95.2       |
|    ep_rew_mean          | 917        |
| time/                   |            |
|    fps                  | 305        |
|    iterations           | 9          |
|    time_elapsed         | 30         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.04045736 |
|    clip_fraction        | 0.0592     |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.294      |
|    learning_rate        | 0.0003     |
|    loss                 | 536        |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0443    |
|    std                  | 0.368      |
|    value_loss           | 1.82e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 11.5        |
| reward_contact          | -0.000958   |
| reward_ctrl             | 12.4        |
| reward_motion           | 2.52        |
| reward_torque           | -3.49       |
| reward_velocity         | 0.0294      |
| rollout/                |             |
|    ep_len_mean          | 91.9        |
|    ep_rew_mean          | 889         |
| time/                   |             |
|    fps                  | 304         |
|    iterations           | 10          |
|    time_elapsed         | 33          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.032536104 |
|    clip_fraction        | 0.0376      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.0003      |
|    loss                 | 502         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0337     |
|    std                  | 0.368       |
|    value_loss           | 1.43e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 11.3        |
| reward_contact          | -0.00123    |
| reward_ctrl             | 12.2        |
| reward_motion           | 2.62        |
| reward_torque           | -3.5        |
| reward_velocity         | 0.0222      |
| rollout/                |             |
|    ep_len_mean          | 73.1        |
|    ep_rew_mean          | 724         |
| time/                   |             |
|    fps                  | 302         |
|    iterations           | 11          |
|    time_elapsed         | 37          |
|    total_timesteps      | 11264       |
| train/                  |             |
|    approx_kl            | 0.049803257 |
|    clip_fraction        | 0.0779      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.523       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.06e+03    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0469     |
|    std                  | 0.368       |
|    value_loss           | 2.82e+03    |
-----------------------------------------
Num timesteps: 12000
Best mean reward: 878.37 - Last mean reward per episode: 723.79
----------------------------------------
| reward                  | 11.4       |
| reward_contact          | -0.00136   |
| reward_ctrl             | 12.3       |
| reward_motion           | 2.62       |
| reward_torque           | -3.5       |
| reward_velocity         | 0.022      |
| rollout/                |            |
|    ep_len_mean          | 72         |
|    ep_rew_mean          | 708        |
| time/                   |            |
|    fps                  | 301        |
|    iterations           | 12         |
|    time_elapsed         | 40         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.05735998 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.141      |
|    learning_rate        | 0.0003     |
|    loss                 | 853        |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.058     |
|    std                  | 0.368      |
|    value_loss           | 2.88e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 11.6        |
| reward_contact          | -0.0018     |
| reward_ctrl             | 12.5        |
| reward_motion           | 2.67        |
| reward_torque           | -3.53       |
| reward_velocity         | 0.0213      |
| rollout/                |             |
|    ep_len_mean          | 70.8        |
|    ep_rew_mean          | 695         |
| time/                   |             |
|    fps                  | 300         |
|    iterations           | 13          |
|    time_elapsed         | 44          |
|    total_timesteps      | 13312       |
| train/                  |             |
|    approx_kl            | 0.040806033 |
|    clip_fraction        | 0.0765      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.499       |
|    learning_rate        | 0.0003      |
|    loss                 | 565         |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0498     |
|    std                  | 0.368       |
|    value_loss           | 1.88e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 11.6        |
| reward_contact          | -0.00219    |
| reward_ctrl             | 12.5        |
| reward_motion           | 2.72        |
| reward_torque           | -3.61       |
| reward_velocity         | 0.0213      |
| rollout/                |             |
|    ep_len_mean          | 80.6        |
|    ep_rew_mean          | 772         |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 14          |
|    time_elapsed         | 47          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.046216436 |
|    clip_fraction        | 0.0629      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.413       |
|    learning_rate        | 0.0003      |
|    loss                 | 762         |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0412     |
|    std                  | 0.368       |
|    value_loss           | 2.02e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 11.4        |
| reward_contact          | -0.0023     |
| reward_ctrl             | 12.3        |
| reward_motion           | 2.69        |
| reward_torque           | -3.6        |
| reward_velocity         | 0.0234      |
| rollout/                |             |
|    ep_len_mean          | 79.3        |
|    ep_rew_mean          | 761         |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 15          |
|    time_elapsed         | 51          |
|    total_timesteps      | 15360       |
| train/                  |             |
|    approx_kl            | 0.038783964 |
|    clip_fraction        | 0.0551      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.0003      |
|    loss                 | 867         |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0469     |
|    std                  | 0.368       |
|    value_loss           | 2.17e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 11.6       |
| reward_contact          | -0.0025    |
| reward_ctrl             | 12.4       |
| reward_motion           | 2.69       |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0231     |
| rollout/                |            |
|    ep_len_mean          | 86.8       |
|    ep_rew_mean          | 831        |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 16         |
|    time_elapsed         | 54         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.04771261 |
|    clip_fraction        | 0.0803     |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.218      |
|    learning_rate        | 0.0003     |
|    loss                 | 663        |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0486    |
|    std                  | 0.368      |
|    value_loss           | 1.92e+03   |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_6
---------------------------------
| reward             | 10.6     |
| reward_contact     | 0        |
| reward_ctrl        | 14.2     |
| reward_motion      | -0.1     |
| reward_torque      | -3.52    |
| reward_velocity    | 0.0105   |
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 392      |
| time/              |          |
|    fps             | 482      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
-----------------------------------------
| reward                  | 9.55        |
| reward_contact          | -0.00277    |
| reward_ctrl             | 13          |
| reward_motion           | -0.0681     |
| reward_torque           | -3.35       |
| reward_velocity         | 0.0126      |
| rollout/                |             |
|    ep_len_mean          | 68          |
|    ep_rew_mean          | 643         |
| time/                   |             |
|    fps                  | 387         |
|    iterations           | 2           |
|    time_elapsed         | 5           |
|    total_timesteps      | 2048        |
| train/                  |             |
|    approx_kl            | 0.055133946 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | -1.6e-05    |
|    learning_rate        | 0.0003      |
|    loss                 | 988         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.108      |
|    std                  | 0.368       |
|    value_loss           | 4.26e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.62        |
| reward_contact          | -0.00201    |
| reward_ctrl             | 13.1        |
| reward_motion           | -0.0768     |
| reward_torque           | -3.38       |
| reward_velocity         | 0.0131      |
| rollout/                |             |
|    ep_len_mean          | 85.3        |
|    ep_rew_mean          | 782         |
| time/                   |             |
|    fps                  | 349         |
|    iterations           | 3           |
|    time_elapsed         | 8           |
|    total_timesteps      | 3072        |
| train/                  |             |
|    approx_kl            | 0.053772744 |
|    clip_fraction        | 0.0851      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.2       |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0003      |
|    loss                 | 642         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0647     |
|    std                  | 0.368       |
|    value_loss           | 2.46e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.48        |
| reward_contact          | -0.00138    |
| reward_ctrl             | 12.8        |
| reward_motion           | -0.0712     |
| reward_torque           | -3.3        |
| reward_velocity         | 0.0167      |
| rollout/                |             |
|    ep_len_mean          | 69.3        |
|    ep_rew_mean          | 647         |
| time/                   |             |
|    fps                  | 333         |
|    iterations           | 4           |
|    time_elapsed         | 12          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.071887545 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.192       |
|    learning_rate        | 0.0003      |
|    loss                 | 599         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0692     |
|    std                  | 0.368       |
|    value_loss           | 2.36e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 9.17       |
| reward_contact          | -0.00164   |
| reward_ctrl             | 12.5       |
| reward_motion           | -0.047     |
| reward_torque           | -3.33      |
| reward_velocity         | 0.0241     |
| rollout/                |            |
|    ep_len_mean          | 84.5       |
|    ep_rew_mean          | 770        |
| time/                   |            |
|    fps                  | 324        |
|    iterations           | 5          |
|    time_elapsed         | 15         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.06008743 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.583      |
|    learning_rate        | 0.0003     |
|    loss                 | 740        |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0612    |
|    std                  | 0.368      |
|    value_loss           | 2.18e+03   |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: 916.73
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | 9.11        |
| reward_contact          | -0.00199    |
| reward_ctrl             | 12.5        |
| reward_motion           | -0.0374     |
| reward_torque           | -3.33       |
| reward_velocity         | 0.0244      |
| rollout/                |             |
|    ep_len_mean          | 102         |
|    ep_rew_mean          | 917         |
| time/                   |             |
|    fps                  | 318         |
|    iterations           | 6           |
|    time_elapsed         | 19          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.069795854 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.0429      |
|    learning_rate        | 0.0003      |
|    loss                 | 623         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0794     |
|    std                  | 0.368       |
|    value_loss           | 2.51e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 8.98       |
| reward_contact          | -0.00185   |
| reward_ctrl             | 12.3       |
| reward_motion           | -0.022     |
| reward_torque           | -3.36      |
| reward_velocity         | 0.0243     |
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | 906        |
| time/                   |            |
|    fps                  | 314        |
|    iterations           | 7          |
|    time_elapsed         | 22         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.06844729 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.173      |
|    learning_rate        | 0.0003     |
|    loss                 | 594        |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0658    |
|    std                  | 0.368      |
|    value_loss           | 2.67e+03   |
----------------------------------------
---------------------------------------
| reward                  | 9.47      |
| reward_contact          | -0.00174  |
| reward_ctrl             | 12.8      |
| reward_motion           | -0.0261   |
| reward_torque           | -3.34     |
| reward_velocity         | 0.0231    |
| rollout/                |           |
|    ep_len_mean          | 90.6      |
|    ep_rew_mean          | 819       |
| time/                   |           |
|    fps                  | 311       |
|    iterations           | 8         |
|    time_elapsed         | 26        |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 0.0293882 |
|    clip_fraction        | 0.0242    |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.595     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.78e+03  |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.0362   |
|    std                  | 0.368     |
|    value_loss           | 3.16e+03  |
---------------------------------------
-----------------------------------------
| reward                  | 9.27        |
| reward_contact          | -0.00184    |
| reward_ctrl             | 12.6        |
| reward_motion           | -0.0261     |
| reward_torque           | -3.34       |
| reward_velocity         | 0.0244      |
| rollout/                |             |
|    ep_len_mean          | 98.4        |
|    ep_rew_mean          | 884         |
| time/                   |             |
|    fps                  | 309         |
|    iterations           | 9           |
|    time_elapsed         | 29          |
|    total_timesteps      | 9216        |
| train/                  |             |
|    approx_kl            | 0.042493284 |
|    clip_fraction        | 0.0475      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.3e+03     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0387     |
|    std                  | 0.368       |
|    value_loss           | 2.83e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 9.5        |
| reward_contact          | -0.00224   |
| reward_ctrl             | 12.8       |
| reward_motion           | -0.026     |
| reward_torque           | -3.34      |
| reward_velocity         | 0.0236     |
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | 910        |
| time/                   |            |
|    fps                  | 307        |
|    iterations           | 10         |
|    time_elapsed         | 33         |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.04812345 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.509      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.03e+03   |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0617    |
|    std                  | 0.368      |
|    value_loss           | 2.64e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 9.82        |
| reward_contact          | -0.00158    |
| reward_ctrl             | 13.2        |
| reward_motion           | -0.0231     |
| reward_torque           | -3.35       |
| reward_velocity         | 0.0273      |
| rollout/                |             |
|    ep_len_mean          | 83.9        |
|    ep_rew_mean          | 766         |
| time/                   |             |
|    fps                  | 306         |
|    iterations           | 11          |
|    time_elapsed         | 36          |
|    total_timesteps      | 11264       |
| train/                  |             |
|    approx_kl            | 0.063712046 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.0356      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.11e+03    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0702     |
|    std                  | 0.368       |
|    value_loss           | 3.19e+03    |
-----------------------------------------
Num timesteps: 12000
Best mean reward: 916.73 - Last mean reward per episode: 693.01
---------------------------------------
| reward                  | 9.64      |
| reward_contact          | -0.00117  |
| reward_ctrl             | 13.1      |
| reward_motion           | -0.0518   |
| reward_torque           | -3.39     |
| reward_velocity         | 0.0213    |
| rollout/                |           |
|    ep_len_mean          | 56        |
|    ep_rew_mean          | 530       |
| time/                   |           |
|    fps                  | 305       |
|    iterations           | 12        |
|    time_elapsed         | 40        |
|    total_timesteps      | 12288     |
| train/                  |           |
|    approx_kl            | 0.0654025 |
|    clip_fraction        | 0.148     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21       |
|    explained_variance   | 0.198     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.13e+03  |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0656   |
|    std                  | 0.368     |
|    value_loss           | 3.05e+03  |
---------------------------------------
-----------------------------------------
| reward                  | 9.61        |
| reward_contact          | -0.00121    |
| reward_ctrl             | 13.1        |
| reward_motion           | -0.0626     |
| reward_torque           | -3.4        |
| reward_velocity         | 0.02        |
| rollout/                |             |
|    ep_len_mean          | 57.5        |
|    ep_rew_mean          | 542         |
| time/                   |             |
|    fps                  | 304         |
|    iterations           | 13          |
|    time_elapsed         | 43          |
|    total_timesteps      | 13312       |
| train/                  |             |
|    approx_kl            | 0.035146132 |
|    clip_fraction        | 0.0587      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.239       |
|    learning_rate        | 0.0003      |
|    loss                 | 912         |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0486     |
|    std                  | 0.368       |
|    value_loss           | 2.72e+03    |
-----------------------------------------
----------------------------------------
| reward                  | 9.87       |
| reward_contact          | -0.000842  |
| reward_ctrl             | 13.4       |
| reward_motion           | -0.0653    |
| reward_torque           | -3.44      |
| reward_velocity         | 0.017      |
| rollout/                |            |
|    ep_len_mean          | 52.4       |
|    ep_rew_mean          | 500        |
| time/                   |            |
|    fps                  | 302        |
|    iterations           | 14         |
|    time_elapsed         | 47         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.03921909 |
|    clip_fraction        | 0.0476     |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.0164     |
|    learning_rate        | 0.0003     |
|    loss                 | 469        |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0474    |
|    std                  | 0.368      |
|    value_loss           | 1.97e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 9.55        |
| reward_contact          | -0.000477   |
| reward_ctrl             | 13          |
| reward_motion           | -0.0653     |
| reward_torque           | -3.43       |
| reward_velocity         | 0.0172      |
| rollout/                |             |
|    ep_len_mean          | 54.1        |
|    ep_rew_mean          | 510         |
| time/                   |             |
|    fps                  | 300         |
|    iterations           | 15          |
|    time_elapsed         | 51          |
|    total_timesteps      | 15360       |
| train/                  |             |
|    approx_kl            | 0.046162732 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.189       |
|    learning_rate        | 0.0003      |
|    loss                 | 385         |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0633     |
|    std                  | 0.368       |
|    value_loss           | 1.41e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.56        |
| reward_contact          | -0.000477   |
| reward_ctrl             | 13          |
| reward_motion           | -0.0653     |
| reward_torque           | -3.43       |
| reward_velocity         | 0.017       |
| rollout/                |             |
|    ep_len_mean          | 59.9        |
|    ep_rew_mean          | 560         |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 16          |
|    time_elapsed         | 54          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.058286734 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | -0.0672     |
|    learning_rate        | 0.0003      |
|    loss                 | 397         |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.062      |
|    std                  | 0.368       |
|    value_loss           | 1.15e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.76        |
| reward_contact          | -0.00024    |
| reward_ctrl             | 13.3        |
| reward_motion           | -0.0653     |
| reward_torque           | -3.44       |
| reward_velocity         | 0.0163      |
| rollout/                |             |
|    ep_len_mean          | 68.4        |
|    ep_rew_mean          | 630         |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 17          |
|    time_elapsed         | 58          |
|    total_timesteps      | 17408       |
| train/                  |             |
|    approx_kl            | 0.037595566 |
|    clip_fraction        | 0.0374      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.0003      |
|    loss                 | 576         |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0411     |
|    std                  | 0.368       |
|    value_loss           | 2.15e+03    |
-----------------------------------------
Num timesteps: 18000
Best mean reward: 916.73 - Last mean reward per episode: 674.37
----------------------------------------
| reward                  | 9.71       |
| reward_contact          | -0.00024   |
| reward_ctrl             | 13.2       |
| reward_motion           | -0.0668    |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0176     |
| rollout/                |            |
|    ep_len_mean          | 73.7       |
|    ep_rew_mean          | 674        |
| time/                   |            |
|    fps                  | 297        |
|    iterations           | 18         |
|    time_elapsed         | 61         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.03569829 |
|    clip_fraction        | 0.0447     |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | -0.127     |
|    learning_rate        | 0.0003     |
|    loss                 | 903        |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0406    |
|    std                  | 0.368      |
|    value_loss           | 2.46e+03   |
----------------------------------------
-----------------------------------------
| reward                  | 9.7         |
| reward_contact          | -0.000462   |
| reward_ctrl             | 13.2        |
| reward_motion           | -0.0668     |
| reward_torque           | -3.42       |
| reward_velocity         | 0.0198      |
| rollout/                |             |
|    ep_len_mean          | 80.7        |
|    ep_rew_mean          | 731         |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 19          |
|    time_elapsed         | 65          |
|    total_timesteps      | 19456       |
| train/                  |             |
|    approx_kl            | 0.055611745 |
|    clip_fraction        | 0.0979      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | -0.0154     |
|    learning_rate        | 0.0003      |
|    loss                 | 392         |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0574     |
|    std                  | 0.368       |
|    value_loss           | 1.72e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.6         |
| reward_contact          | -0.000462   |
| reward_ctrl             | 13.1        |
| reward_motion           | -0.0668     |
| reward_torque           | -3.43       |
| reward_velocity         | 0.02        |
| rollout/                |             |
|    ep_len_mean          | 90.4        |
|    ep_rew_mean          | 813         |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 20          |
|    time_elapsed         | 68          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.030617608 |
|    clip_fraction        | 0.0259      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.0102      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.32e+03    |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0396     |
|    std                  | 0.368       |
|    value_loss           | 4.55e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.69        |
| reward_contact          | -0.000462   |
| reward_ctrl             | 13.1        |
| reward_motion           | -0.0647     |
| reward_torque           | -3.41       |
| reward_velocity         | 0.0224      |
| rollout/                |             |
|    ep_len_mean          | 89.7        |
|    ep_rew_mean          | 808         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 21          |
|    time_elapsed         | 72          |
|    total_timesteps      | 21504       |
| train/                  |             |
|    approx_kl            | 0.042898893 |
|    clip_fraction        | 0.085       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.469       |
|    learning_rate        | 0.0003      |
|    loss                 | 495         |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.05       |
|    std                  | 0.368       |
|    value_loss           | 1.98e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 9.89        |
| reward_contact          | -0.000686   |
| reward_ctrl             | 13.3        |
| reward_motion           | -0.0494     |
| reward_torque           | -3.39       |
| reward_velocity         | 0.0221      |
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 929         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 22          |
|    time_elapsed         | 76          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.019014295 |
|    clip_fraction        | 0.00557     |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.0003      |
|    loss                 | 768         |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0197     |
|    std                  | 0.368       |
|    value_loss           | 2.17e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | 10.2        |
| reward_contact          | -0.000686   |
| reward_ctrl             | 13.7        |
| reward_motion           | -0.0494     |
| reward_torque           | -3.44       |
| reward_velocity         | 0.0238      |
| rollout/                |             |
|    ep_len_mean          | 108         |
|    ep_rew_mean          | 953         |
| time/                   |             |
|    fps                  | 296         |
|    iterations           | 23          |
|    time_elapsed         | 79          |
|    total_timesteps      | 23552       |
| train/                  |             |
|    approx_kl            | 0.050864376 |
|    clip_fraction        | 0.0683      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.0003      |
|    loss                 | 961         |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0516     |
|    std                  | 0.368       |
|    value_loss           | 2.86e+03    |
-----------------------------------------
Num timesteps: 24000
Best mean reward: 916.73 - Last mean reward per episode: 952.60
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | 9.99        |
| reward_contact          | -0.000686   |
| reward_ctrl             | 13.5        |
| reward_motion           | -0.0586     |
| reward_torque           | -3.44       |
| reward_velocity         | 0.0246      |
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 927         |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 24          |
|    time_elapsed         | 82          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.046912353 |
|    clip_fraction        | 0.0636      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.3         |
|    learning_rate        | 0.0003      |
|    loss                 | 1.21e+03    |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0585     |
|    std                  | 0.368       |
|    value_loss           | 3.29e+03    |
-----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_7
---------------------------------
| reward             | -3.82    |
| reward_contact     | 0        |
| reward_ctrl        | -0.1     |
| reward_motion      | -0.1     |
| reward_torque      | -3.64    |
| reward_velocity    | 0.011    |
| rollout/           |          |
|    ep_len_mean     | 28.7     |
|    ep_rew_mean     | -94.4    |
| time/              |          |
|    fps             | 484      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | -3.56      |
| reward_contact          | -0.00141   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0709    |
| reward_torque           | -3.4       |
| reward_velocity         | 0.0207     |
| rollout/                |            |
|    ep_len_mean          | 57.2       |
|    ep_rew_mean          | -180       |
| time/                   |            |
|    fps                  | 377        |
|    iterations           | 2          |
|    time_elapsed         | 5          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.12333509 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.00606    |
|    learning_rate        | 0.0003     |
|    loss                 | 7.88       |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.127     |
|    std                  | 0.367      |
|    value_loss           | 330        |
----------------------------------------
---------------------------------------
| reward                  | -3.52     |
| reward_contact          | -0.00137  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0442   |
| reward_torque           | -3.39     |
| reward_velocity         | 0.0221    |
| rollout/                |           |
|    ep_len_mean          | 84.1      |
|    ep_rew_mean          | -262      |
| time/                   |           |
|    fps                  | 340       |
|    iterations           | 3         |
|    time_elapsed         | 9         |
|    total_timesteps      | 3072      |
| train/                  |           |
|    approx_kl            | 0.1174901 |
|    clip_fraction        | 0.277     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.1     |
|    explained_variance   | 0.343     |
|    learning_rate        | 0.0003    |
|    loss                 | 31.1      |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.121    |
|    std                  | 0.367     |
|    value_loss           | 142       |
---------------------------------------
-----------------------------------------
| reward                  | -3.52       |
| reward_contact          | -0.00112    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0364     |
| reward_torque           | -3.41       |
| reward_velocity         | 0.0254      |
| rollout/                |             |
|    ep_len_mean          | 75.3        |
|    ep_rew_mean          | -236        |
| time/                   |             |
|    fps                  | 324         |
|    iterations           | 4           |
|    time_elapsed         | 12          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.054415382 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.4        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0729     |
|    std                  | 0.367       |
|    value_loss           | 155         |
-----------------------------------------
----------------------------------------
| reward                  | -3.54      |
| reward_contact          | -0.00096   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0242    |
| reward_torque           | -3.44      |
| reward_velocity         | 0.0241     |
| rollout/                |            |
|    ep_len_mean          | 90.5       |
|    ep_rew_mean          | -283       |
| time/                   |            |
|    fps                  | 315        |
|    iterations           | 5          |
|    time_elapsed         | 16         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.07612029 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.0003     |
|    loss                 | 23.3       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0755    |
|    std                  | 0.367      |
|    value_loss           | 131        |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -297.27
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.56      |
| reward_contact          | -0.000828  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0346    |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0242     |
| rollout/                |            |
|    ep_len_mean          | 94.6       |
|    ep_rew_mean          | -297       |
| time/                   |            |
|    fps                  | 309        |
|    iterations           | 6          |
|    time_elapsed         | 19         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.08958479 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.455      |
|    learning_rate        | 0.0003     |
|    loss                 | 32.8       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0961    |
|    std                  | 0.367      |
|    value_loss           | 150        |
----------------------------------------
----------------------------------------
| reward                  | -3.58      |
| reward_contact          | -0.000866  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0467    |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0249     |
| rollout/                |            |
|    ep_len_mean          | 95.1       |
|    ep_rew_mean          | -300       |
| time/                   |            |
|    fps                  | 305        |
|    iterations           | 7          |
|    time_elapsed         | 23         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.07138507 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.298      |
|    learning_rate        | 0.0003     |
|    loss                 | 39.7       |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0889    |
|    std                  | 0.367      |
|    value_loss           | 176        |
----------------------------------------
----------------------------------------
| reward                  | -3.58      |
| reward_contact          | -0.000843  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0371    |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0264     |
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | -333       |
| time/                   |            |
|    fps                  | 303        |
|    iterations           | 8          |
|    time_elapsed         | 26         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.09170306 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.186      |
|    learning_rate        | 0.0003     |
|    loss                 | 83         |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.367      |
|    value_loss           | 280        |
----------------------------------------
----------------------------------------
| reward                  | -3.62      |
| reward_contact          | -0.00109   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0522    |
| reward_torque           | -3.49      |
| reward_velocity         | 0.023      |
| rollout/                |            |
|    ep_len_mean          | 88.8       |
|    ep_rew_mean          | -280       |
| time/                   |            |
|    fps                  | 301        |
|    iterations           | 9          |
|    time_elapsed         | 30         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.08530077 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.574      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.3       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0976    |
|    std                  | 0.367      |
|    value_loss           | 140        |
----------------------------------------
-----------------------------------------
| reward                  | -3.63       |
| reward_contact          | -0.000614   |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0549     |
| reward_torque           | -3.49       |
| reward_velocity         | 0.0212      |
| rollout/                |             |
|    ep_len_mean          | 83.9        |
|    ep_rew_mean          | -266        |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 10          |
|    time_elapsed         | 34          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.091505446 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.169       |
|    learning_rate        | 0.0003      |
|    loss                 | 182         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.109      |
|    std                  | 0.367       |
|    value_loss           | 513         |
-----------------------------------------
----------------------------------------
| reward                  | -3.64      |
| reward_contact          | -0.000614  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0549    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.021      |
| rollout/                |            |
|    ep_len_mean          | 85.5       |
|    ep_rew_mean          | -271       |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 11         |
|    time_elapsed         | 37         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.08441191 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.559      |
|    learning_rate        | 0.0003     |
|    loss                 | 60         |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0764    |
|    std                  | 0.366      |
|    value_loss           | 382        |
----------------------------------------
Num timesteps: 12000
Best mean reward: -297.27 - Last mean reward per episode: -308.44
-----------------------------------------
| reward                  | -3.65       |
| reward_contact          | -0.00101    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0577     |
| reward_torque           | -3.52       |
| reward_velocity         | 0.0205      |
| rollout/                |             |
|    ep_len_mean          | 97.5        |
|    ep_rew_mean          | -308        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 12          |
|    time_elapsed         | 41          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.058747333 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.1       |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | 43.6        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0713     |
|    std                  | 0.366       |
|    value_loss           | 237         |
-----------------------------------------
----------------------------------------
| reward                  | -3.66      |
| reward_contact          | -0.00112   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0679    |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0191     |
| rollout/                |            |
|    ep_len_mean          | 89.3       |
|    ep_rew_mean          | -285       |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 13         |
|    time_elapsed         | 44         |
|    total_timesteps      | 13312      |
| train/                  |            |
|    approx_kl            | 0.06798034 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | 35.5       |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0833    |
|    std                  | 0.366      |
|    value_loss           | 209        |
----------------------------------------
----------------------------------------
| reward                  | -3.67      |
| reward_contact          | -0.00112   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0679    |
| reward_torque           | -3.52      |
| reward_velocity         | 0.019      |
| rollout/                |            |
|    ep_len_mean          | 81.3       |
|    ep_rew_mean          | -260       |
| time/                   |            |
|    fps                  | 294        |
|    iterations           | 14         |
|    time_elapsed         | 48         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.13578144 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.446      |
|    learning_rate        | 0.0003     |
|    loss                 | 37.2       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.113     |
|    std                  | 0.366      |
|    value_loss           | 201        |
----------------------------------------
----------------------------------------
| reward                  | -3.66      |
| reward_contact          | -0.000958  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0694    |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0176     |
| rollout/                |            |
|    ep_len_mean          | 83.3       |
|    ep_rew_mean          | -267       |
| time/                   |            |
|    fps                  | 293        |
|    iterations           | 15         |
|    time_elapsed         | 52         |
|    total_timesteps      | 15360      |
| train/                  |            |
|    approx_kl            | 0.07831302 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.436      |
|    learning_rate        | 0.0003     |
|    loss                 | 68.8       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0844    |
|    std                  | 0.366      |
|    value_loss           | 299        |
----------------------------------------
----------------------------------------
| reward                  | -3.66      |
| reward_contact          | -0.00129   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0676    |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0195     |
| rollout/                |            |
|    ep_len_mean          | 81.6       |
|    ep_rew_mean          | -262       |
| time/                   |            |
|    fps                  | 292        |
|    iterations           | 16         |
|    time_elapsed         | 55         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.05408106 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.0003     |
|    loss                 | 42         |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0729    |
|    std                  | 0.366      |
|    value_loss           | 247        |
----------------------------------------
-----------------------------------------
| reward                  | -3.63       |
| reward_contact          | -0.00132    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.076      |
| reward_torque           | -3.47       |
| reward_velocity         | 0.0193      |
| rollout/                |             |
|    ep_len_mean          | 81.5        |
|    ep_rew_mean          | -262        |
| time/                   |             |
|    fps                  | 292         |
|    iterations           | 17          |
|    time_elapsed         | 59          |
|    total_timesteps      | 17408       |
| train/                  |             |
|    approx_kl            | 0.098303586 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.458       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.6        |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.071      |
|    std                  | 0.366       |
|    value_loss           | 272         |
-----------------------------------------
Num timesteps: 18000
Best mean reward: -297.27 - Last mean reward per episode: -261.67
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.62      |
| reward_contact          | -0.00148   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.076     |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0192     |
| rollout/                |            |
|    ep_len_mean          | 87.5       |
|    ep_rew_mean          | -280       |
| time/                   |            |
|    fps                  | 291        |
|    iterations           | 18         |
|    time_elapsed         | 63         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.08449566 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.212      |
|    learning_rate        | 0.0003     |
|    loss                 | 36.6       |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0973    |
|    std                  | 0.366      |
|    value_loss           | 286        |
----------------------------------------
----------------------------------------
| reward                  | -3.65      |
| reward_contact          | -0.00148   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0831    |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0194     |
| rollout/                |            |
|    ep_len_mean          | 88.6       |
|    ep_rew_mean          | -285       |
| time/                   |            |
|    fps                  | 291        |
|    iterations           | 19         |
|    time_elapsed         | 66         |
|    total_timesteps      | 19456      |
| train/                  |            |
|    approx_kl            | 0.09105019 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.623      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.85       |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.366      |
|    value_loss           | 111        |
----------------------------------------
-----------------------------------------
| reward                  | -3.64       |
| reward_contact          | -0.00158    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.077      |
| reward_torque           | -3.48       |
| reward_velocity         | 0.0214      |
| rollout/                |             |
|    ep_len_mean          | 96          |
|    ep_rew_mean          | -308        |
| time/                   |             |
|    fps                  | 291         |
|    iterations           | 20          |
|    time_elapsed         | 70          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.076098844 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.1       |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.89        |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0995     |
|    std                  | 0.366       |
|    value_loss           | 131         |
-----------------------------------------
-----------------------------------------
| reward                  | -3.62       |
| reward_contact          | -0.00141    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0591     |
| reward_torque           | -3.48       |
| reward_velocity         | 0.0213      |
| rollout/                |             |
|    ep_len_mean          | 85.9        |
|    ep_rew_mean          | -277        |
| time/                   |             |
|    fps                  | 291         |
|    iterations           | 21          |
|    time_elapsed         | 73          |
|    total_timesteps      | 21504       |
| train/                  |             |
|    approx_kl            | 0.116045415 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.49        |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0975     |
|    std                  | 0.366       |
|    value_loss           | 79.1        |
-----------------------------------------
-----------------------------------------
| reward                  | -3.63       |
| reward_contact          | -0.00189    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0591     |
| reward_torque           | -3.49       |
| reward_velocity         | 0.0221      |
| rollout/                |             |
|    ep_len_mean          | 85.9        |
|    ep_rew_mean          | -277        |
| time/                   |             |
|    fps                  | 291         |
|    iterations           | 22          |
|    time_elapsed         | 77          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.068719216 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.448       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.5        |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0901     |
|    std                  | 0.366       |
|    value_loss           | 231         |
-----------------------------------------
----------------------------------------
| reward                  | -3.63      |
| reward_contact          | -0.00205   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0591    |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0221     |
| rollout/                |            |
|    ep_len_mean          | 96.4       |
|    ep_rew_mean          | -310       |
| time/                   |            |
|    fps                  | 290        |
|    iterations           | 23         |
|    time_elapsed         | 81         |
|    total_timesteps      | 23552      |
| train/                  |            |
|    approx_kl            | 0.08666834 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.0003     |
|    loss                 | 23         |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0861    |
|    std                  | 0.366      |
|    value_loss           | 348        |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_8
---------------------------------
| reward             | -3.58    |
| reward_contact     | -0.00667 |
| reward_ctrl        | -0.1     |
| reward_motion      | -0.1     |
| reward_torque      | -3.39    |
| reward_velocity    | 0.0173   |
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -101     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | -3.72      |
| reward_contact          | -0.00168   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.53      |
| reward_velocity         | 0.014      |
| rollout/                |            |
|    ep_len_mean          | 71.1       |
|    ep_rew_mean          | -222       |
| time/                   |            |
|    fps                  | 378        |
|    iterations           | 2          |
|    time_elapsed         | 5          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.10417083 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.0168     |
|    learning_rate        | 0.0003     |
|    loss                 | 65         |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.367      |
|    value_loss           | 475        |
----------------------------------------
----------------------------------------
| reward                  | -3.64      |
| reward_contact          | -0.00185   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0147     |
| rollout/                |            |
|    ep_len_mean          | 69.6       |
|    ep_rew_mean          | -220       |
| time/                   |            |
|    fps                  | 343        |
|    iterations           | 3          |
|    time_elapsed         | 8          |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.11021443 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | -0.334     |
|    learning_rate        | 0.0003     |
|    loss                 | 15.5       |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.367      |
|    value_loss           | 99.7       |
----------------------------------------
-----------------------------------------
| reward                  | -3.64       |
| reward_contact          | -0.0021     |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0882     |
| reward_torque           | -3.47       |
| reward_velocity         | 0.0174      |
| rollout/                |             |
|    ep_len_mean          | 58.7        |
|    ep_rew_mean          | -186        |
| time/                   |             |
|    fps                  | 329         |
|    iterations           | 4           |
|    time_elapsed         | 12          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.091333374 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | -0.0565     |
|    learning_rate        | 0.0003      |
|    loss                 | 36.8        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.117      |
|    std                  | 0.367       |
|    value_loss           | 162         |
-----------------------------------------
-----------------------------------------
| reward                  | -3.58       |
| reward_contact          | -0.00234    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.084      |
| reward_torque           | -3.41       |
| reward_velocity         | 0.0183      |
| rollout/                |             |
|    ep_len_mean          | 55.3        |
|    ep_rew_mean          | -176        |
| time/                   |             |
|    fps                  | 321         |
|    iterations           | 5           |
|    time_elapsed         | 15          |
|    total_timesteps      | 5120        |
| train/                  |             |
|    approx_kl            | 0.067936674 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | 0.222       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.8        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0818     |
|    std                  | 0.367       |
|    value_loss           | 237         |
-----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -138.63
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.51      |
| reward_contact          | -0.00219   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0743    |
| reward_torque           | -3.36      |
| reward_velocity         | 0.0206     |
| rollout/                |            |
|    ep_len_mean          | 42.8       |
|    ep_rew_mean          | -139       |
| time/                   |            |
|    fps                  | 315        |
|    iterations           | 6          |
|    time_elapsed         | 19         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.06220818 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.264      |
|    learning_rate        | 0.0003     |
|    loss                 | 88.8       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.11      |
|    std                  | 0.367      |
|    value_loss           | 254        |
----------------------------------------
----------------------------------------
| reward                  | -3.5       |
| reward_contact          | -0.00223   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0638    |
| reward_torque           | -3.35      |
| reward_velocity         | 0.0212     |
| rollout/                |            |
|    ep_len_mean          | 47.6       |
|    ep_rew_mean          | -153       |
| time/                   |            |
|    fps                  | 311        |
|    iterations           | 7          |
|    time_elapsed         | 23         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.09764336 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.0257     |
|    learning_rate        | 0.0003     |
|    loss                 | 59         |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.115     |
|    std                  | 0.367      |
|    value_loss           | 201        |
----------------------------------------
-----------------------------------------
| reward                  | -3.51       |
| reward_contact          | -0.00208    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0554     |
| reward_torque           | -3.38       |
| reward_velocity         | 0.0233      |
| rollout/                |             |
|    ep_len_mean          | 48.5        |
|    ep_rew_mean          | -157        |
| time/                   |             |
|    fps                  | 308         |
|    iterations           | 8           |
|    time_elapsed         | 26          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.096551545 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.5        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.113      |
|    std                  | 0.367       |
|    value_loss           | 132         |
-----------------------------------------
----------------------------------------
| reward                  | -3.51      |
| reward_contact          | -0.00208   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0522    |
| reward_torque           | -3.38      |
| reward_velocity         | 0.024      |
| rollout/                |            |
|    ep_len_mean          | 58.3       |
|    ep_rew_mean          | -187       |
| time/                   |            |
|    fps                  | 305        |
|    iterations           | 9          |
|    time_elapsed         | 30         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.07261966 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.298      |
|    learning_rate        | 0.0003     |
|    loss                 | 56.5       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.367      |
|    value_loss           | 248        |
----------------------------------------
---------------------------------------
| reward                  | -3.53     |
| reward_contact          | -0.00183  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0591   |
| reward_torque           | -3.39     |
| reward_velocity         | 0.0267    |
| rollout/                |           |
|    ep_len_mean          | 68        |
|    ep_rew_mean          | -217      |
| time/                   |           |
|    fps                  | 303       |
|    iterations           | 10        |
|    time_elapsed         | 33        |
|    total_timesteps      | 10240     |
| train/                  |           |
|    approx_kl            | 0.0808554 |
|    clip_fraction        | 0.162     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.826     |
|    learning_rate        | 0.0003    |
|    loss                 | 47        |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.0864   |
|    std                  | 0.367     |
|    value_loss           | 195       |
---------------------------------------
----------------------------------------
| reward                  | -3.54      |
| reward_contact          | -0.00183   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0591    |
| reward_torque           | -3.4       |
| reward_velocity         | 0.0263     |
| rollout/                |            |
|    ep_len_mean          | 71         |
|    ep_rew_mean          | -227       |
| time/                   |            |
|    fps                  | 301        |
|    iterations           | 11         |
|    time_elapsed         | 37         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.12363265 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.133      |
|    learning_rate        | 0.0003     |
|    loss                 | 76.2       |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.367      |
|    value_loss           | 263        |
----------------------------------------
Num timesteps: 12000
Best mean reward: -138.63 - Last mean reward per episode: -231.23
----------------------------------------
| reward                  | -3.54      |
| reward_contact          | -0.00204   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0591    |
| reward_torque           | -3.41      |
| reward_velocity         | 0.0255     |
| rollout/                |            |
|    ep_len_mean          | 73.6       |
|    ep_rew_mean          | -236       |
| time/                   |            |
|    fps                  | 299        |
|    iterations           | 12         |
|    time_elapsed         | 40         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.07578746 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.548      |
|    learning_rate        | 0.0003     |
|    loss                 | 27.8       |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.367      |
|    value_loss           | 206        |
----------------------------------------
-----------------------------------------
| reward                  | -3.59       |
| reward_contact          | -0.00237    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0516     |
| reward_torque           | -3.47       |
| reward_velocity         | 0.027       |
| rollout/                |             |
|    ep_len_mean          | 63.5        |
|    ep_rew_mean          | -205        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 13          |
|    time_elapsed         | 44          |
|    total_timesteps      | 13312       |
| train/                  |             |
|    approx_kl            | 0.062469497 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.7       |
|    explained_variance   | 0.157       |
|    learning_rate        | 0.0003      |
|    loss                 | 163         |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0867     |
|    std                  | 0.366       |
|    value_loss           | 456         |
-----------------------------------------
----------------------------------------
| reward                  | -3.6       |
| reward_contact          | -0.00237   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0621    |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0263     |
| rollout/                |            |
|    ep_len_mean          | 63.5       |
|    ep_rew_mean          | -204       |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 14         |
|    time_elapsed         | 48         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.07475457 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.293      |
|    learning_rate        | 0.0003     |
|    loss                 | 126        |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0937    |
|    std                  | 0.366      |
|    value_loss           | 506        |
----------------------------------------
----------------------------------------
| reward                  | -3.57      |
| reward_contact          | -0.00254   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0674    |
| reward_torque           | -3.43      |
| reward_velocity         | 0.0317     |
| rollout/                |            |
|    ep_len_mean          | 72.3       |
|    ep_rew_mean          | -231       |
| time/                   |            |
|    fps                  | 297        |
|    iterations           | 15         |
|    time_elapsed         | 51         |
|    total_timesteps      | 15360      |
| train/                  |            |
|    approx_kl            | 0.09738049 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.486      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.92       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.366      |
|    value_loss           | 90.2       |
----------------------------------------
----------------------------------------
| reward                  | -3.55      |
| reward_contact          | -0.00225   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0669    |
| reward_torque           | -3.41      |
| reward_velocity         | 0.0275     |
| rollout/                |            |
|    ep_len_mean          | 60.5       |
|    ep_rew_mean          | -196       |
| time/                   |            |
|    fps                  | 296        |
|    iterations           | 16         |
|    time_elapsed         | 55         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.07088742 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.486      |
|    learning_rate        | 0.0003     |
|    loss                 | 64.8       |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0858    |
|    std                  | 0.366      |
|    value_loss           | 371        |
----------------------------------------
-----------------------------------------
| reward                  | -3.54       |
| reward_contact          | -0.00205    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0669     |
| reward_torque           | -3.4        |
| reward_velocity         | 0.0292      |
| rollout/                |             |
|    ep_len_mean          | 52.5        |
|    ep_rew_mean          | -170        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 17          |
|    time_elapsed         | 58          |
|    total_timesteps      | 17408       |
| train/                  |             |
|    approx_kl            | 0.074232996 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.0003      |
|    loss                 | 41.9        |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0949     |
|    std                  | 0.366       |
|    value_loss           | 230         |
-----------------------------------------
Num timesteps: 18000
Best mean reward: -138.63 - Last mean reward per episode: -186.61
----------------------------------------
| reward                  | -3.56      |
| reward_contact          | -0.00203   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0854    |
| reward_torque           | -3.4       |
| reward_velocity         | 0.031      |
| rollout/                |            |
|    ep_len_mean          | 57.3       |
|    ep_rew_mean          | -185       |
| time/                   |            |
|    fps                  | 294        |
|    iterations           | 18         |
|    time_elapsed         | 62         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.06691042 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.749      |
|    learning_rate        | 0.0003     |
|    loss                 | 64.5       |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0853    |
|    std                  | 0.366      |
|    value_loss           | 376        |
----------------------------------------
----------------------------------------
| reward                  | -3.55      |
| reward_contact          | -0.00186   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0854    |
| reward_torque           | -3.39      |
| reward_velocity         | 0.0312     |
| rollout/                |            |
|    ep_len_mean          | 58.5       |
|    ep_rew_mean          | -190       |
| time/                   |            |
|    fps                  | 293        |
|    iterations           | 19         |
|    time_elapsed         | 66         |
|    total_timesteps      | 19456      |
| train/                  |            |
|    approx_kl            | 0.08361464 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.128      |
|    learning_rate        | 0.0003     |
|    loss                 | 52.1       |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.366      |
|    value_loss           | 293        |
----------------------------------------
-----------------------------------------
| reward                  | -3.57       |
| reward_contact          | -0.00168    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0789     |
| reward_torque           | -3.42       |
| reward_velocity         | 0.0275      |
| rollout/                |             |
|    ep_len_mean          | 59          |
|    ep_rew_mean          | -192        |
| time/                   |             |
|    fps                  | 291         |
|    iterations           | 20          |
|    time_elapsed         | 70          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.068159856 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.2       |
|    explained_variance   | 0.307       |
|    learning_rate        | 0.0003      |
|    loss                 | 33.3        |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.107      |
|    std                  | 0.366       |
|    value_loss           | 229         |
-----------------------------------------
----------------------------------------
| reward                  | -3.57      |
| reward_contact          | -0.00259   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0735    |
| reward_torque           | -3.42      |
| reward_velocity         | 0.03       |
| rollout/                |            |
|    ep_len_mean          | 68.6       |
|    ep_rew_mean          | -222       |
| time/                   |            |
|    fps                  | 291        |
|    iterations           | 21         |
|    time_elapsed         | 73         |
|    total_timesteps      | 21504      |
| train/                  |            |
|    approx_kl            | 0.07058899 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.627      |
|    learning_rate        | 0.0003     |
|    loss                 | 45.4       |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.094     |
|    std                  | 0.366      |
|    value_loss           | 168        |
----------------------------------------
-----------------------------------------
| reward                  | -3.58       |
| reward_contact          | -0.00308    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0683     |
| reward_torque           | -3.43       |
| reward_velocity         | 0.0233      |
| rollout/                |             |
|    ep_len_mean          | 58.2        |
|    ep_rew_mean          | -189        |
| time/                   |             |
|    fps                  | 291         |
|    iterations           | 22          |
|    time_elapsed         | 77          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.072769254 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.6       |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.4        |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0982     |
|    std                  | 0.366       |
|    value_loss           | 91.2        |
-----------------------------------------
----------------------------------------
| reward                  | -3.65      |
| reward_contact          | -0.00332   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0683    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.019      |
| rollout/                |            |
|    ep_len_mean          | 52.8       |
|    ep_rew_mean          | -173       |
| time/                   |            |
|    fps                  | 290        |
|    iterations           | 23         |
|    time_elapsed         | 80         |
|    total_timesteps      | 23552      |
| train/                  |            |
|    approx_kl            | 0.06702521 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.212      |
|    learning_rate        | 0.0003     |
|    loss                 | 50.8       |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0992    |
|    std                  | 0.366      |
|    value_loss           | 342        |
----------------------------------------
Num timesteps: 24000
Best mean reward: -138.63 - Last mean reward per episode: -172.74
----------------------------------------
| reward                  | -3.63      |
| reward_contact          | -0.00318   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0551    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.024      |
| rollout/                |            |
|    ep_len_mean          | 62         |
|    ep_rew_mean          | -202       |
| time/                   |            |
|    fps                  | 290        |
|    iterations           | 24         |
|    time_elapsed         | 84         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.08230947 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.62       |
|    learning_rate        | 0.0003     |
|    loss                 | 26.1       |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.11      |
|    std                  | 0.366      |
|    value_loss           | 221        |
----------------------------------------
----------------------------------------
| reward                  | -3.63      |
| reward_contact          | -0.00277   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0627    |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0222     |
| rollout/                |            |
|    ep_len_mean          | 67.5       |
|    ep_rew_mean          | -218       |
| time/                   |            |
|    fps                  | 289        |
|    iterations           | 25         |
|    time_elapsed         | 88         |
|    total_timesteps      | 25600      |
| train/                  |            |
|    approx_kl            | 0.06597649 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.55       |
|    learning_rate        | 0.0003     |
|    loss                 | 26.5       |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0866    |
|    std                  | 0.366      |
|    value_loss           | 260        |
----------------------------------------
-----------------------------------------
| reward                  | -3.6        |
| reward_contact          | -0.00275    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0541     |
| reward_torque           | -3.47       |
| reward_velocity         | 0.0223      |
| rollout/                |             |
|    ep_len_mean          | 67.4        |
|    ep_rew_mean          | -218        |
| time/                   |             |
|    fps                  | 289         |
|    iterations           | 26          |
|    time_elapsed         | 92          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.072470814 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.314       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.4        |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.104      |
|    std                  | 0.366       |
|    value_loss           | 158         |
-----------------------------------------
----------------------------------------
| reward                  | -3.62      |
| reward_contact          | -0.00205   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0632    |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0188     |
| rollout/                |            |
|    ep_len_mean          | 55.9       |
|    ep_rew_mean          | -182       |
| time/                   |            |
|    fps                  | 288        |
|    iterations           | 27         |
|    time_elapsed         | 95         |
|    total_timesteps      | 27648      |
| train/                  |            |
|    approx_kl            | 0.08116076 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.488      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.8       |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.088     |
|    std                  | 0.366      |
|    value_loss           | 177        |
----------------------------------------
-----------------------------------------
| reward                  | -3.6        |
| reward_contact          | -0.00184    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0526     |
| reward_torque           | -3.47       |
| reward_velocity         | 0.0199      |
| rollout/                |             |
|    ep_len_mean          | 66.3        |
|    ep_rew_mean          | -215        |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 28          |
|    time_elapsed         | 99          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.069119915 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.5       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 15.1        |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0947     |
|    std                  | 0.366       |
|    value_loss           | 126         |
-----------------------------------------
----------------------------------------
| reward                  | -3.6       |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0526    |
| reward_torque           | -3.47      |
| reward_velocity         | 0.02       |
| rollout/                |            |
|    ep_len_mean          | 76.5       |
|    ep_rew_mean          | -247       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 29         |
|    time_elapsed         | 103        |
|    total_timesteps      | 29696      |
| train/                  |            |
|    approx_kl            | 0.07737486 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.348      |
|    learning_rate        | 0.0003     |
|    loss                 | 24         |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.366      |
|    value_loss           | 203        |
----------------------------------------
Num timesteps: 30000
Best mean reward: -138.63 - Last mean reward per episode: -253.93
----------------------------------------
| reward                  | -3.63      |
| reward_contact          | -0.00161   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0551    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.023      |
| rollout/                |            |
|    ep_len_mean          | 79.2       |
|    ep_rew_mean          | -256       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 30         |
|    time_elapsed         | 107        |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.06954172 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.682      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.17       |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.0992    |
|    std                  | 0.366      |
|    value_loss           | 72.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.64      |
| reward_contact          | -0.00137   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0551    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.0228     |
| rollout/                |            |
|    ep_len_mean          | 87.7       |
|    ep_rew_mean          | -283       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 31         |
|    time_elapsed         | 110        |
|    total_timesteps      | 31744      |
| train/                  |            |
|    approx_kl            | 0.10670468 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | -0.0102    |
|    learning_rate        | 0.0003     |
|    loss                 | 7.05       |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.366      |
|    value_loss           | 133        |
----------------------------------------
----------------------------------------
| reward                  | -3.63      |
| reward_contact          | -0.00137   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0477    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.0226     |
| rollout/                |            |
|    ep_len_mean          | 97.9       |
|    ep_rew_mean          | -316       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 32         |
|    time_elapsed         | 114        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.07300088 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.6      |
|    explained_variance   | 0.197      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.67       |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0911    |
|    std                  | 0.366      |
|    value_loss           | 84.7       |
----------------------------------------
-----------------------------------------
| reward                  | -3.62       |
| reward_contact          | -0.00136    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0477     |
| reward_torque           | -3.5        |
| reward_velocity         | 0.0231      |
| rollout/                |             |
|    ep_len_mean          | 103         |
|    ep_rew_mean          | -333        |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 33          |
|    time_elapsed         | 118         |
|    total_timesteps      | 33792       |
| train/                  |             |
|    approx_kl            | 0.088946536 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.6       |
|    explained_variance   | -1.07       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.7         |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.109      |
|    std                  | 0.366       |
|    value_loss           | 73.2        |
-----------------------------------------
-----------------------------------------
| reward                  | -3.6        |
| reward_contact          | -0.0014     |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0455     |
| reward_torque           | -3.47       |
| reward_velocity         | 0.0233      |
| rollout/                |             |
|    ep_len_mean          | 103         |
|    ep_rew_mean          | -334        |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 34          |
|    time_elapsed         | 121         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.097367436 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.2       |
|    explained_variance   | 0.118       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.5         |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.119      |
|    std                  | 0.366       |
|    value_loss           | 139         |
-----------------------------------------
----------------------------------------
| reward                  | -3.61      |
| reward_contact          | -0.0014    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0492    |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0235     |
| rollout/                |            |
|    ep_len_mean          | 113        |
|    ep_rew_mean          | -365       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 35         |
|    time_elapsed         | 125        |
|    total_timesteps      | 35840      |
| train/                  |            |
|    approx_kl            | 0.09249616 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.205      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.08       |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.366      |
|    value_loss           | 160        |
----------------------------------------
Num timesteps: 36000
Best mean reward: -138.63 - Last mean reward per episode: -346.75
-----------------------------------------
| reward                  | -3.66       |
| reward_contact          | -0.00169    |
| reward_ctrl             | -0.0909     |
| reward_motion           | -0.0639     |
| reward_torque           | -3.53       |
| reward_velocity         | 0.0223      |
| rollout/                |             |
|    ep_len_mean          | 95.8        |
|    ep_rew_mean          | -311        |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 36          |
|    time_elapsed         | 129         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.070547566 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.4       |
|    explained_variance   | 0.312       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.49        |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.101      |
|    std                  | 0.366       |
|    value_loss           | 77.4        |
-----------------------------------------
----------------------------------------
| reward                  | -3.68      |
| reward_contact          | -0.00182   |
| reward_ctrl             | -0.0909    |
| reward_motion           | -0.0722    |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0209     |
| rollout/                |            |
|    ep_len_mean          | 93.8       |
|    ep_rew_mean          | -304       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 37         |
|    time_elapsed         | 132        |
|    total_timesteps      | 37888      |
| train/                  |            |
|    approx_kl            | 0.06791668 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.176      |
|    learning_rate        | 0.0003     |
|    loss                 | 37.4       |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.366      |
|    value_loss           | 388        |
----------------------------------------
-----------------------------------------
| reward                  | -3.69       |
| reward_contact          | -0.00227    |
| reward_ctrl             | -0.0909     |
| reward_motion           | -0.0752     |
| reward_torque           | -3.55       |
| reward_velocity         | 0.0225      |
| rollout/                |             |
|    ep_len_mean          | 59.6        |
|    ep_rew_mean          | -196        |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 38          |
|    time_elapsed         | 136         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.099047005 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.5       |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.61        |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.109      |
|    std                  | 0.366       |
|    value_loss           | 147         |
-----------------------------------------
----------------------------------------
| reward                  | -3.67      |
| reward_contact          | -0.00228   |
| reward_ctrl             | -0.0909    |
| reward_motion           | -0.0647    |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0225     |
| rollout/                |            |
|    ep_len_mean          | 57.5       |
|    ep_rew_mean          | -189       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 39         |
|    time_elapsed         | 139        |
|    total_timesteps      | 39936      |
| train/                  |            |
|    approx_kl            | 0.07516223 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.101      |
|    learning_rate        | 0.0003     |
|    loss                 | 38.7       |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.366      |
|    value_loss           | 221        |
----------------------------------------
----------------------------------------
| reward                  | -3.7       |
| reward_contact          | -0.0023    |
| reward_ctrl             | -0.0909    |
| reward_motion           | -0.0739    |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0233     |
| rollout/                |            |
|    ep_len_mean          | 47.3       |
|    ep_rew_mean          | -157       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 40         |
|    time_elapsed         | 143        |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.10898656 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.766      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.14       |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.113     |
|    std                  | 0.366      |
|    value_loss           | 103        |
----------------------------------------
----------------------------------------
| reward                  | -3.7       |
| reward_contact          | -0.00206   |
| reward_ctrl             | -0.0909    |
| reward_motion           | -0.0658    |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0246     |
| rollout/                |            |
|    ep_len_mean          | 56.9       |
|    ep_rew_mean          | -187       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 41         |
|    time_elapsed         | 146        |
|    total_timesteps      | 41984      |
| train/                  |            |
|    approx_kl            | 0.11880714 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.326      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.7       |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.366      |
|    value_loss           | 220        |
----------------------------------------
Num timesteps: 42000
Best mean reward: -138.63 - Last mean reward per episode: -187.23
----------------------------------------
| reward                  | -3.68      |
| reward_contact          | -0.00186   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0608    |
| reward_torque           | -3.54      |
| reward_velocity         | 0.024      |
| rollout/                |            |
|    ep_len_mean          | 52         |
|    ep_rew_mean          | -172       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 42         |
|    time_elapsed         | 150        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.10621195 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23.1      |
|    explained_variance   | 0.657      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.87       |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.366      |
|    value_loss           | 75.6       |
----------------------------------------
----------------------------------------
| reward                  | -3.64      |
| reward_contact          | -0.0019    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0487    |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0213     |
| rollout/                |            |
|    ep_len_mean          | 51.9       |
|    ep_rew_mean          | -172       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 43         |
|    time_elapsed         | 153        |
|    total_timesteps      | 44032      |
| train/                  |            |
|    approx_kl            | 0.07928381 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | -0.056     |
|    learning_rate        | 0.0003     |
|    loss                 | 15.3       |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.366      |
|    value_loss           | 233        |
----------------------------------------
----------------------------------------
| reward                  | -3.64      |
| reward_contact          | -0.00247   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0697    |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0155     |
| rollout/                |            |
|    ep_len_mean          | 27         |
|    ep_rew_mean          | -93.1      |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 44         |
|    time_elapsed         | 157        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.07353075 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | -0.095     |
|    learning_rate        | 0.0003     |
|    loss                 | 20.5       |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.366      |
|    value_loss           | 273        |
----------------------------------------
----------------------------------------
| reward                  | -3.64      |
| reward_contact          | -0.00247   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0697    |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0137     |
| rollout/                |            |
|    ep_len_mean          | 27.8       |
|    ep_rew_mean          | -95.7      |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 45         |
|    time_elapsed         | 161        |
|    total_timesteps      | 46080      |
| train/                  |            |
|    approx_kl            | 0.07727442 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.0922     |
|    learning_rate        | 0.0003     |
|    loss                 | 25.5       |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.365      |
|    value_loss           | 242        |
----------------------------------------
----------------------------------------
| reward                  | -3.63      |
| reward_contact          | -0.00214   |
| reward_ctrl             | -0.0963    |
| reward_motion           | -0.0591    |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0161     |
| rollout/                |            |
|    ep_len_mean          | 36.9       |
|    ep_rew_mean          | -124       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 46         |
|    time_elapsed         | 164        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.09110732 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.372      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.78       |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.365      |
|    value_loss           | 118        |
----------------------------------------
Num timesteps: 48000
Best mean reward: -138.63 - Last mean reward per episode: -156.28
----------------------------------------
| reward                  | -3.65      |
| reward_contact          | -0.00254   |
| reward_ctrl             | -0.0963    |
| reward_motion           | -0.0528    |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0167     |
| rollout/                |            |
|    ep_len_mean          | 46.9       |
|    ep_rew_mean          | -156       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 47         |
|    time_elapsed         | 168        |
|    total_timesteps      | 48128      |
| train/                  |            |
|    approx_kl            | 0.11572909 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.498      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.65       |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.365      |
|    value_loss           | 134        |
----------------------------------------
----------------------------------------
| reward                  | -3.62      |
| reward_contact          | -0.00185   |
| reward_ctrl             | -0.0963    |
| reward_motion           | -0.0623    |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0161     |
| rollout/                |            |
|    ep_len_mean          | 57.2       |
|    ep_rew_mean          | -189       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 48         |
|    time_elapsed         | 172        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.07793903 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.644      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.54       |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.365      |
|    value_loss           | 96.7       |
----------------------------------------
----------------------------------------
| reward                  | -3.63      |
| reward_contact          | -0.00185   |
| reward_ctrl             | -0.0963    |
| reward_motion           | -0.0623    |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0159     |
| rollout/                |            |
|    ep_len_mean          | 57.4       |
|    ep_rew_mean          | -189       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 49         |
|    time_elapsed         | 176        |
|    total_timesteps      | 50176      |
| train/                  |            |
|    approx_kl            | 0.15945438 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.15       |
|    learning_rate        | 0.0003     |
|    loss                 | 10.9       |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.14      |
|    std                  | 0.365      |
|    value_loss           | 169        |
----------------------------------------
----------------------------------------
| reward                  | -3.6       |
| reward_contact          | -0.00176   |
| reward_ctrl             | -0.0963    |
| reward_motion           | -0.0636    |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0162     |
| rollout/                |            |
|    ep_len_mean          | 74.6       |
|    ep_rew_mean          | -245       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 50         |
|    time_elapsed         | 179        |
|    total_timesteps      | 51200      |
| train/                  |            |
|    approx_kl            | 0.09482075 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.7      |
|    explained_variance   | 0.406      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.38       |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.126     |
|    std                  | 0.365      |
|    value_loss           | 54.8       |
----------------------------------------
-----------------------------------------
| reward                  | -3.62       |
| reward_contact          | -0.00192    |
| reward_ctrl             | -0.0963     |
| reward_motion           | -0.0586     |
| reward_torque           | -3.48       |
| reward_velocity         | 0.0178      |
| rollout/                |             |
|    ep_len_mean          | 84.8        |
|    ep_rew_mean          | -278        |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 51          |
|    time_elapsed         | 183         |
|    total_timesteps      | 52224       |
| train/                  |             |
|    approx_kl            | 0.058142394 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.5       |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.3        |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.0863     |
|    std                  | 0.365       |
|    value_loss           | 213         |
-----------------------------------------
---------------------------------------
| reward                  | -3.61     |
| reward_contact          | -0.00192  |
| reward_ctrl             | -0.0963   |
| reward_motion           | -0.0478   |
| reward_torque           | -3.49     |
| reward_velocity         | 0.0197    |
| rollout/                |           |
|    ep_len_mean          | 94.8      |
|    ep_rew_mean          | -309      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 52        |
|    time_elapsed         | 186       |
|    total_timesteps      | 53248     |
| train/                  |           |
|    approx_kl            | 0.1113736 |
|    clip_fraction        | 0.189     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.9     |
|    explained_variance   | 0.827     |
|    learning_rate        | 0.0003    |
|    loss                 | 12.5      |
|    n_updates            | 1020      |
|    policy_gradient_loss | -0.108    |
|    std                  | 0.365     |
|    value_loss           | 98.1      |
---------------------------------------
Num timesteps: 54000
Best mean reward: -138.63 - Last mean reward per episode: -311.77
----------------------------------------
| reward                  | -3.63      |
| reward_contact          | -0.00216   |
| reward_ctrl             | -0.0963    |
| reward_motion           | -0.0511    |
| reward_torque           | -3.5       |
| reward_velocity         | 0.0193     |
| rollout/                |            |
|    ep_len_mean          | 95.6       |
|    ep_rew_mean          | -312       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 53         |
|    time_elapsed         | 190        |
|    total_timesteps      | 54272      |
| train/                  |            |
|    approx_kl            | 0.09085918 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.697      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.2       |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.104     |
|    std                  | 0.365      |
|    value_loss           | 133        |
----------------------------------------
----------------------------------------
| reward                  | -3.64      |
| reward_contact          | -0.00204   |
| reward_ctrl             | -0.0963    |
| reward_motion           | -0.0465    |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0225     |
| rollout/                |            |
|    ep_len_mean          | 105        |
|    ep_rew_mean          | -343       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 54         |
|    time_elapsed         | 193        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.08517785 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.721      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.16       |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.365      |
|    value_loss           | 130        |
----------------------------------------
----------------------------------------
| reward                  | -3.68      |
| reward_contact          | -0.0018    |
| reward_ctrl             | -0.0963    |
| reward_motion           | -0.0411    |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0239     |
| rollout/                |            |
|    ep_len_mean          | 115        |
|    ep_rew_mean          | -371       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 55         |
|    time_elapsed         | 197        |
|    total_timesteps      | 56320      |
| train/                  |            |
|    approx_kl            | 0.07932863 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.409      |
|    learning_rate        | 0.0003     |
|    loss                 | 31.1       |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.365      |
|    value_loss           | 302        |
----------------------------------------
---------------------------------------
| reward                  | -3.68     |
| reward_contact          | -0.0018   |
| reward_ctrl             | -0.0963   |
| reward_motion           | -0.0411   |
| reward_torque           | -3.57     |
| reward_velocity         | 0.0233    |
| rollout/                |           |
|    ep_len_mean          | 114       |
|    ep_rew_mean          | -370      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 56        |
|    time_elapsed         | 201       |
|    total_timesteps      | 57344     |
| train/                  |           |
|    approx_kl            | 0.0811523 |
|    clip_fraction        | 0.2       |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.746     |
|    learning_rate        | 0.0003    |
|    loss                 | 18.3      |
|    n_updates            | 1100      |
|    policy_gradient_loss | -0.0949   |
|    std                  | 0.365     |
|    value_loss           | 195       |
---------------------------------------
----------------------------------------
| reward                  | -3.71      |
| reward_contact          | -0.00155   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0464    |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0217     |
| rollout/                |            |
|    ep_len_mean          | 91.9       |
|    ep_rew_mean          | -299       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 57         |
|    time_elapsed         | 204        |
|    total_timesteps      | 58368      |
| train/                  |            |
|    approx_kl            | 0.09470156 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.722      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.53       |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.121     |
|    std                  | 0.365      |
|    value_loss           | 80.7       |
----------------------------------------
-----------------------------------------
| reward                  | -3.69       |
| reward_contact          | -0.00155    |
| reward_ctrl             | -0.0899     |
| reward_motion           | -0.0429     |
| reward_torque           | -3.58       |
| reward_velocity         | 0.0238      |
| rollout/                |             |
|    ep_len_mean          | 91.2        |
|    ep_rew_mean          | -296        |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 58          |
|    time_elapsed         | 208         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.069616064 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | -0.0606     |
|    learning_rate        | 0.0003      |
|    loss                 | 30.2        |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.104      |
|    std                  | 0.365       |
|    value_loss           | 409         |
-----------------------------------------
Num timesteps: 60000
Best mean reward: -138.63 - Last mean reward per episode: -208.97
-----------------------------------------
| reward                  | -3.71       |
| reward_contact          | -0.00148    |
| reward_ctrl             | -0.0899     |
| reward_motion           | -0.0577     |
| reward_torque           | -3.58       |
| reward_velocity         | 0.0223      |
| rollout/                |             |
|    ep_len_mean          | 63.8        |
|    ep_rew_mean          | -209        |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 59          |
|    time_elapsed         | 211         |
|    total_timesteps      | 60416       |
| train/                  |             |
|    approx_kl            | 0.077760346 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.4       |
|    explained_variance   | 0.512       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.3         |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.0975     |
|    std                  | 0.365       |
|    value_loss           | 116         |
-----------------------------------------
-----------------------------------------
| reward                  | -3.68       |
| reward_contact          | -0.00124    |
| reward_ctrl             | -0.0899     |
| reward_motion           | -0.0521     |
| reward_torque           | -3.56       |
| reward_velocity         | 0.0233      |
| rollout/                |             |
|    ep_len_mean          | 74.7        |
|    ep_rew_mean          | -244        |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 60          |
|    time_elapsed         | 215         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.092641875 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.2       |
|    explained_variance   | 0.439       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.35        |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.116      |
|    std                  | 0.365       |
|    value_loss           | 114         |
-----------------------------------------
----------------------------------------
| reward                  | -3.66      |
| reward_contact          | -0.00119   |
| reward_ctrl             | -0.0899    |
| reward_motion           | -0.0496    |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0222     |
| rollout/                |            |
|    ep_len_mean          | 75.4       |
|    ep_rew_mean          | -246       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 61         |
|    time_elapsed         | 218        |
|    total_timesteps      | 62464      |
| train/                  |            |
|    approx_kl            | 0.09082776 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.34       |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.365      |
|    value_loss           | 162        |
----------------------------------------
----------------------------------------
| reward                  | -3.68      |
| reward_contact          | -0.00183   |
| reward_ctrl             | -0.0899    |
| reward_motion           | -0.0572    |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0227     |
| rollout/                |            |
|    ep_len_mean          | 55.7       |
|    ep_rew_mean          | -184       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 62         |
|    time_elapsed         | 222        |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.08267109 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.087      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.7       |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.123     |
|    std                  | 0.365      |
|    value_loss           | 247        |
----------------------------------------
----------------------------------------
| reward                  | -3.67      |
| reward_contact          | -0.00165   |
| reward_ctrl             | -0.0899    |
| reward_motion           | -0.0572    |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0228     |
| rollout/                |            |
|    ep_len_mean          | 55.1       |
|    ep_rew_mean          | -182       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 63         |
|    time_elapsed         | 225        |
|    total_timesteps      | 64512      |
| train/                  |            |
|    approx_kl            | 0.06325209 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.0251     |
|    learning_rate        | 0.0003     |
|    loss                 | 19.8       |
|    n_updates            | 1240       |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.365      |
|    value_loss           | 336        |
----------------------------------------
----------------------------------------
| reward                  | -3.68      |
| reward_contact          | -0.00218   |
| reward_ctrl             | -0.0801    |
| reward_motion           | -0.0461    |
| reward_torque           | -3.57      |
| reward_velocity         | 0.0245     |
| rollout/                |            |
|    ep_len_mean          | 66.3       |
|    ep_rew_mean          | -218       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 64         |
|    time_elapsed         | 229        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.09174758 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.26       |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.124     |
|    std                  | 0.365      |
|    value_loss           | 67.7       |
----------------------------------------
Num timesteps: 66000
Best mean reward: -138.63 - Last mean reward per episode: -217.63
----------------------------------------
| reward                  | -3.73      |
| reward_contact          | -0.00215   |
| reward_ctrl             | -0.0902    |
| reward_motion           | -0.0697    |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0273     |
| rollout/                |            |
|    ep_len_mean          | 43.9       |
|    ep_rew_mean          | -146       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 65         |
|    time_elapsed         | 233        |
|    total_timesteps      | 66560      |
| train/                  |            |
|    approx_kl            | 0.09963308 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.97       |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.127     |
|    std                  | 0.365      |
|    value_loss           | 117        |
----------------------------------------
----------------------------------------
| reward                  | -3.73      |
| reward_contact          | -0.00215   |
| reward_ctrl             | -0.0902    |
| reward_motion           | -0.0697    |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0273     |
| rollout/                |            |
|    ep_len_mean          | 53.6       |
|    ep_rew_mean          | -177       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 66         |
|    time_elapsed         | 236        |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.11569247 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.147      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.64       |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.141     |
|    std                  | 0.365      |
|    value_loss           | 211        |
----------------------------------------
----------------------------------------
| reward                  | -3.71      |
| reward_contact          | -0.00196   |
| reward_ctrl             | -0.0902    |
| reward_motion           | -0.0736    |
| reward_torque           | -3.57      |
| reward_velocity         | 0.0272     |
| rollout/                |            |
|    ep_len_mean          | 53.3       |
|    ep_rew_mean          | -176       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 67         |
|    time_elapsed         | 240        |
|    total_timesteps      | 68608      |
| train/                  |            |
|    approx_kl            | 0.09667668 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.86       |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.365      |
|    value_loss           | 111        |
----------------------------------------
-----------------------------------------
| reward                  | -3.69       |
| reward_contact          | -0.00152    |
| reward_ctrl             | -0.0902     |
| reward_motion           | -0.0636     |
| reward_torque           | -3.56       |
| reward_velocity         | 0.028       |
| rollout/                |             |
|    ep_len_mean          | 63.4        |
|    ep_rew_mean          | -208        |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 68          |
|    time_elapsed         | 244         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.121750504 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.508       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.34        |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.138      |
|    std                  | 0.365       |
|    value_loss           | 142         |
-----------------------------------------
----------------------------------------
| reward                  | -3.69      |
| reward_contact          | -0.00156   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0686    |
| reward_torque           | -3.54      |
| reward_velocity         | 0.023      |
| rollout/                |            |
|    ep_len_mean          | 44.3       |
|    ep_rew_mean          | -148       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 69         |
|    time_elapsed         | 247        |
|    total_timesteps      | 70656      |
| train/                  |            |
|    approx_kl            | 0.18296453 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.88       |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.157     |
|    std                  | 0.365      |
|    value_loss           | 140        |
----------------------------------------
----------------------------------------
| reward                  | -3.65      |
| reward_contact          | -0.00135   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0492    |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0245     |
| rollout/                |            |
|    ep_len_mean          | 54         |
|    ep_rew_mean          | -178       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 70         |
|    time_elapsed         | 251        |
|    total_timesteps      | 71680      |
| train/                  |            |
|    approx_kl            | 0.11368002 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.91       |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.148     |
|    std                  | 0.365      |
|    value_loss           | 166        |
----------------------------------------
Num timesteps: 72000
Best mean reward: -138.63 - Last mean reward per episode: -182.54
----------------------------------------
| reward                  | -3.73      |
| reward_contact          | -0.00186   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0371    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.0248     |
| rollout/                |            |
|    ep_len_mean          | 46.6       |
|    ep_rew_mean          | -156       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 71         |
|    time_elapsed         | 254        |
|    total_timesteps      | 72704      |
| train/                  |            |
|    approx_kl            | 0.16698757 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.722      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.12       |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.365      |
|    value_loss           | 113        |
----------------------------------------
-----------------------------------------
| reward                  | -3.73       |
| reward_contact          | -0.00209    |
| reward_ctrl             | -0.1        |
| reward_motion           | -0.0445     |
| reward_torque           | -3.6        |
| reward_velocity         | 0.0237      |
| rollout/                |             |
|    ep_len_mean          | 38.4        |
|    ep_rew_mean          | -131        |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 72          |
|    time_elapsed         | 258         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.085742936 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.3       |
|    explained_variance   | 0.0433      |
|    learning_rate        | 0.0003      |
|    loss                 | 12.4        |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.125      |
|    std                  | 0.365       |
|    value_loss           | 213         |
-----------------------------------------
----------------------------------------
| reward                  | -3.76      |
| reward_contact          | -0.0022    |
| reward_ctrl             | -0.0916    |
| reward_motion           | -0.0591    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0253     |
| rollout/                |            |
|    ep_len_mean          | 45.5       |
|    ep_rew_mean          | -152       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 73         |
|    time_elapsed         | 261        |
|    total_timesteps      | 74752      |
| train/                  |            |
|    approx_kl            | 0.12684867 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.715      |
|    learning_rate        | 0.0003     |
|    loss                 | 11         |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.365      |
|    value_loss           | 137        |
----------------------------------------
----------------------------------------
| reward                  | -3.76      |
| reward_contact          | -0.00196   |
| reward_ctrl             | -0.0916    |
| reward_motion           | -0.0591    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0245     |
| rollout/                |            |
|    ep_len_mean          | 36.2       |
|    ep_rew_mean          | -123       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 74         |
|    time_elapsed         | 265        |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.18529704 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.41       |
|    learning_rate        | 0.0003     |
|    loss                 | 8.06       |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.141     |
|    std                  | 0.365      |
|    value_loss           | 191        |
----------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.00184   |
| reward_ctrl             | -0.0916    |
| reward_motion           | -0.0729    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.0265     |
| rollout/                |            |
|    ep_len_mean          | 45.2       |
|    ep_rew_mean          | -152       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 75         |
|    time_elapsed         | 268        |
|    total_timesteps      | 76800      |
| train/                  |            |
|    approx_kl            | 0.17445703 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.446      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.09       |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.151     |
|    std                  | 0.365      |
|    value_loss           | 116        |
----------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00184   |
| reward_ctrl             | -0.0916    |
| reward_motion           | -0.0668    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.028      |
| rollout/                |            |
|    ep_len_mean          | 55         |
|    ep_rew_mean          | -183       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 76         |
|    time_elapsed         | 272        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.13276449 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.614      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.05       |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.151     |
|    std                  | 0.365      |
|    value_loss           | 156        |
----------------------------------------
Num timesteps: 78000
Best mean reward: -138.63 - Last mean reward per episode: -182.60
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00193   |
| reward_ctrl             | -0.0825    |
| reward_motion           | -0.0623    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0304     |
| rollout/                |            |
|    ep_len_mean          | 64.5       |
|    ep_rew_mean          | -212       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 77         |
|    time_elapsed         | 275        |
|    total_timesteps      | 78848      |
| train/                  |            |
|    approx_kl            | 0.10273191 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.12       |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.131     |
|    std                  | 0.365      |
|    value_loss           | 129        |
----------------------------------------
---------------------------------------
| reward                  | -3.74     |
| reward_contact          | -0.00101  |
| reward_ctrl             | -0.0909   |
| reward_motion           | -0.0549   |
| reward_torque           | -3.62     |
| reward_velocity         | 0.0287    |
| rollout/                |           |
|    ep_len_mean          | 52.5      |
|    ep_rew_mean          | -173      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 78        |
|    time_elapsed         | 279       |
|    total_timesteps      | 79872     |
| train/                  |           |
|    approx_kl            | 0.1510182 |
|    clip_fraction        | 0.303     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.778     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.46      |
|    n_updates            | 1540      |
|    policy_gradient_loss | -0.142    |
|    std                  | 0.365     |
|    value_loss           | 130       |
---------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.00125   |
| reward_ctrl             | -0.0909    |
| reward_motion           | -0.0549    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0285     |
| rollout/                |            |
|    ep_len_mean          | 53.6       |
|    ep_rew_mean          | -177       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 79         |
|    time_elapsed         | 283        |
|    total_timesteps      | 80896      |
| train/                  |            |
|    approx_kl            | 0.11147174 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.0869     |
|    learning_rate        | 0.0003     |
|    loss                 | 8.01       |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.146     |
|    std                  | 0.365      |
|    value_loss           | 149        |
----------------------------------------
----------------------------------------
| reward                  | -3.78      |
| reward_contact          | -0.00159   |
| reward_ctrl             | -0.0909    |
| reward_motion           | -0.0579    |
| reward_torque           | -3.66      |
| reward_velocity         | 0.0253     |
| rollout/                |            |
|    ep_len_mean          | 63.6       |
|    ep_rew_mean          | -208       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 80         |
|    time_elapsed         | 286        |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.14892924 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.682      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.52       |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.144     |
|    std                  | 0.365      |
|    value_loss           | 73.1       |
----------------------------------------
----------------------------------------
| reward                  | -3.79      |
| reward_contact          | -0.00186   |
| reward_ctrl             | -0.0909    |
| reward_motion           | -0.0726    |
| reward_torque           | -3.65      |
| reward_velocity         | 0.019      |
| rollout/                |            |
|    ep_len_mean          | 43         |
|    ep_rew_mean          | -143       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 81         |
|    time_elapsed         | 290        |
|    total_timesteps      | 82944      |
| train/                  |            |
|    approx_kl            | 0.15100369 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.54       |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.151     |
|    std                  | 0.365      |
|    value_loss           | 76.8       |
----------------------------------------
----------------------------------------
| reward                  | -3.78      |
| reward_contact          | -0.00156   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0771    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0148     |
| rollout/                |            |
|    ep_len_mean          | 42.7       |
|    ep_rew_mean          | -142       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 82         |
|    time_elapsed         | 293        |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.16122808 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.488      |
|    learning_rate        | 0.0003     |
|    loss                 | 10         |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.365      |
|    value_loss           | 144        |
----------------------------------------
Num timesteps: 84000
Best mean reward: -138.63 - Last mean reward per episode: -142.50
----------------------------------------
| reward                  | -3.78      |
| reward_contact          | -0.00177   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.061     |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 52.9       |
|    ep_rew_mean          | -175       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 83         |
|    time_elapsed         | 297        |
|    total_timesteps      | 84992      |
| train/                  |            |
|    approx_kl            | 0.15519834 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.43       |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.161     |
|    std                  | 0.365      |
|    value_loss           | 90.4       |
----------------------------------------
---------------------------------------
| reward                  | -3.79     |
| reward_contact          | -0.00249  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0714   |
| reward_torque           | -3.63     |
| reward_velocity         | 0.0155    |
| rollout/                |           |
|    ep_len_mean          | 52.3      |
|    ep_rew_mean          | -173      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 84        |
|    time_elapsed         | 301       |
|    total_timesteps      | 86016     |
| train/                  |           |
|    approx_kl            | 0.2233521 |
|    clip_fraction        | 0.327     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.121     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.25      |
|    n_updates            | 1660      |
|    policy_gradient_loss | -0.161    |
|    std                  | 0.364     |
|    value_loss           | 136       |
---------------------------------------
----------------------------------------
| reward                  | -3.81      |
| reward_contact          | -0.00242   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0735    |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0158     |
| rollout/                |            |
|    ep_len_mean          | 51.7       |
|    ep_rew_mean          | -171       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 85         |
|    time_elapsed         | 305        |
|    total_timesteps      | 87040      |
| train/                  |            |
|    approx_kl            | 0.13412023 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.89       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.5        |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.138     |
|    std                  | 0.364      |
|    value_loss           | 77.1       |
----------------------------------------
----------------------------------------
| reward                  | -3.79      |
| reward_contact          | -0.00208   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0561    |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 53         |
|    ep_rew_mean          | -175       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 86         |
|    time_elapsed         | 308        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.12134208 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.732      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.2       |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.143     |
|    std                  | 0.364      |
|    value_loss           | 136        |
----------------------------------------
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.00208   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0561    |
| reward_torque           | -3.66      |
| reward_velocity         | 0.0183     |
| rollout/                |            |
|    ep_len_mean          | 53         |
|    ep_rew_mean          | -176       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 87         |
|    time_elapsed         | 312        |
|    total_timesteps      | 89088      |
| train/                  |            |
|    approx_kl            | 0.18956733 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | -0.0476    |
|    learning_rate        | 0.0003     |
|    loss                 | 3.31       |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.364      |
|    value_loss           | 99.3       |
----------------------------------------
Num timesteps: 90000
Best mean reward: -138.63 - Last mean reward per episode: -208.26
---------------------------------------
| reward                  | -3.8      |
| reward_contact          | -0.00226  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0498   |
| reward_torque           | -3.67     |
| reward_velocity         | 0.0193    |
| rollout/                |           |
|    ep_len_mean          | 63.3      |
|    ep_rew_mean          | -208      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 88        |
|    time_elapsed         | 316       |
|    total_timesteps      | 90112     |
| train/                  |           |
|    approx_kl            | 0.1579457 |
|    clip_fraction        | 0.307     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.482     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.95      |
|    n_updates            | 1740      |
|    policy_gradient_loss | -0.143    |
|    std                  | 0.364     |
|    value_loss           | 52.7      |
---------------------------------------
----------------------------------------
| reward                  | -3.76      |
| reward_contact          | -0.00187   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0529    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0205     |
| rollout/                |            |
|    ep_len_mean          | 63.5       |
|    ep_rew_mean          | -209       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 89         |
|    time_elapsed         | 320        |
|    total_timesteps      | 91136      |
| train/                  |            |
|    approx_kl            | 0.12793954 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.88       |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.154     |
|    std                  | 0.364      |
|    value_loss           | 102        |
----------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0539    |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0216     |
| rollout/                |            |
|    ep_len_mean          | 63.7       |
|    ep_rew_mean          | -210       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 90         |
|    time_elapsed         | 323        |
|    total_timesteps      | 92160      |
| train/                  |            |
|    approx_kl            | 0.18851484 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.0106     |
|    learning_rate        | 0.0003     |
|    loss                 | 9.62       |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.364      |
|    value_loss           | 182        |
----------------------------------------
----------------------------------------
| reward                  | -3.69      |
| reward_contact          | -0.00103   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0623    |
| reward_torque           | -3.55      |
| reward_velocity         | 0.018      |
| rollout/                |            |
|    ep_len_mean          | 52.9       |
|    ep_rew_mean          | -178       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 91         |
|    time_elapsed         | 327        |
|    total_timesteps      | 93184      |
| train/                  |            |
|    approx_kl            | 0.21907353 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.547      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.74       |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.364      |
|    value_loss           | 78.1       |
----------------------------------------
----------------------------------------
| reward                  | -3.69      |
| reward_contact          | -0.00112   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0591    |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0211     |
| rollout/                |            |
|    ep_len_mean          | 62.4       |
|    ep_rew_mean          | -208       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 92         |
|    time_elapsed         | 331        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.23029736 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.394      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.78       |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.168     |
|    std                  | 0.364      |
|    value_loss           | 80.5       |
----------------------------------------
----------------------------------------
| reward                  | -3.68      |
| reward_contact          | -0.00101   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0663    |
| reward_torque           | -3.53      |
| reward_velocity         | 0.0235     |
| rollout/                |            |
|    ep_len_mean          | 72         |
|    ep_rew_mean          | -239       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 93         |
|    time_elapsed         | 334        |
|    total_timesteps      | 95232      |
| train/                  |            |
|    approx_kl            | 0.18609498 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.442      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.29       |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.136     |
|    std                  | 0.364      |
|    value_loss           | 56.4       |
----------------------------------------
Num timesteps: 96000
Best mean reward: -138.63 - Last mean reward per episode: -175.89
----------------------------------------
| reward                  | -3.69      |
| reward_contact          | -0.000854  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0668    |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0227     |
| rollout/                |            |
|    ep_len_mean          | 52.3       |
|    ep_rew_mean          | -176       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 94         |
|    time_elapsed         | 338        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.28788555 |
|    clip_fraction        | 0.441      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.7        |
|    learning_rate        | 0.0003     |
|    loss                 | 20.5       |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.364      |
|    value_loss           | 131        |
----------------------------------------
----------------------------------------
| reward                  | -3.68      |
| reward_contact          | -0.00109   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0599    |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0249     |
| rollout/                |            |
|    ep_len_mean          | 61.3       |
|    ep_rew_mean          | -205       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 95         |
|    time_elapsed         | 342        |
|    total_timesteps      | 97280      |
| train/                  |            |
|    approx_kl            | 0.17428435 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.318      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.01       |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.364      |
|    value_loss           | 164        |
----------------------------------------
----------------------------------------
| reward                  | -3.73      |
| reward_contact          | -0.00124   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0696    |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0249     |
| rollout/                |            |
|    ep_len_mean          | 62.2       |
|    ep_rew_mean          | -208       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 96         |
|    time_elapsed         | 345        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.18998115 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | -0.148     |
|    learning_rate        | 0.0003     |
|    loss                 | 5.74       |
|    n_updates            | 1900       |
|    policy_gradient_loss | -0.16      |
|    std                  | 0.364      |
|    value_loss           | 130        |
----------------------------------------
----------------------------------------
| reward                  | -3.73      |
| reward_contact          | -0.00124   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0623    |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0254     |
| rollout/                |            |
|    ep_len_mean          | 72.3       |
|    ep_rew_mean          | -240       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 97         |
|    time_elapsed         | 349        |
|    total_timesteps      | 99328      |
| train/                  |            |
|    approx_kl            | 0.17720614 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.354      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.6        |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.16      |
|    std                  | 0.364      |
|    value_loss           | 75.6       |
----------------------------------------
---------------------------------------
| reward                  | -3.76     |
| reward_contact          | -0.00128  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0629   |
| reward_torque           | -3.62     |
| reward_velocity         | 0.0242    |
| rollout/                |           |
|    ep_len_mean          | 53.8      |
|    ep_rew_mean          | -179      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 98        |
|    time_elapsed         | 353       |
|    total_timesteps      | 100352    |
| train/                  |           |
|    approx_kl            | 0.1779699 |
|    clip_fraction        | 0.36      |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.405     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.74      |
|    n_updates            | 1940      |
|    policy_gradient_loss | -0.133    |
|    std                  | 0.364     |
|    value_loss           | 57.4      |
---------------------------------------
---------------------------------------
| reward                  | -3.77     |
| reward_contact          | -0.00148  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0599   |
| reward_torque           | -3.63     |
| reward_velocity         | 0.023     |
| rollout/                |           |
|    ep_len_mean          | 54.2      |
|    ep_rew_mean          | -179      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 99        |
|    time_elapsed         | 356       |
|    total_timesteps      | 101376    |
| train/                  |           |
|    approx_kl            | 0.1428752 |
|    clip_fraction        | 0.294     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | -0.112    |
|    learning_rate        | 0.0003    |
|    loss                 | 11.7      |
|    n_updates            | 1960      |
|    policy_gradient_loss | -0.158    |
|    std                  | 0.364     |
|    value_loss           | 177       |
---------------------------------------
Num timesteps: 102000
Best mean reward: -138.63 - Last mean reward per episode: -162.43
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.000756  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0668    |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0196     |
| rollout/                |            |
|    ep_len_mean          | 48.6       |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 100        |
|    time_elapsed         | 360        |
|    total_timesteps      | 102400     |
| train/                  |            |
|    approx_kl            | 0.22451073 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.17       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.35       |
|    n_updates            | 1980       |
|    policy_gradient_loss | -0.193     |
|    std                  | 0.364      |
|    value_loss           | 148        |
----------------------------------------
----------------------------------------
| reward                  | -3.82      |
| reward_contact          | -0.000605  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0785    |
| reward_torque           | -3.66      |
| reward_velocity         | 0.0162     |
| rollout/                |            |
|    ep_len_mean          | 33.8       |
|    ep_rew_mean          | -115       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 101        |
|    time_elapsed         | 364        |
|    total_timesteps      | 103424     |
| train/                  |            |
|    approx_kl            | 0.19149505 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.488      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.14       |
|    n_updates            | 2000       |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.364      |
|    value_loss           | 123        |
----------------------------------------
---------------------------------------
| reward                  | -3.81     |
| reward_contact          | -0.000605 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0834   |
| reward_torque           | -3.64     |
| reward_velocity         | 0.0184    |
| rollout/                |           |
|    ep_len_mean          | 43.1      |
|    ep_rew_mean          | -144      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 102       |
|    time_elapsed         | 367       |
|    total_timesteps      | 104448    |
| train/                  |           |
|    approx_kl            | 0.2511599 |
|    clip_fraction        | 0.416     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.494     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.66      |
|    n_updates            | 2020      |
|    policy_gradient_loss | -0.17     |
|    std                  | 0.364     |
|    value_loss           | 100       |
---------------------------------------
----------------------------------------
| reward                  | -3.81      |
| reward_contact          | -0.000401  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0834    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 42.5       |
|    ep_rew_mean          | -142       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 103        |
|    time_elapsed         | 371        |
|    total_timesteps      | 105472     |
| train/                  |            |
|    approx_kl            | 0.17901304 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.625      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.55       |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.144     |
|    std                  | 0.364      |
|    value_loss           | 56.2       |
----------------------------------------
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.000554  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0768    |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0182     |
| rollout/                |            |
|    ep_len_mean          | 52.5       |
|    ep_rew_mean          | -173       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 104        |
|    time_elapsed         | 374        |
|    total_timesteps      | 106496     |
| train/                  |            |
|    approx_kl            | 0.11949587 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.42       |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.124     |
|    std                  | 0.364      |
|    value_loss           | 105        |
----------------------------------------
----------------------------------------
| reward                  | -3.78      |
| reward_contact          | -0.000518  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0665    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0217     |
| rollout/                |            |
|    ep_len_mean          | 63.5       |
|    ep_rew_mean          | -209       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 105        |
|    time_elapsed         | 378        |
|    total_timesteps      | 107520     |
| train/                  |            |
|    approx_kl            | 0.23475204 |
|    clip_fraction        | 0.407      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.715      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.57       |
|    n_updates            | 2080       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.364      |
|    value_loss           | 43.1       |
----------------------------------------
Num timesteps: 108000
Best mean reward: -138.63 - Last mean reward per episode: -240.00
---------------------------------------
| reward                  | -3.77     |
| reward_contact          | -0.000518 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0665   |
| reward_torque           | -3.62     |
| reward_velocity         | 0.0219    |
| rollout/                |           |
|    ep_len_mean          | 73.4      |
|    ep_rew_mean          | -240      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 106       |
|    time_elapsed         | 381       |
|    total_timesteps      | 108544    |
| train/                  |           |
|    approx_kl            | 0.2534588 |
|    clip_fraction        | 0.371     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.215     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.09      |
|    n_updates            | 2100      |
|    policy_gradient_loss | -0.157    |
|    std                  | 0.363     |
|    value_loss           | 137       |
---------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.000922  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0556    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0275     |
| rollout/                |            |
|    ep_len_mean          | 83.5       |
|    ep_rew_mean          | -271       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 107        |
|    time_elapsed         | 385        |
|    total_timesteps      | 109568     |
| train/                  |            |
|    approx_kl            | 0.18999796 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.678      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.91       |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.148     |
|    std                  | 0.363      |
|    value_loss           | 62.9       |
----------------------------------------
----------------------------------------
| reward                  | -3.77      |
| reward_contact          | -0.00106   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0637    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0277     |
| rollout/                |            |
|    ep_len_mean          | 93.9       |
|    ep_rew_mean          | -305       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 108        |
|    time_elapsed         | 389        |
|    total_timesteps      | 110592     |
| train/                  |            |
|    approx_kl            | 0.12192519 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.843      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.04       |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.128     |
|    std                  | 0.363      |
|    value_loss           | 67.6       |
----------------------------------------
----------------------------------------
| reward                  | -3.78      |
| reward_contact          | -0.00127   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0637    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0292     |
| rollout/                |            |
|    ep_len_mean          | 99.9       |
|    ep_rew_mean          | -324       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 109        |
|    time_elapsed         | 392        |
|    total_timesteps      | 111616     |
| train/                  |            |
|    approx_kl            | 0.23648953 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.459      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.47       |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.363      |
|    value_loss           | 98.7       |
----------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.00166   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0583    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0305     |
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | -343       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 110        |
|    time_elapsed         | 396        |
|    total_timesteps      | 112640     |
| train/                  |            |
|    approx_kl            | 0.15398324 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.568      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.21       |
|    n_updates            | 2180       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.363      |
|    value_loss           | 121        |
----------------------------------------
----------------------------------------
| reward                  | -3.73      |
| reward_contact          | -0.00186   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0583    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.0344     |
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | -342       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 111        |
|    time_elapsed         | 399        |
|    total_timesteps      | 113664     |
| train/                  |            |
|    approx_kl            | 0.15456894 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.538      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.41       |
|    n_updates            | 2200       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.363      |
|    value_loss           | 92.2       |
----------------------------------------
Num timesteps: 114000
Best mean reward: -138.63 - Last mean reward per episode: -342.33
----------------------------------------
| reward                  | -3.72      |
| reward_contact          | -0.00186   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0496    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.035      |
| rollout/                |            |
|    ep_len_mean          | 116        |
|    ep_rew_mean          | -376       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 112        |
|    time_elapsed         | 403        |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.25120765 |
|    clip_fraction        | 0.453      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.713      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.08       |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.184     |
|    std                  | 0.363      |
|    value_loss           | 94.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.71      |
| reward_contact          | -0.00198   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0465    |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0358     |
| rollout/                |            |
|    ep_len_mean          | 120        |
|    ep_rew_mean          | -388       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 113        |
|    time_elapsed         | 407        |
|    total_timesteps      | 115712     |
| train/                  |            |
|    approx_kl            | 0.19363809 |
|    clip_fraction        | 0.394      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.715      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.73       |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.15      |
|    std                  | 0.363      |
|    value_loss           | 68         |
----------------------------------------
--------------------------------------
| reward                  | -3.74    |
| reward_contact          | -0.00191 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.052   |
| reward_torque           | -3.62    |
| reward_velocity         | 0.0326   |
| rollout/                |          |
|    ep_len_mean          | 96.7     |
|    ep_rew_mean          | -314     |
| time/                   |          |
|    fps                  | 284      |
|    iterations           | 114      |
|    time_elapsed         | 410      |
|    total_timesteps      | 116736   |
| train/                  |          |
|    approx_kl            | 0.138787 |
|    clip_fraction        | 0.363    |
|    clip_range           | 0.4      |
|    entropy_loss         | -21.1    |
|    explained_variance   | 0.758    |
|    learning_rate        | 0.0003   |
|    loss                 | 4.72     |
|    n_updates            | 2260     |
|    policy_gradient_loss | -0.145   |
|    std                  | 0.363    |
|    value_loss           | 108      |
--------------------------------------
----------------------------------------
| reward                  | -3.72      |
| reward_contact          | -0.00185   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0568    |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0319     |
| rollout/                |            |
|    ep_len_mean          | 97.1       |
|    ep_rew_mean          | -315       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 115        |
|    time_elapsed         | 414        |
|    total_timesteps      | 117760     |
| train/                  |            |
|    approx_kl            | 0.22771211 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.93       |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.363      |
|    value_loss           | 144        |
----------------------------------------
----------------------------------------
| reward                  | -3.71      |
| reward_contact          | -0.00247   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0433    |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0385     |
| rollout/                |            |
|    ep_len_mean          | 87.4       |
|    ep_rew_mean          | -284       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 116        |
|    time_elapsed         | 418        |
|    total_timesteps      | 118784     |
| train/                  |            |
|    approx_kl            | 0.14404258 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.518      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.32       |
|    n_updates            | 2300       |
|    policy_gradient_loss | -0.147     |
|    std                  | 0.363      |
|    value_loss           | 179        |
----------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00192   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0517    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0333     |
| rollout/                |            |
|    ep_len_mean          | 87.7       |
|    ep_rew_mean          | -285       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 117        |
|    time_elapsed         | 422        |
|    total_timesteps      | 119808     |
| train/                  |            |
|    approx_kl            | 0.15724035 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.719      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.7        |
|    n_updates            | 2320       |
|    policy_gradient_loss | -0.146     |
|    std                  | 0.363      |
|    value_loss           | 137        |
----------------------------------------
Num timesteps: 120000
Best mean reward: -138.63 - Last mean reward per episode: -282.19
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00169   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0526    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0357     |
| rollout/                |            |
|    ep_len_mean          | 78.3       |
|    ep_rew_mean          | -256       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 118        |
|    time_elapsed         | 425        |
|    total_timesteps      | 120832     |
| train/                  |            |
|    approx_kl            | 0.18062328 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.714      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.2        |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.137     |
|    std                  | 0.363      |
|    value_loss           | 52.9       |
----------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00216   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0455    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0341     |
| rollout/                |            |
|    ep_len_mean          | 87.9       |
|    ep_rew_mean          | -286       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 119        |
|    time_elapsed         | 429        |
|    total_timesteps      | 121856     |
| train/                  |            |
|    approx_kl            | 0.14713405 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.523      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.42       |
|    n_updates            | 2360       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.363      |
|    value_loss           | 253        |
----------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.00252   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0519    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0292     |
| rollout/                |            |
|    ep_len_mean          | 63.2       |
|    ep_rew_mean          | -208       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 120        |
|    time_elapsed         | 433        |
|    total_timesteps      | 122880     |
| train/                  |            |
|    approx_kl            | 0.16059622 |
|    clip_fraction        | 0.358      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.16       |
|    n_updates            | 2380       |
|    policy_gradient_loss | -0.154     |
|    std                  | 0.363      |
|    value_loss           | 179        |
----------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.0025    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0482    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.0262     |
| rollout/                |            |
|    ep_len_mean          | 52.4       |
|    ep_rew_mean          | -175       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 121        |
|    time_elapsed         | 436        |
|    total_timesteps      | 123904     |
| train/                  |            |
|    approx_kl            | 0.19410208 |
|    clip_fraction        | 0.386      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.95       |
|    n_updates            | 2400       |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.363      |
|    value_loss           | 111        |
----------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00263   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0581    |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0209     |
| rollout/                |            |
|    ep_len_mean          | 45         |
|    ep_rew_mean          | -151       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 122        |
|    time_elapsed         | 440        |
|    total_timesteps      | 124928     |
| train/                  |            |
|    approx_kl            | 0.36102858 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.696      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.78       |
|    n_updates            | 2420       |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.363      |
|    value_loss           | 146        |
----------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00263   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0533    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.0212     |
| rollout/                |            |
|    ep_len_mean          | 55         |
|    ep_rew_mean          | -183       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 123        |
|    time_elapsed         | 444        |
|    total_timesteps      | 125952     |
| train/                  |            |
|    approx_kl            | 0.20007198 |
|    clip_fraction        | 0.37       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.576      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.49       |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.363      |
|    value_loss           | 170        |
----------------------------------------
Num timesteps: 126000
Best mean reward: -138.63 - Last mean reward per episode: -182.91
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00269   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0509    |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0214     |
| rollout/                |            |
|    ep_len_mean          | 64.7       |
|    ep_rew_mean          | -213       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 124        |
|    time_elapsed         | 447        |
|    total_timesteps      | 126976     |
| train/                  |            |
|    approx_kl            | 0.13550422 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.538      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.86       |
|    n_updates            | 2460       |
|    policy_gradient_loss | -0.144     |
|    std                  | 0.363      |
|    value_loss           | 130        |
----------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00269   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0509    |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0214     |
| rollout/                |            |
|    ep_len_mean          | 74.6       |
|    ep_rew_mean          | -244       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 125        |
|    time_elapsed         | 451        |
|    total_timesteps      | 128000     |
| train/                  |            |
|    approx_kl            | 0.13427624 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.495      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.46       |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.363      |
|    value_loss           | 166        |
----------------------------------------
----------------------------------------
| reward                  | -3.73      |
| reward_contact          | -0.00244   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0556    |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0215     |
| rollout/                |            |
|    ep_len_mean          | 64.9       |
|    ep_rew_mean          | -214       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 126        |
|    time_elapsed         | 455        |
|    total_timesteps      | 129024     |
| train/                  |            |
|    approx_kl            | 0.37402228 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.79       |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.152     |
|    std                  | 0.363      |
|    value_loss           | 42.4       |
----------------------------------------
----------------------------------------
| reward                  | -3.73      |
| reward_contact          | -0.00187   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0553    |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0218     |
| rollout/                |            |
|    ep_len_mean          | 73.8       |
|    ep_rew_mean          | -242       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 127        |
|    time_elapsed         | 459        |
|    total_timesteps      | 130048     |
| train/                  |            |
|    approx_kl            | 0.29079568 |
|    clip_fraction        | 0.403      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.41       |
|    n_updates            | 2520       |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.363      |
|    value_loss           | 120        |
----------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.00141   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0514    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.0222     |
| rollout/                |            |
|    ep_len_mean          | 75.4       |
|    ep_rew_mean          | -247       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 128        |
|    time_elapsed         | 462        |
|    total_timesteps      | 131072     |
| train/                  |            |
|    approx_kl            | 0.18076289 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.86       |
|    learning_rate        | 0.0003     |
|    loss                 | 6.3        |
|    n_updates            | 2540       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.363      |
|    value_loss           | 202        |
----------------------------------------
Num timesteps: 132000
Best mean reward: -138.63 - Last mean reward per episode: -246.14
----------------------------------------
| reward                  | -3.76      |
| reward_contact          | -0.00141   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0514    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0217     |
| rollout/                |            |
|    ep_len_mean          | 75         |
|    ep_rew_mean          | -246       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 129        |
|    time_elapsed         | 466        |
|    total_timesteps      | 132096     |
| train/                  |            |
|    approx_kl            | 0.19317493 |
|    clip_fraction        | 0.337      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.838      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.32       |
|    n_updates            | 2560       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.363      |
|    value_loss           | 218        |
----------------------------------------
----------------------------------------
| reward                  | -3.78      |
| reward_contact          | -0.00168   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0733    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0161     |
| rollout/                |            |
|    ep_len_mean          | 50.1       |
|    ep_rew_mean          | -167       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 130        |
|    time_elapsed         | 469        |
|    total_timesteps      | 133120     |
| train/                  |            |
|    approx_kl            | 0.11982493 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.739      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.15       |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.143     |
|    std                  | 0.363      |
|    value_loss           | 158        |
----------------------------------------
---------------------------------------
| reward                  | -3.8      |
| reward_contact          | -0.00169  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0706   |
| reward_torque           | -3.64     |
| reward_velocity         | 0.0141    |
| rollout/                |           |
|    ep_len_mean          | 30.4      |
|    ep_rew_mean          | -105      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 131       |
|    time_elapsed         | 473       |
|    total_timesteps      | 134144    |
| train/                  |           |
|    approx_kl            | 0.1396409 |
|    clip_fraction        | 0.324     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.2       |
|    learning_rate        | 0.0003    |
|    loss                 | 22.2      |
|    n_updates            | 2600      |
|    policy_gradient_loss | -0.173    |
|    std                  | 0.363     |
|    value_loss           | 263       |
---------------------------------------
----------------------------------------
| reward                  | -3.79      |
| reward_contact          | -0.00169   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0593    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0163     |
| rollout/                |            |
|    ep_len_mean          | 29.3       |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 132        |
|    time_elapsed         | 476        |
|    total_timesteps      | 135168     |
| train/                  |            |
|    approx_kl            | 0.21981317 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.453      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.46       |
|    n_updates            | 2620       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.363      |
|    value_loss           | 141        |
----------------------------------------
----------------------------------------
| reward                  | -3.79      |
| reward_contact          | -0.00164   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0627    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0135     |
| rollout/                |            |
|    ep_len_mean          | 29.4       |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 133        |
|    time_elapsed         | 480        |
|    total_timesteps      | 136192     |
| train/                  |            |
|    approx_kl            | 0.25822717 |
|    clip_fraction        | 0.423      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.0146     |
|    learning_rate        | 0.0003     |
|    loss                 | 4.02       |
|    n_updates            | 2640       |
|    policy_gradient_loss | -0.184     |
|    std                  | 0.363      |
|    value_loss           | 121        |
----------------------------------------
----------------------------------------
| reward                  | -3.81      |
| reward_contact          | -0.00183   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0627    |
| reward_torque           | -3.66      |
| reward_velocity         | 0.014      |
| rollout/                |            |
|    ep_len_mean          | 40.1       |
|    ep_rew_mean          | -135       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 134        |
|    time_elapsed         | 483        |
|    total_timesteps      | 137216     |
| train/                  |            |
|    approx_kl            | 0.25418472 |
|    clip_fraction        | 0.443      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.504      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.28       |
|    n_updates            | 2660       |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.363      |
|    value_loss           | 71.5       |
----------------------------------------
Num timesteps: 138000
Best mean reward: -138.63 - Last mean reward per episode: -162.64
---------------------------------------
| reward                  | -3.79     |
| reward_contact          | -0.00136  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0579   |
| reward_torque           | -3.65     |
| reward_velocity         | 0.017     |
| rollout/                |           |
|    ep_len_mean          | 48.5      |
|    ep_rew_mean          | -163      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 135       |
|    time_elapsed         | 487       |
|    total_timesteps      | 138240    |
| train/                  |           |
|    approx_kl            | 0.2237775 |
|    clip_fraction        | 0.447     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.2     |
|    explained_variance   | 0.558     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.32      |
|    n_updates            | 2680      |
|    policy_gradient_loss | -0.174    |
|    std                  | 0.363     |
|    value_loss           | 96.1      |
---------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.00108   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0479    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.016      |
| rollout/                |            |
|    ep_len_mean          | 59.8       |
|    ep_rew_mean          | -199       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 136        |
|    time_elapsed         | 490        |
|    total_timesteps      | 139264     |
| train/                  |            |
|    approx_kl            | 0.21458992 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.552      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.84       |
|    n_updates            | 2700       |
|    policy_gradient_loss | -0.169     |
|    std                  | 0.363      |
|    value_loss           | 122        |
----------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.00126   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0638    |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0165     |
| rollout/                |            |
|    ep_len_mean          | 38.8       |
|    ep_rew_mean          | -132       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 137        |
|    time_elapsed         | 494        |
|    total_timesteps      | 140288     |
| train/                  |            |
|    approx_kl            | 0.26278722 |
|    clip_fraction        | 0.437      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.69       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.29       |
|    n_updates            | 2720       |
|    policy_gradient_loss | -0.189     |
|    std                  | 0.363      |
|    value_loss           | 95.5       |
----------------------------------------
---------------------------------------
| reward                  | -3.73     |
| reward_contact          | -0.000831 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0685   |
| reward_torque           | -3.57     |
| reward_velocity         | 0.0136    |
| rollout/                |           |
|    ep_len_mean          | 30.1      |
|    ep_rew_mean          | -104      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 138       |
|    time_elapsed         | 498       |
|    total_timesteps      | 141312    |
| train/                  |           |
|    approx_kl            | 0.2073907 |
|    clip_fraction        | 0.368     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.0882    |
|    learning_rate        | 0.0003    |
|    loss                 | 2.59      |
|    n_updates            | 2740      |
|    policy_gradient_loss | -0.186    |
|    std                  | 0.363     |
|    value_loss           | 112       |
---------------------------------------
----------------------------------------
| reward                  | -3.83      |
| reward_contact          | -0.00113   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0834    |
| reward_torque           | -3.66      |
| reward_velocity         | 0.00953    |
| rollout/                |            |
|    ep_len_mean          | 27.6       |
|    ep_rew_mean          | -96.4      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 139        |
|    time_elapsed         | 501        |
|    total_timesteps      | 142336     |
| train/                  |            |
|    approx_kl            | 0.23781297 |
|    clip_fraction        | 0.428      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.787      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.44       |
|    n_updates            | 2760       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.363      |
|    value_loss           | 77.7       |
----------------------------------------
---------------------------------------
| reward                  | -3.78     |
| reward_contact          | -0.00136  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0694   |
| reward_torque           | -3.62     |
| reward_velocity         | 0.00981   |
| rollout/                |           |
|    ep_len_mean          | 28.4      |
|    ep_rew_mean          | -99.3     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 140       |
|    time_elapsed         | 505       |
|    total_timesteps      | 143360    |
| train/                  |           |
|    approx_kl            | 0.3014912 |
|    clip_fraction        | 0.424     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.71      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.8       |
|    n_updates            | 2780      |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.362     |
|    value_loss           | 98.4      |
---------------------------------------
Num timesteps: 144000
Best mean reward: -138.63 - Last mean reward per episode: -132.07
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.77      |
| reward_contact          | -0.00156   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0601    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0103     |
| rollout/                |            |
|    ep_len_mean          | 40.3       |
|    ep_rew_mean          | -138       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 141        |
|    time_elapsed         | 508        |
|    total_timesteps      | 144384     |
| train/                  |            |
|    approx_kl            | 0.32839063 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.39       |
|    n_updates            | 2800       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.362      |
|    value_loss           | 83.6       |
----------------------------------------
---------------------------------------
| reward                  | -3.79     |
| reward_contact          | -0.00156  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0601   |
| reward_torque           | -3.64     |
| reward_velocity         | 0.0111    |
| rollout/                |           |
|    ep_len_mean          | 40.4      |
|    ep_rew_mean          | -138      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 142       |
|    time_elapsed         | 512       |
|    total_timesteps      | 145408    |
| train/                  |           |
|    approx_kl            | 0.4918242 |
|    clip_fraction        | 0.527     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.1     |
|    explained_variance   | 0.821     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.1       |
|    n_updates            | 2820      |
|    policy_gradient_loss | -0.193    |
|    std                  | 0.362     |
|    value_loss           | 67.7      |
---------------------------------------
----------------------------------------
| reward                  | -3.79      |
| reward_contact          | -0.00171   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0572    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0114     |
| rollout/                |            |
|    ep_len_mean          | 40.6       |
|    ep_rew_mean          | -138       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 143        |
|    time_elapsed         | 515        |
|    total_timesteps      | 146432     |
| train/                  |            |
|    approx_kl            | 0.32793552 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.891      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.77       |
|    n_updates            | 2840       |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.362      |
|    value_loss           | 73         |
----------------------------------------
----------------------------------------
| reward                  | -3.75      |
| reward_contact          | -0.00211   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0536    |
| reward_torque           | -3.61      |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 51.6       |
|    ep_rew_mean          | -174       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 144        |
|    time_elapsed         | 519        |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.27646038 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | -0.107     |
|    learning_rate        | 0.0003     |
|    loss                 | 3.49       |
|    n_updates            | 2860       |
|    policy_gradient_loss | -0.183     |
|    std                  | 0.362      |
|    value_loss           | 102        |
----------------------------------------
----------------------------------------
| reward                  | -3.76      |
| reward_contact          | -0.00221   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0601    |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0157     |
| rollout/                |            |
|    ep_len_mean          | 58.8       |
|    ep_rew_mean          | -196       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 145        |
|    time_elapsed         | 522        |
|    total_timesteps      | 148480     |
| train/                  |            |
|    approx_kl            | 0.24255568 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.96       |
|    n_updates            | 2880       |
|    policy_gradient_loss | -0.186     |
|    std                  | 0.362      |
|    value_loss           | 119        |
----------------------------------------
----------------------------------------
| reward                  | -3.84      |
| reward_contact          | -0.00202   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0793    |
| reward_torque           | -3.68      |
| reward_velocity         | 0.0153     |
| rollout/                |            |
|    ep_len_mean          | 47.7       |
|    ep_rew_mean          | -160       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 146        |
|    time_elapsed         | 526        |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.33325788 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | -0.394     |
|    learning_rate        | 0.0003     |
|    loss                 | 3.97       |
|    n_updates            | 2900       |
|    policy_gradient_loss | -0.2       |
|    std                  | 0.362      |
|    value_loss           | 89.4       |
----------------------------------------
Num timesteps: 150000
Best mean reward: -132.07 - Last mean reward per episode: -187.75
----------------------------------------
| reward                  | -3.84      |
| reward_contact          | -0.00182   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0793    |
| reward_torque           | -3.68      |
| reward_velocity         | 0.0155     |
| rollout/                |            |
|    ep_len_mean          | 56.3       |
|    ep_rew_mean          | -188       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 147        |
|    time_elapsed         | 529        |
|    total_timesteps      | 150528     |
| train/                  |            |
|    approx_kl            | 0.31835955 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.758      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.23       |
|    n_updates            | 2920       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.362      |
|    value_loss           | 70.1       |
----------------------------------------
----------------------------------------
| reward                  | -3.81      |
| reward_contact          | -0.00177   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0699    |
| reward_torque           | -3.66      |
| reward_velocity         | 0.0151     |
| rollout/                |            |
|    ep_len_mean          | 56.8       |
|    ep_rew_mean          | -190       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 148        |
|    time_elapsed         | 533        |
|    total_timesteps      | 151552     |
| train/                  |            |
|    approx_kl            | 0.32852384 |
|    clip_fraction        | 0.496      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.47       |
|    n_updates            | 2940       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.362      |
|    value_loss           | 75         |
----------------------------------------
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.00203   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0507    |
| reward_torque           | -3.66      |
| reward_velocity         | 0.0164     |
| rollout/                |            |
|    ep_len_mean          | 56.9       |
|    ep_rew_mean          | -190       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 149        |
|    time_elapsed         | 536        |
|    total_timesteps      | 152576     |
| train/                  |            |
|    approx_kl            | 0.45275003 |
|    clip_fraction        | 0.477      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.699      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.64       |
|    n_updates            | 2960       |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.362      |
|    value_loss           | 78.8       |
----------------------------------------
----------------------------------------
| reward                  | -3.81      |
| reward_contact          | -0.00158   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0432    |
| reward_torque           | -3.68      |
| reward_velocity         | 0.0129     |
| rollout/                |            |
|    ep_len_mean          | 46.6       |
|    ep_rew_mean          | -158       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 150        |
|    time_elapsed         | 540        |
|    total_timesteps      | 153600     |
| train/                  |            |
|    approx_kl            | 0.29522333 |
|    clip_fraction        | 0.493      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.66       |
|    n_updates            | 2980       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.362      |
|    value_loss           | 86.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.81      |
| reward_contact          | -0.00219   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0382    |
| reward_torque           | -3.68      |
| reward_velocity         | 0.0146     |
| rollout/                |            |
|    ep_len_mean          | 57.4       |
|    ep_rew_mean          | -192       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 151        |
|    time_elapsed         | 544        |
|    total_timesteps      | 154624     |
| train/                  |            |
|    approx_kl            | 0.34428954 |
|    clip_fraction        | 0.485      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.04       |
|    n_updates            | 3000       |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.362      |
|    value_loss           | 119        |
----------------------------------------
----------------------------------------
| reward                  | -3.76      |
| reward_contact          | -0.00202   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.046     |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0206     |
| rollout/                |            |
|    ep_len_mean          | 59.4       |
|    ep_rew_mean          | -198       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 152        |
|    time_elapsed         | 547        |
|    total_timesteps      | 155648     |
| train/                  |            |
|    approx_kl            | 0.20169242 |
|    clip_fraction        | 0.42       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.861      |
|    learning_rate        | 0.0003     |
|    loss                 | 21.2       |
|    n_updates            | 3020       |
|    policy_gradient_loss | -0.172     |
|    std                  | 0.362      |
|    value_loss           | 125        |
----------------------------------------
Num timesteps: 156000
Best mean reward: -132.07 - Last mean reward per episode: -165.63
---------------------------------------
| reward                  | -3.77     |
| reward_contact          | -0.00145  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0703   |
| reward_torque           | -3.62     |
| reward_velocity         | 0.0203    |
| rollout/                |           |
|    ep_len_mean          | 37.9      |
|    ep_rew_mean          | -129      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 153       |
|    time_elapsed         | 551       |
|    total_timesteps      | 156672    |
| train/                  |           |
|    approx_kl            | 0.1484077 |
|    clip_fraction        | 0.312     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.564     |
|    learning_rate        | 0.0003    |
|    loss                 | 11.6      |
|    n_updates            | 3040      |
|    policy_gradient_loss | -0.161    |
|    std                  | 0.362     |
|    value_loss           | 149       |
---------------------------------------
---------------------------------------
| reward                  | -3.75     |
| reward_contact          | -0.00154  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0355   |
| reward_torque           | -3.63     |
| reward_velocity         | 0.0214    |
| rollout/                |           |
|    ep_len_mean          | 48.2      |
|    ep_rew_mean          | -162      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 154       |
|    time_elapsed         | 554       |
|    total_timesteps      | 157696    |
| train/                  |           |
|    approx_kl            | 0.3438017 |
|    clip_fraction        | 0.506     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.581     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.6       |
|    n_updates            | 3060      |
|    policy_gradient_loss | -0.214    |
|    std                  | 0.362     |
|    value_loss           | 97.8      |
---------------------------------------
----------------------------------------
| reward                  | -3.76      |
| reward_contact          | -0.00164   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0427    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0234     |
| rollout/                |            |
|    ep_len_mean          | 58.1       |
|    ep_rew_mean          | -194       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 155        |
|    time_elapsed         | 558        |
|    total_timesteps      | 158720     |
| train/                  |            |
|    approx_kl            | 0.34817958 |
|    clip_fraction        | 0.511      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.64       |
|    n_updates            | 3080       |
|    policy_gradient_loss | -0.183     |
|    std                  | 0.362      |
|    value_loss           | 92.5       |
----------------------------------------
----------------------------------------
| reward                  | -3.77      |
| reward_contact          | -0.00151   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0355    |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0149     |
| rollout/                |            |
|    ep_len_mean          | 36.9       |
|    ep_rew_mean          | -126       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 156        |
|    time_elapsed         | 562        |
|    total_timesteps      | 159744     |
| train/                  |            |
|    approx_kl            | 0.25933456 |
|    clip_fraction        | 0.481      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.804      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.2        |
|    n_updates            | 3100       |
|    policy_gradient_loss | -0.145     |
|    std                  | 0.362      |
|    value_loss           | 64         |
----------------------------------------
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.00158   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0786    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0103     |
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | -73.3      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 157        |
|    time_elapsed         | 565        |
|    total_timesteps      | 160768     |
| train/                  |            |
|    approx_kl            | 0.20894763 |
|    clip_fraction        | 0.412      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | -0.224     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.62       |
|    n_updates            | 3120       |
|    policy_gradient_loss | -0.197     |
|    std                  | 0.362      |
|    value_loss           | 128        |
----------------------------------------
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.000868  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0759    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0146     |
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | -72.3      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 158        |
|    time_elapsed         | 569        |
|    total_timesteps      | 161792     |
| train/                  |            |
|    approx_kl            | 0.17032357 |
|    clip_fraction        | 0.38       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.133      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.54       |
|    n_updates            | 3140       |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.362      |
|    value_loss           | 162        |
----------------------------------------
Num timesteps: 162000
Best mean reward: -132.07 - Last mean reward per episode: -72.31
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.000868  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.066     |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0158     |
| rollout/                |            |
|    ep_len_mean          | 30.3       |
|    ep_rew_mean          | -105       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 159        |
|    time_elapsed         | 572        |
|    total_timesteps      | 162816     |
| train/                  |            |
|    approx_kl            | 0.23768029 |
|    clip_fraction        | 0.401      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.38       |
|    n_updates            | 3160       |
|    policy_gradient_loss | -0.199     |
|    std                  | 0.361      |
|    value_loss           | 134        |
----------------------------------------
---------------------------------------
| reward                  | -3.86     |
| reward_contact          | -0.0002   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0722   |
| reward_torque           | -3.7      |
| reward_velocity         | 0.0171    |
| rollout/                |           |
|    ep_len_mean          | 29.3      |
|    ep_rew_mean          | -102      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 160       |
|    time_elapsed         | 576       |
|    total_timesteps      | 163840    |
| train/                  |           |
|    approx_kl            | 0.3199792 |
|    clip_fraction        | 0.517     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21       |
|    explained_variance   | 0.749     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.02      |
|    n_updates            | 3180      |
|    policy_gradient_loss | -0.171    |
|    std                  | 0.361     |
|    value_loss           | 97.5      |
---------------------------------------
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | 0          |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0722    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.0172     |
| rollout/                |            |
|    ep_len_mean          | 30.3       |
|    ep_rew_mean          | -106       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 161        |
|    time_elapsed         | 579        |
|    total_timesteps      | 164864     |
| train/                  |            |
|    approx_kl            | 0.23922631 |
|    clip_fraction        | 0.402      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | -0.0291    |
|    learning_rate        | 0.0003     |
|    loss                 | 2.36       |
|    n_updates            | 3200       |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.361      |
|    value_loss           | 136        |
----------------------------------------
----------------------------------------
| reward                  | -3.86      |
| reward_contact          | -8.68e-05  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.067     |
| reward_torque           | -3.71      |
| reward_velocity         | 0.0154     |
| rollout/                |            |
|    ep_len_mean          | 40.9       |
|    ep_rew_mean          | -140       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 162        |
|    time_elapsed         | 583        |
|    total_timesteps      | 165888     |
| train/                  |            |
|    approx_kl            | 0.31808227 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.4        |
|    n_updates            | 3220       |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.361      |
|    value_loss           | 61.2       |
----------------------------------------
----------------------------------------
| reward                  | -3.85      |
| reward_contact          | -0.000302  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0631    |
| reward_torque           | -3.71      |
| reward_velocity         | 0.0197     |
| rollout/                |            |
|    ep_len_mean          | 51.3       |
|    ep_rew_mean          | -173       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 163        |
|    time_elapsed         | 586        |
|    total_timesteps      | 166912     |
| train/                  |            |
|    approx_kl            | 0.26486766 |
|    clip_fraction        | 0.51       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.812      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.43       |
|    n_updates            | 3240       |
|    policy_gradient_loss | -0.182     |
|    std                  | 0.361      |
|    value_loss           | 117        |
----------------------------------------
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.000302  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0717    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.0202     |
| rollout/                |            |
|    ep_len_mean          | 51.5       |
|    ep_rew_mean          | -174       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 164        |
|    time_elapsed         | 590        |
|    total_timesteps      | 167936     |
| train/                  |            |
|    approx_kl            | 0.13627161 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.0003     |
|    loss                 | 11.6       |
|    n_updates            | 3260       |
|    policy_gradient_loss | -0.154     |
|    std                  | 0.361      |
|    value_loss           | 148        |
----------------------------------------
Num timesteps: 168000
Best mean reward: -72.31 - Last mean reward per episode: -173.58
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.000302  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0717    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.0202     |
| rollout/                |            |
|    ep_len_mean          | 61.7       |
|    ep_rew_mean          | -206       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 165        |
|    time_elapsed         | 593        |
|    total_timesteps      | 168960     |
| train/                  |            |
|    approx_kl            | 0.15843682 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.762      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.23       |
|    n_updates            | 3280       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.361      |
|    value_loss           | 131        |
----------------------------------------
---------------------------------------
| reward                  | -3.85     |
| reward_contact          | -0.0012   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0695   |
| reward_torque           | -3.7      |
| reward_velocity         | 0.022     |
| rollout/                |           |
|    ep_len_mean          | 60.8      |
|    ep_rew_mean          | -202      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 166       |
|    time_elapsed         | 597       |
|    total_timesteps      | 169984    |
| train/                  |           |
|    approx_kl            | 0.5230099 |
|    clip_fraction        | 0.533     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.676     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.14      |
|    n_updates            | 3300      |
|    policy_gradient_loss | -0.141    |
|    std                  | 0.361     |
|    value_loss           | 63.3      |
---------------------------------------
----------------------------------------
| reward                  | -3.79      |
| reward_contact          | -0.00159   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0623    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0177     |
| rollout/                |            |
|    ep_len_mean          | 56.9       |
|    ep_rew_mean          | -191       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 167        |
|    time_elapsed         | 600        |
|    total_timesteps      | 171008     |
| train/                  |            |
|    approx_kl            | 0.45631516 |
|    clip_fraction        | 0.578      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.691      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.23       |
|    n_updates            | 3320       |
|    policy_gradient_loss | -0.203     |
|    std                  | 0.361      |
|    value_loss           | 182        |
----------------------------------------
---------------------------------------
| reward                  | -3.82     |
| reward_contact          | -0.00142  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0826   |
| reward_torque           | -3.65     |
| reward_velocity         | 0.0129    |
| rollout/                |           |
|    ep_len_mean          | 25.5      |
|    ep_rew_mean          | -90.5     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 168       |
|    time_elapsed         | 604       |
|    total_timesteps      | 172032    |
| train/                  |           |
|    approx_kl            | 0.1938377 |
|    clip_fraction        | 0.409     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.595     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.81      |
|    n_updates            | 3340      |
|    policy_gradient_loss | -0.193    |
|    std                  | 0.361     |
|    value_loss           | 175       |
---------------------------------------
---------------------------------------
| reward                  | -3.82     |
| reward_contact          | -0.00142  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0826   |
| reward_torque           | -3.65     |
| reward_velocity         | 0.0119    |
| rollout/                |           |
|    ep_len_mean          | 35.1      |
|    ep_rew_mean          | -121      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 169       |
|    time_elapsed         | 607       |
|    total_timesteps      | 173056    |
| train/                  |           |
|    approx_kl            | 0.3627219 |
|    clip_fraction        | 0.465     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.829     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.32      |
|    n_updates            | 3360      |
|    policy_gradient_loss | -0.186    |
|    std                  | 0.361     |
|    value_loss           | 144       |
---------------------------------------
Num timesteps: 174000
Best mean reward: -72.31 - Last mean reward per episode: -152.75
----------------------------------------
| reward                  | -3.83      |
| reward_contact          | -0.00177   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0806    |
| reward_torque           | -3.67      |
| reward_velocity         | 0.0175     |
| rollout/                |            |
|    ep_len_mean          | 45         |
|    ep_rew_mean          | -153       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 170        |
|    time_elapsed         | 611        |
|    total_timesteps      | 174080     |
| train/                  |            |
|    approx_kl            | 0.50357187 |
|    clip_fraction        | 0.526      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.846      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.65       |
|    n_updates            | 3380       |
|    policy_gradient_loss | -0.144     |
|    std                  | 0.361      |
|    value_loss           | 84.4       |
----------------------------------------
----------------------------------------
| reward                  | -3.82      |
| reward_contact          | -0.00177   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0711    |
| reward_torque           | -3.66      |
| reward_velocity         | 0.018      |
| rollout/                |            |
|    ep_len_mean          | 55.7       |
|    ep_rew_mean          | -187       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 171        |
|    time_elapsed         | 615        |
|    total_timesteps      | 175104     |
| train/                  |            |
|    approx_kl            | 0.13590036 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.0003     |
|    loss                 | 36.3       |
|    n_updates            | 3400       |
|    policy_gradient_loss | -0.146     |
|    std                  | 0.361      |
|    value_loss           | 324        |
----------------------------------------
----------------------------------------
| reward                  | -3.79      |
| reward_contact          | -0.00176   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0817    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0194     |
| rollout/                |            |
|    ep_len_mean          | 49.3       |
|    ep_rew_mean          | -166       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 172        |
|    time_elapsed         | 618        |
|    total_timesteps      | 176128     |
| train/                  |            |
|    approx_kl            | 0.17257623 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.25       |
|    n_updates            | 3420       |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.361      |
|    value_loss           | 156        |
----------------------------------------
---------------------------------------
| reward                  | -3.8      |
| reward_contact          | -0.00181  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0776   |
| reward_torque           | -3.64     |
| reward_velocity         | 0.0217    |
| rollout/                |           |
|    ep_len_mean          | 60.5      |
|    ep_rew_mean          | -203      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 173       |
|    time_elapsed         | 622       |
|    total_timesteps      | 177152    |
| train/                  |           |
|    approx_kl            | 0.2123828 |
|    clip_fraction        | 0.407     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.237     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.57      |
|    n_updates            | 3440      |
|    policy_gradient_loss | -0.194    |
|    std                  | 0.361     |
|    value_loss           | 204       |
---------------------------------------
---------------------------------------
| reward                  | -3.8      |
| reward_contact          | -0.00181  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0776   |
| reward_torque           | -3.65     |
| reward_velocity         | 0.0231    |
| rollout/                |           |
|    ep_len_mean          | 70.5      |
|    ep_rew_mean          | -234      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 174       |
|    time_elapsed         | 625       |
|    total_timesteps      | 178176    |
| train/                  |           |
|    approx_kl            | 0.3664076 |
|    clip_fraction        | 0.492     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.673     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.34      |
|    n_updates            | 3460      |
|    policy_gradient_loss | -0.178    |
|    std                  | 0.361     |
|    value_loss           | 86        |
---------------------------------------
----------------------------------------
| reward                  | -3.78      |
| reward_contact          | -0.00181   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0714    |
| reward_torque           | -3.63      |
| reward_velocity         | 0.023      |
| rollout/                |            |
|    ep_len_mean          | 69.2       |
|    ep_rew_mean          | -229       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 175        |
|    time_elapsed         | 629        |
|    total_timesteps      | 179200     |
| train/                  |            |
|    approx_kl            | 0.21481404 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.71       |
|    n_updates            | 3480       |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.361      |
|    value_loss           | 78.4       |
----------------------------------------
Num timesteps: 180000
Best mean reward: -72.31 - Last mean reward per episode: -261.22
---------------------------------------
| reward                  | -3.75     |
| reward_contact          | -0.0017   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0717   |
| reward_torque           | -3.6      |
| reward_velocity         | 0.0255    |
| rollout/                |           |
|    ep_len_mean          | 79.3      |
|    ep_rew_mean          | -261      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 176       |
|    time_elapsed         | 633       |
|    total_timesteps      | 180224    |
| train/                  |           |
|    approx_kl            | 0.2458967 |
|    clip_fraction        | 0.443     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.685     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.47      |
|    n_updates            | 3500      |
|    policy_gradient_loss | -0.17     |
|    std                  | 0.361     |
|    value_loss           | 96.3      |
---------------------------------------
----------------------------------------
| reward                  | -3.74      |
| reward_contact          | -0.00146   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0694    |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0214     |
| rollout/                |            |
|    ep_len_mean          | 69.1       |
|    ep_rew_mean          | -229       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 177        |
|    time_elapsed         | 636        |
|    total_timesteps      | 181248     |
| train/                  |            |
|    approx_kl            | 0.20583606 |
|    clip_fraction        | 0.398      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.37       |
|    n_updates            | 3520       |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.361      |
|    value_loss           | 118        |
----------------------------------------
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.00115   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0729    |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0215     |
| rollout/                |            |
|    ep_len_mean          | 68.7       |
|    ep_rew_mean          | -227       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 178        |
|    time_elapsed         | 640        |
|    total_timesteps      | 182272     |
| train/                  |            |
|    approx_kl            | 0.23189592 |
|    clip_fraction        | 0.415      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.66       |
|    n_updates            | 3540       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.361      |
|    value_loss           | 115        |
----------------------------------------
----------------------------------------
| reward                  | -3.8       |
| reward_contact          | -0.0013    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0653    |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0214     |
| rollout/                |            |
|    ep_len_mean          | 78.7       |
|    ep_rew_mean          | -259       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 179        |
|    time_elapsed         | 643        |
|    total_timesteps      | 183296     |
| train/                  |            |
|    approx_kl            | 0.16957897 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.589      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.27       |
|    n_updates            | 3560       |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.361      |
|    value_loss           | 155        |
----------------------------------------
----------------------------------------
| reward                  | -3.85      |
| reward_contact          | -0.00177   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0713    |
| reward_torque           | -3.69      |
| reward_velocity         | 0.0133     |
| rollout/                |            |
|    ep_len_mean          | 48.5       |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 180        |
|    time_elapsed         | 647        |
|    total_timesteps      | 184320     |
| train/                  |            |
|    approx_kl            | 0.34666207 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.06       |
|    n_updates            | 3580       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.361      |
|    value_loss           | 76.3       |
----------------------------------------
---------------------------------------
| reward                  | -3.89     |
| reward_contact          | -0.00194  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0816   |
| reward_torque           | -3.71     |
| reward_velocity         | 0.00964   |
| rollout/                |           |
|    ep_len_mean          | 29.1      |
|    ep_rew_mean          | -102      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 181       |
|    time_elapsed         | 650       |
|    total_timesteps      | 185344    |
| train/                  |           |
|    approx_kl            | 0.2027188 |
|    clip_fraction        | 0.394     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.00364   |
|    learning_rate        | 0.0003    |
|    loss                 | 6.55      |
|    n_updates            | 3600      |
|    policy_gradient_loss | -0.196    |
|    std                  | 0.361     |
|    value_loss           | 189       |
---------------------------------------
Num timesteps: 186000
Best mean reward: -72.31 - Last mean reward per episode: -102.37
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.00191   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0843    |
| reward_torque           | -3.7       |
| reward_velocity         | 0.0104     |
| rollout/                |            |
|    ep_len_mean          | 28.8       |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 182        |
|    time_elapsed         | 654        |
|    total_timesteps      | 186368     |
| train/                  |            |
|    approx_kl            | 0.25482127 |
|    clip_fraction        | 0.434      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.765      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.25       |
|    n_updates            | 3620       |
|    policy_gradient_loss | -0.202     |
|    std                  | 0.361      |
|    value_loss           | 174        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00191   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0843    |
| reward_torque           | -3.7       |
| reward_velocity         | 0.00971    |
| rollout/                |            |
|    ep_len_mean          | 38.9       |
|    ep_rew_mean          | -134       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 183        |
|    time_elapsed         | 658        |
|    total_timesteps      | 187392     |
| train/                  |            |
|    approx_kl            | 0.34795552 |
|    clip_fraction        | 0.476      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.502      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.93       |
|    n_updates            | 3640       |
|    policy_gradient_loss | -0.183     |
|    std                  | 0.361      |
|    value_loss           | 125        |
----------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00181   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0889    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0105     |
| rollout/                |            |
|    ep_len_mean          | 37.5       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 184        |
|    time_elapsed         | 661        |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.39554232 |
|    clip_fraction        | 0.476      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.605      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.8        |
|    n_updates            | 3660       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.361      |
|    value_loss           | 53.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00217   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0939    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.00953    |
| rollout/                |            |
|    ep_len_mean          | 30.3       |
|    ep_rew_mean          | -107       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 185        |
|    time_elapsed         | 665        |
|    total_timesteps      | 189440     |
| train/                  |            |
|    approx_kl            | 0.22385332 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | -0.00799   |
|    learning_rate        | 0.0003     |
|    loss                 | 3.84       |
|    n_updates            | 3680       |
|    policy_gradient_loss | -0.198     |
|    std                  | 0.361      |
|    value_loss           | 172        |
----------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00217   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0869    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0114     |
| rollout/                |            |
|    ep_len_mean          | 30.8       |
|    ep_rew_mean          | -109       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 186        |
|    time_elapsed         | 668        |
|    total_timesteps      | 190464     |
| train/                  |            |
|    approx_kl            | 0.25057673 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.46       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.88       |
|    n_updates            | 3700       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.361      |
|    value_loss           | 164        |
----------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00234   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0931    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.011      |
| rollout/                |            |
|    ep_len_mean          | 31.2       |
|    ep_rew_mean          | -110       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 187        |
|    time_elapsed         | 672        |
|    total_timesteps      | 191488     |
| train/                  |            |
|    approx_kl            | 0.24711326 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.62       |
|    n_updates            | 3720       |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.36       |
|    value_loss           | 118        |
----------------------------------------
Num timesteps: 192000
Best mean reward: -72.31 - Last mean reward per episode: -136.37
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.00174  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0902   |
| reward_torque           | -3.78     |
| reward_velocity         | 0.0126    |
| rollout/                |           |
|    ep_len_mean          | 36.9      |
|    ep_rew_mean          | -127      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 188       |
|    time_elapsed         | 675       |
|    total_timesteps      | 192512    |
| train/                  |           |
|    approx_kl            | 0.5058031 |
|    clip_fraction        | 0.563     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.811     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.05      |
|    n_updates            | 3740      |
|    policy_gradient_loss | -0.2      |
|    std                  | 0.36      |
|    value_loss           | 56.3      |
---------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00174   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0902    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0134     |
| rollout/                |            |
|    ep_len_mean          | 36.8       |
|    ep_rew_mean          | -127       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 189        |
|    time_elapsed         | 679        |
|    total_timesteps      | 193536     |
| train/                  |            |
|    approx_kl            | 0.31118467 |
|    clip_fraction        | 0.456      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.689      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.65       |
|    n_updates            | 3760       |
|    policy_gradient_loss | -0.21      |
|    std                  | 0.36       |
|    value_loss           | 131        |
----------------------------------------
---------------------------------------
| reward                  | -3.89     |
| reward_contact          | -0.00142  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0764   |
| reward_torque           | -3.73     |
| reward_velocity         | 0.0174    |
| rollout/                |           |
|    ep_len_mean          | 47.1      |
|    ep_rew_mean          | -160      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 190       |
|    time_elapsed         | 683       |
|    total_timesteps      | 194560    |
| train/                  |           |
|    approx_kl            | 0.3108564 |
|    clip_fraction        | 0.453     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.802     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.1       |
|    n_updates            | 3780      |
|    policy_gradient_loss | -0.181    |
|    std                  | 0.36      |
|    value_loss           | 53.6      |
---------------------------------------
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.00173   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0633    |
| reward_torque           | -3.73      |
| reward_velocity         | 0.0171     |
| rollout/                |            |
|    ep_len_mean          | 36.7       |
|    ep_rew_mean          | -126       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 191        |
|    time_elapsed         | 686        |
|    total_timesteps      | 195584     |
| train/                  |            |
|    approx_kl            | 0.27092522 |
|    clip_fraction        | 0.469      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.07       |
|    n_updates            | 3800       |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.36       |
|    value_loss           | 127        |
----------------------------------------
----------------------------------------
| reward                  | -3.85      |
| reward_contact          | -0.00217   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0598    |
| reward_torque           | -3.71      |
| reward_velocity         | 0.0157     |
| rollout/                |            |
|    ep_len_mean          | 46.1       |
|    ep_rew_mean          | -156       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 192        |
|    time_elapsed         | 690        |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.23524715 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.692      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.42       |
|    n_updates            | 3820       |
|    policy_gradient_loss | -0.186     |
|    std                  | 0.36       |
|    value_loss           | 132        |
----------------------------------------
----------------------------------------
| reward                  | -3.83      |
| reward_contact          | -0.00304   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0627    |
| reward_torque           | -3.68      |
| reward_velocity         | 0.014      |
| rollout/                |            |
|    ep_len_mean          | 39.4       |
|    ep_rew_mean          | -135       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 193        |
|    time_elapsed         | 693        |
|    total_timesteps      | 197632     |
| train/                  |            |
|    approx_kl            | 0.24884942 |
|    clip_fraction        | 0.398      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.583      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.38       |
|    n_updates            | 3840       |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.36       |
|    value_loss           | 99.3       |
----------------------------------------
Num timesteps: 198000
Best mean reward: -72.31 - Last mean reward per episode: -135.46
----------------------------------------
| reward                  | -3.83      |
| reward_contact          | -0.0028    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0531    |
| reward_torque           | -3.68      |
| reward_velocity         | 0.0141     |
| rollout/                |            |
|    ep_len_mean          | 48.7       |
|    ep_rew_mean          | -165       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 194        |
|    time_elapsed         | 697        |
|    total_timesteps      | 198656     |
| train/                  |            |
|    approx_kl            | 0.20364907 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.256      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.63       |
|    n_updates            | 3860       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.36       |
|    value_loss           | 179        |
----------------------------------------
----------------------------------------
| reward                  | -3.83      |
| reward_contact          | -0.00302   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0531    |
| reward_torque           | -3.69      |
| reward_velocity         | 0.0134     |
| rollout/                |            |
|    ep_len_mean          | 48.6       |
|    ep_rew_mean          | -165       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 195        |
|    time_elapsed         | 701        |
|    total_timesteps      | 199680     |
| train/                  |            |
|    approx_kl            | 0.22442748 |
|    clip_fraction        | 0.418      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.8       |
|    n_updates            | 3880       |
|    policy_gradient_loss | -0.172     |
|    std                  | 0.36       |
|    value_loss           | 84.7       |
----------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00277   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0884    |
| reward_torque           | -3.73      |
| reward_velocity         | 0.0109     |
| rollout/                |            |
|    ep_len_mean          | 40.2       |
|    ep_rew_mean          | -139       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 196        |
|    time_elapsed         | 704        |
|    total_timesteps      | 200704     |
| train/                  |            |
|    approx_kl            | 0.24759886 |
|    clip_fraction        | 0.423      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.816      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.21       |
|    n_updates            | 3900       |
|    policy_gradient_loss | -0.183     |
|    std                  | 0.36       |
|    value_loss           | 83.9       |
----------------------------------------
---------------------------------------
| reward                  | -3.88     |
| reward_contact          | -0.00229  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0798   |
| reward_torque           | -3.71     |
| reward_velocity         | 0.0135    |
| rollout/                |           |
|    ep_len_mean          | 50.2      |
|    ep_rew_mean          | -171      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 197       |
|    time_elapsed         | 708       |
|    total_timesteps      | 201728    |
| train/                  |           |
|    approx_kl            | 0.3088709 |
|    clip_fraction        | 0.489     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.707     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.4       |
|    n_updates            | 3920      |
|    policy_gradient_loss | -0.213    |
|    std                  | 0.36      |
|    value_loss           | 179       |
---------------------------------------
---------------------------------------
| reward                  | -3.88     |
| reward_contact          | -0.00229  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0798   |
| reward_torque           | -3.71     |
| reward_velocity         | 0.0135    |
| rollout/                |           |
|    ep_len_mean          | 60.2      |
|    ep_rew_mean          | -203      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 198       |
|    time_elapsed         | 711       |
|    total_timesteps      | 202752    |
| train/                  |           |
|    approx_kl            | 0.3405066 |
|    clip_fraction        | 0.484     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.776     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.6       |
|    n_updates            | 3940      |
|    policy_gradient_loss | -0.181    |
|    std                  | 0.36      |
|    value_loss           | 86.8      |
---------------------------------------
---------------------------------------
| reward                  | -3.91     |
| reward_contact          | -0.00187  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0698   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.0128    |
| rollout/                |           |
|    ep_len_mean          | 38.4      |
|    ep_rew_mean          | -132      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 199       |
|    time_elapsed         | 715       |
|    total_timesteps      | 203776    |
| train/                  |           |
|    approx_kl            | 0.2285565 |
|    clip_fraction        | 0.387     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.2     |
|    explained_variance   | 0.803     |
|    learning_rate        | 0.0003    |
|    loss                 | 14.4      |
|    n_updates            | 3960      |
|    policy_gradient_loss | -0.166    |
|    std                  | 0.36      |
|    value_loss           | 106       |
---------------------------------------
Num timesteps: 204000
Best mean reward: -72.31 - Last mean reward per episode: -132.32
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0698    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0126     |
| rollout/                |            |
|    ep_len_mean          | 46.1       |
|    ep_rew_mean          | -158       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 200        |
|    time_elapsed         | 719        |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.21328148 |
|    clip_fraction        | 0.418      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.03       |
|    n_updates            | 3980       |
|    policy_gradient_loss | -0.197     |
|    std                  | 0.36       |
|    value_loss           | 197        |
----------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0594    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0135     |
| rollout/                |            |
|    ep_len_mean          | 55.6       |
|    ep_rew_mean          | -189       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 201        |
|    time_elapsed         | 722        |
|    total_timesteps      | 205824     |
| train/                  |            |
|    approx_kl            | 0.30242336 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.35       |
|    n_updates            | 4000       |
|    policy_gradient_loss | -0.194     |
|    std                  | 0.36       |
|    value_loss           | 157        |
----------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00197   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.076     |
| reward_torque           | -3.79      |
| reward_velocity         | 0.0119     |
| rollout/                |            |
|    ep_len_mean          | 36.8       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 202        |
|    time_elapsed         | 726        |
|    total_timesteps      | 206848     |
| train/                  |            |
|    approx_kl            | 0.17908296 |
|    clip_fraction        | 0.427      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.57       |
|    n_updates            | 4020       |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.36       |
|    value_loss           | 79.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00173   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.076     |
| reward_torque           | -3.79      |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 46.8       |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 203        |
|    time_elapsed         | 729        |
|    total_timesteps      | 207872     |
| train/                  |            |
|    approx_kl            | 0.25434375 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.507      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.45       |
|    n_updates            | 4040       |
|    policy_gradient_loss | -0.194     |
|    std                  | 0.36       |
|    value_loss           | 220        |
----------------------------------------
---------------------------------------
| reward                  | -3.95     |
| reward_contact          | -0.00185  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0724   |
| reward_torque           | -3.79     |
| reward_velocity         | 0.0173    |
| rollout/                |           |
|    ep_len_mean          | 57.2      |
|    ep_rew_mean          | -196      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 204       |
|    time_elapsed         | 733       |
|    total_timesteps      | 208896    |
| train/                  |           |
|    approx_kl            | 0.2696988 |
|    clip_fraction        | 0.475     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.6       |
|    learning_rate        | 0.0003    |
|    loss                 | 14.7      |
|    n_updates            | 4060      |
|    policy_gradient_loss | -0.15     |
|    std                  | 0.36      |
|    value_loss           | 117       |
---------------------------------------
--------------------------------------
| reward                  | -3.95    |
| reward_contact          | -0.00161 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0643  |
| reward_torque           | -3.81    |
| reward_velocity         | 0.0178   |
| rollout/                |          |
|    ep_len_mean          | 67.4     |
|    ep_rew_mean          | -228     |
| time/                   |          |
|    fps                  | 284      |
|    iterations           | 205      |
|    time_elapsed         | 737      |
|    total_timesteps      | 209920   |
| train/                  |          |
|    approx_kl            | 0.296696 |
|    clip_fraction        | 0.479    |
|    clip_range           | 0.4      |
|    entropy_loss         | -21.4    |
|    explained_variance   | 0.725    |
|    learning_rate        | 0.0003   |
|    loss                 | 3.71     |
|    n_updates            | 4080     |
|    policy_gradient_loss | -0.164   |
|    std                  | 0.36     |
|    value_loss           | 63.4     |
--------------------------------------
Num timesteps: 210000
Best mean reward: -72.31 - Last mean reward per episode: -227.87
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.0015    |
| reward_ctrl             | -0.0926    |
| reward_motion           | -0.0597    |
| reward_torque           | -3.73      |
| reward_velocity         | 0.015      |
| rollout/                |            |
|    ep_len_mean          | 50.2       |
|    ep_rew_mean          | -172       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 206        |
|    time_elapsed         | 740        |
|    total_timesteps      | 210944     |
| train/                  |            |
|    approx_kl            | 0.25137722 |
|    clip_fraction        | 0.441      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.801      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.17       |
|    n_updates            | 4100       |
|    policy_gradient_loss | -0.151     |
|    std                  | 0.36       |
|    value_loss           | 99.7       |
----------------------------------------
----------------------------------------
| reward                  | -3.82      |
| reward_contact          | -0.00166   |
| reward_ctrl             | -0.0926    |
| reward_motion           | -0.0606    |
| reward_torque           | -3.68      |
| reward_velocity         | 0.0109     |
| rollout/                |            |
|    ep_len_mean          | 18.7       |
|    ep_rew_mean          | -69.1      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 207        |
|    time_elapsed         | 744        |
|    total_timesteps      | 211968     |
| train/                  |            |
|    approx_kl            | 0.18408452 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.128      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.2       |
|    n_updates            | 4120       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.36       |
|    value_loss           | 281        |
----------------------------------------
----------------------------------------
| reward                  | -3.85      |
| reward_contact          | -0.0019    |
| reward_ctrl             | -0.0926    |
| reward_motion           | -0.0601    |
| reward_torque           | -3.71      |
| reward_velocity         | 0.0122     |
| rollout/                |            |
|    ep_len_mean          | 18.1       |
|    ep_rew_mean          | -67        |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 208        |
|    time_elapsed         | 748        |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.18281552 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.143      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.08       |
|    n_updates            | 4140       |
|    policy_gradient_loss | -0.186     |
|    std                  | 0.36       |
|    value_loss           | 215        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00218   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.066     |
| reward_torque           | -3.72      |
| reward_velocity         | 0.011      |
| rollout/                |            |
|    ep_len_mean          | 27.8       |
|    ep_rew_mean          | -97.8      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 209        |
|    time_elapsed         | 751        |
|    total_timesteps      | 214016     |
| train/                  |            |
|    approx_kl            | 0.27804562 |
|    clip_fraction        | 0.483      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.818      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.33       |
|    n_updates            | 4160       |
|    policy_gradient_loss | -0.19      |
|    std                  | 0.36       |
|    value_loss           | 107        |
----------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00144   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0764    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0105     |
| rollout/                |            |
|    ep_len_mean          | 27.3       |
|    ep_rew_mean          | -95.8      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 210        |
|    time_elapsed         | 755        |
|    total_timesteps      | 215040     |
| train/                  |            |
|    approx_kl            | 0.28606674 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.703      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.6        |
|    n_updates            | 4180       |
|    policy_gradient_loss | -0.199     |
|    std                  | 0.36       |
|    value_loss           | 207        |
----------------------------------------
Num timesteps: 216000
Best mean reward: -72.31 - Last mean reward per episode: -129.44
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00144   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0761    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.00994    |
| rollout/                |            |
|    ep_len_mean          | 37.5       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 211        |
|    time_elapsed         | 758        |
|    total_timesteps      | 216064     |
| train/                  |            |
|    approx_kl            | 0.28486478 |
|    clip_fraction        | 0.486      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.746      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.3        |
|    n_updates            | 4200       |
|    policy_gradient_loss | -0.199     |
|    std                  | 0.36       |
|    value_loss           | 137        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00157   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0687    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.00989    |
| rollout/                |            |
|    ep_len_mean          | 37.8       |
|    ep_rew_mean          | -131       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 212        |
|    time_elapsed         | 762        |
|    total_timesteps      | 217088     |
| train/                  |            |
|    approx_kl            | 0.31958354 |
|    clip_fraction        | 0.481      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.749      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.66       |
|    n_updates            | 4220       |
|    policy_gradient_loss | -0.2       |
|    std                  | 0.36       |
|    value_loss           | 141        |
----------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00143   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0731    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.00742    |
| rollout/                |            |
|    ep_len_mean          | 38.2       |
|    ep_rew_mean          | -132       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 213        |
|    time_elapsed         | 766        |
|    total_timesteps      | 218112     |
| train/                  |            |
|    approx_kl            | 0.30686766 |
|    clip_fraction        | 0.532      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.619      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.57       |
|    n_updates            | 4240       |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.36       |
|    value_loss           | 95.5       |
----------------------------------------
---------------------------------------
| reward                  | -3.92     |
| reward_contact          | -0.00158  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0672   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.00805   |
| rollout/                |           |
|    ep_len_mean          | 46.3      |
|    ep_rew_mean          | -158      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 214       |
|    time_elapsed         | 769       |
|    total_timesteps      | 219136    |
| train/                  |           |
|    approx_kl            | 0.3711713 |
|    clip_fraction        | 0.529     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.782     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.8       |
|    n_updates            | 4260      |
|    policy_gradient_loss | -0.19     |
|    std                  | 0.36      |
|    value_loss           | 71.3      |
---------------------------------------
---------------------------------------
| reward                  | -3.92     |
| reward_contact          | -0.00134  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0626   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.0122    |
| rollout/                |           |
|    ep_len_mean          | 56.2      |
|    ep_rew_mean          | -190      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 215       |
|    time_elapsed         | 773       |
|    total_timesteps      | 220160    |
| train/                  |           |
|    approx_kl            | 0.5033884 |
|    clip_fraction        | 0.561     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.799     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.48      |
|    n_updates            | 4280      |
|    policy_gradient_loss | -0.185    |
|    std                  | 0.36      |
|    value_loss           | 82.4      |
---------------------------------------
---------------------------------------
| reward                  | -3.9      |
| reward_contact          | -0.0012   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0654   |
| reward_torque           | -3.75     |
| reward_velocity         | 0.0147    |
| rollout/                |           |
|    ep_len_mean          | 66.1      |
|    ep_rew_mean          | -222      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 216       |
|    time_elapsed         | 776       |
|    total_timesteps      | 221184    |
| train/                  |           |
|    approx_kl            | 0.2191947 |
|    clip_fraction        | 0.447     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.8     |
|    explained_variance   | 0.197     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.32      |
|    n_updates            | 4300      |
|    policy_gradient_loss | -0.129    |
|    std                  | 0.36      |
|    value_loss           | 84.3      |
---------------------------------------
Num timesteps: 222000
Best mean reward: -72.31 - Last mean reward per episode: -256.36
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00177   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0557    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0154     |
| rollout/                |            |
|    ep_len_mean          | 67.6       |
|    ep_rew_mean          | -227       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 217        |
|    time_elapsed         | 780        |
|    total_timesteps      | 222208     |
| train/                  |            |
|    approx_kl            | 0.36121154 |
|    clip_fraction        | 0.534      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.724      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.13       |
|    n_updates            | 4320       |
|    policy_gradient_loss | -0.154     |
|    std                  | 0.36       |
|    value_loss           | 71.1       |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00169   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.078     |
| reward_torque           | -3.72      |
| reward_velocity         | 0.0157     |
| rollout/                |            |
|    ep_len_mean          | 47.7       |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 218        |
|    time_elapsed         | 783        |
|    total_timesteps      | 223232     |
| train/                  |            |
|    approx_kl            | 0.32178152 |
|    clip_fraction        | 0.496      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.755      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.89       |
|    n_updates            | 4340       |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.36       |
|    value_loss           | 218        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00169   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0736    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.0159     |
| rollout/                |            |
|    ep_len_mean          | 58.2       |
|    ep_rew_mean          | -196       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 219        |
|    time_elapsed         | 787        |
|    total_timesteps      | 224256     |
| train/                  |            |
|    approx_kl            | 0.27824563 |
|    clip_fraction        | 0.422      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.516      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.18       |
|    n_updates            | 4360       |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.36       |
|    value_loss           | 243        |
----------------------------------------
--------------------------------------
| reward                  | -3.89    |
| reward_contact          | -0.00169 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0781  |
| reward_torque           | -3.72    |
| reward_velocity         | 0.0101   |
| rollout/                |          |
|    ep_len_mean          | 38.3     |
|    ep_rew_mean          | -132     |
| time/                   |          |
|    fps                  | 284      |
|    iterations           | 220      |
|    time_elapsed         | 791      |
|    total_timesteps      | 225280   |
| train/                  |          |
|    approx_kl            | 0.314102 |
|    clip_fraction        | 0.492    |
|    clip_range           | 0.4      |
|    entropy_loss         | -21.1    |
|    explained_variance   | 0.614    |
|    learning_rate        | 0.0003   |
|    loss                 | 7.01     |
|    n_updates            | 4380     |
|    policy_gradient_loss | -0.182   |
|    std                  | 0.36     |
|    value_loss           | 93.8     |
--------------------------------------
----------------------------------------
| reward                  | -3.9       |
| reward_contact          | -0.00173   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0921    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.0107     |
| rollout/                |            |
|    ep_len_mean          | 38.6       |
|    ep_rew_mean          | -134       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 221        |
|    time_elapsed         | 794        |
|    total_timesteps      | 226304     |
| train/                  |            |
|    approx_kl            | 0.22075033 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.574      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.52       |
|    n_updates            | 4400       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.36       |
|    value_loss           | 126        |
----------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00194   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0843    |
| reward_torque           | -3.73      |
| reward_velocity         | 0.0141     |
| rollout/                |            |
|    ep_len_mean          | 48.2       |
|    ep_rew_mean          | -165       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 222        |
|    time_elapsed         | 798        |
|    total_timesteps      | 227328     |
| train/                  |            |
|    approx_kl            | 0.27911896 |
|    clip_fraction        | 0.453      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.213      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.09       |
|    n_updates            | 4420       |
|    policy_gradient_loss | -0.205     |
|    std                  | 0.36       |
|    value_loss           | 213        |
----------------------------------------
Num timesteps: 228000
Best mean reward: -72.31 - Last mean reward per episode: -164.31
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.0015    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0843    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0141     |
| rollout/                |            |
|    ep_len_mean          | 48         |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 223        |
|    time_elapsed         | 802        |
|    total_timesteps      | 228352     |
| train/                  |            |
|    approx_kl            | 0.45910394 |
|    clip_fraction        | 0.516      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.79       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.35       |
|    n_updates            | 4440       |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.36       |
|    value_loss           | 86.2       |
----------------------------------------
--------------------------------------
| reward                  | -3.93    |
| reward_contact          | -0.00165 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0837  |
| reward_torque           | -3.76    |
| reward_velocity         | 0.0148   |
| rollout/                |          |
|    ep_len_mean          | 51.8     |
|    ep_rew_mean          | -177     |
| time/                   |          |
|    fps                  | 284      |
|    iterations           | 224      |
|    time_elapsed         | 805      |
|    total_timesteps      | 229376   |
| train/                  |          |
|    approx_kl            | 0.622482 |
|    clip_fraction        | 0.552    |
|    clip_range           | 0.4      |
|    entropy_loss         | -21.1    |
|    explained_variance   | 0.888    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.33     |
|    n_updates            | 4460     |
|    policy_gradient_loss | -0.163   |
|    std                  | 0.36     |
|    value_loss           | 42.4     |
--------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00197   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0872    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.014      |
| rollout/                |            |
|    ep_len_mean          | 41.6       |
|    ep_rew_mean          | -144       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 225        |
|    time_elapsed         | 809        |
|    total_timesteps      | 230400     |
| train/                  |            |
|    approx_kl            | 0.40062478 |
|    clip_fraction        | 0.536      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.81       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.39       |
|    n_updates            | 4480       |
|    policy_gradient_loss | -0.181     |
|    std                  | 0.36       |
|    value_loss           | 148        |
----------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00203   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0846    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0171     |
| rollout/                |            |
|    ep_len_mean          | 52.2       |
|    ep_rew_mean          | -178       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 226        |
|    time_elapsed         | 813        |
|    total_timesteps      | 231424     |
| train/                  |            |
|    approx_kl            | 0.27791643 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.24       |
|    n_updates            | 4500       |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.359      |
|    value_loss           | 113        |
----------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00173   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0881    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0145     |
| rollout/                |            |
|    ep_len_mean          | 51.5       |
|    ep_rew_mean          | -175       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 227        |
|    time_elapsed         | 816        |
|    total_timesteps      | 232448     |
| train/                  |            |
|    approx_kl            | 0.34172493 |
|    clip_fraction        | 0.523      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.691      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.26       |
|    n_updates            | 4520       |
|    policy_gradient_loss | -0.204     |
|    std                  | 0.359      |
|    value_loss           | 121        |
----------------------------------------
---------------------------------------
| reward                  | -3.94     |
| reward_contact          | -0.00128  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0825   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.0127    |
| rollout/                |           |
|    ep_len_mean          | 38.5      |
|    ep_rew_mean          | -133      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 228       |
|    time_elapsed         | 820       |
|    total_timesteps      | 233472    |
| train/                  |           |
|    approx_kl            | 0.4931413 |
|    clip_fraction        | 0.566     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21       |
|    explained_variance   | 0.345     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.7       |
|    n_updates            | 4540      |
|    policy_gradient_loss | -0.207    |
|    std                  | 0.359     |
|    value_loss           | 98.2      |
---------------------------------------
Num timesteps: 234000
Best mean reward: -72.31 - Last mean reward per episode: -101.59
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.000744  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0851    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 28.7       |
|    ep_rew_mean          | -102       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 229        |
|    time_elapsed         | 823        |
|    total_timesteps      | 234496     |
| train/                  |            |
|    approx_kl            | 0.27011216 |
|    clip_fraction        | 0.464      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | -0.249     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.73       |
|    n_updates            | 4560       |
|    policy_gradient_loss | -0.211     |
|    std                  | 0.359      |
|    value_loss           | 133        |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.000761  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0848    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0107     |
| rollout/                |            |
|    ep_len_mean          | 29         |
|    ep_rew_mean          | -103       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 230        |
|    time_elapsed         | 827        |
|    total_timesteps      | 235520     |
| train/                  |            |
|    approx_kl            | 0.39494225 |
|    clip_fraction        | 0.533      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.669      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.44       |
|    n_updates            | 4580       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.359      |
|    value_loss           | 84.5       |
----------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00113   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0809    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0134     |
| rollout/                |            |
|    ep_len_mean          | 38.8       |
|    ep_rew_mean          | -135       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 231        |
|    time_elapsed         | 830        |
|    total_timesteps      | 236544     |
| train/                  |            |
|    approx_kl            | 0.29833746 |
|    clip_fraction        | 0.482      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.588      |
|    learning_rate        | 0.0003     |
|    loss                 | 4          |
|    n_updates            | 4600       |
|    policy_gradient_loss | -0.19      |
|    std                  | 0.359      |
|    value_loss           | 158        |
----------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.000896  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.083     |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0158     |
| rollout/                |            |
|    ep_len_mean          | 38.6       |
|    ep_rew_mean          | -134       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 232        |
|    time_elapsed         | 834        |
|    total_timesteps      | 237568     |
| train/                  |            |
|    approx_kl            | 0.34127718 |
|    clip_fraction        | 0.496      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.677      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.77       |
|    n_updates            | 4620       |
|    policy_gradient_loss | -0.205     |
|    std                  | 0.359      |
|    value_loss           | 166        |
----------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00116   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0738    |
| reward_torque           | -3.73      |
| reward_velocity         | 0.0164     |
| rollout/                |            |
|    ep_len_mean          | 49         |
|    ep_rew_mean          | -167       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 233        |
|    time_elapsed         | 838        |
|    total_timesteps      | 238592     |
| train/                  |            |
|    approx_kl            | 0.37395412 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.1        |
|    n_updates            | 4640       |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.359      |
|    value_loss           | 87.7       |
----------------------------------------
---------------------------------------
| reward                  | -3.91     |
| reward_contact          | -0.0016   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0762   |
| reward_torque           | -3.75     |
| reward_velocity         | 0.0135    |
| rollout/                |           |
|    ep_len_mean          | 28.8      |
|    ep_rew_mean          | -102      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 234       |
|    time_elapsed         | 841       |
|    total_timesteps      | 239616    |
| train/                  |           |
|    approx_kl            | 0.3462485 |
|    clip_fraction        | 0.502     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.2     |
|    explained_variance   | 0.791     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.31      |
|    n_updates            | 4660      |
|    policy_gradient_loss | -0.194    |
|    std                  | 0.359     |
|    value_loss           | 124       |
---------------------------------------
Num timesteps: 240000
Best mean reward: -72.31 - Last mean reward per episode: -102.14
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.0016    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0762    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0139     |
| rollout/                |            |
|    ep_len_mean          | 28.9       |
|    ep_rew_mean          | -102       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 235        |
|    time_elapsed         | 845        |
|    total_timesteps      | 240640     |
| train/                  |            |
|    approx_kl            | 0.27733186 |
|    clip_fraction        | 0.453      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.0923     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.13       |
|    n_updates            | 4680       |
|    policy_gradient_loss | -0.214     |
|    std                  | 0.359      |
|    value_loss           | 136        |
----------------------------------------
---------------------------------------
| reward                  | -3.88     |
| reward_contact          | -0.00172  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0922   |
| reward_torque           | -3.71     |
| reward_velocity         | 0.0174    |
| rollout/                |           |
|    ep_len_mean          | 27.8      |
|    ep_rew_mean          | -98.1     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 236       |
|    time_elapsed         | 849       |
|    total_timesteps      | 241664    |
| train/                  |           |
|    approx_kl            | 0.3557347 |
|    clip_fraction        | 0.484     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.6     |
|    explained_variance   | 0.648     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.15      |
|    n_updates            | 4700      |
|    policy_gradient_loss | -0.172    |
|    std                  | 0.359     |
|    value_loss           | 82.4      |
---------------------------------------
---------------------------------------
| reward                  | -3.87     |
| reward_contact          | -0.00118  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0874   |
| reward_torque           | -3.7      |
| reward_velocity         | 0.0184    |
| rollout/                |           |
|    ep_len_mean          | 34.7      |
|    ep_rew_mean          | -120      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 237       |
|    time_elapsed         | 852       |
|    total_timesteps      | 242688    |
| train/                  |           |
|    approx_kl            | 0.5407515 |
|    clip_fraction        | 0.568     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.796     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.21      |
|    n_updates            | 4720      |
|    policy_gradient_loss | -0.197    |
|    std                  | 0.359     |
|    value_loss           | 118       |
---------------------------------------
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.00118   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0807    |
| reward_torque           | -3.7       |
| reward_velocity         | 0.0186     |
| rollout/                |            |
|    ep_len_mean          | 44.6       |
|    ep_rew_mean          | -152       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 238        |
|    time_elapsed         | 856        |
|    total_timesteps      | 243712     |
| train/                  |            |
|    approx_kl            | 0.34348375 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.427      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.91       |
|    n_updates            | 4740       |
|    policy_gradient_loss | -0.201     |
|    std                  | 0.359      |
|    value_loss           | 240        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00114   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0778    |
| reward_torque           | -3.71      |
| reward_velocity         | 0.0123     |
| rollout/                |            |
|    ep_len_mean          | 28.8       |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 239        |
|    time_elapsed         | 860        |
|    total_timesteps      | 244736     |
| train/                  |            |
|    approx_kl            | 0.27531302 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.735      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.8        |
|    n_updates            | 4760       |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.359      |
|    value_loss           | 99.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00096   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0954    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0106     |
| rollout/                |            |
|    ep_len_mean          | 18.3       |
|    ep_rew_mean          | -67.9      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 240        |
|    time_elapsed         | 864        |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.19908765 |
|    clip_fraction        | 0.409      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.0101     |
|    learning_rate        | 0.0003     |
|    loss                 | 6.64       |
|    n_updates            | 4780       |
|    policy_gradient_loss | -0.199     |
|    std                  | 0.359      |
|    value_loss           | 210        |
----------------------------------------
Num timesteps: 246000
Best mean reward: -72.31 - Last mean reward per episode: -68.51
Saving new best model to rl/out_dir/models/exp74/best_model.zip
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.00072  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.1      |
| reward_torque           | -3.77     |
| reward_velocity         | 0.0111    |
| rollout/                |           |
|    ep_len_mean          | 18.8      |
|    ep_rew_mean          | -69.7     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 241       |
|    time_elapsed         | 868       |
|    total_timesteps      | 246784    |
| train/                  |           |
|    approx_kl            | 0.4209903 |
|    clip_fraction        | 0.454     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.162     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.95      |
|    n_updates            | 4800      |
|    policy_gradient_loss | -0.209    |
|    std                  | 0.359     |
|    value_loss           | 158       |
---------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.001     |
| reward_ctrl             | -0.0944    |
| reward_motion           | -0.0988    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0154     |
| rollout/                |            |
|    ep_len_mean          | 31.9       |
|    ep_rew_mean          | -112       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 242        |
|    time_elapsed         | 871        |
|    total_timesteps      | 247808     |
| train/                  |            |
|    approx_kl            | 0.38348752 |
|    clip_fraction        | 0.501      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.752      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.81       |
|    n_updates            | 4820       |
|    policy_gradient_loss | -0.198     |
|    std                  | 0.359      |
|    value_loss           | 97.5       |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.001     |
| reward_ctrl             | -0.0944    |
| reward_motion           | -0.0988    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0155     |
| rollout/                |            |
|    ep_len_mean          | 32.3       |
|    ep_rew_mean          | -113       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 243        |
|    time_elapsed         | 875        |
|    total_timesteps      | 248832     |
| train/                  |            |
|    approx_kl            | 0.50151694 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.24       |
|    n_updates            | 4840       |
|    policy_gradient_loss | -0.194     |
|    std                  | 0.359      |
|    value_loss           | 101        |
----------------------------------------
--------------------------------------
| reward                  | -3.95    |
| reward_contact          | -0.00109 |
| reward_ctrl             | -0.0944  |
| reward_motion           | -0.0926  |
| reward_torque           | -3.78    |
| reward_velocity         | 0.0173   |
| rollout/                |          |
|    ep_len_mean          | 42.4     |
|    ep_rew_mean          | -145     |
| time/                   |          |
|    fps                  | 284      |
|    iterations           | 244      |
|    time_elapsed         | 879      |
|    total_timesteps      | 249856   |
| train/                  |          |
|    approx_kl            | 0.754702 |
|    clip_fraction        | 0.567    |
|    clip_range           | 0.4      |
|    entropy_loss         | -20.8    |
|    explained_variance   | 0.784    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.86     |
|    n_updates            | 4860     |
|    policy_gradient_loss | -0.179   |
|    std                  | 0.359    |
|    value_loss           | 61.2     |
--------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00127   |
| reward_ctrl             | -0.0944    |
| reward_motion           | -0.089     |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0194     |
| rollout/                |            |
|    ep_len_mean          | 52.2       |
|    ep_rew_mean          | -176       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 245        |
|    time_elapsed         | 883        |
|    total_timesteps      | 250880     |
| train/                  |            |
|    approx_kl            | 0.33218917 |
|    clip_fraction        | 0.554      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.7      |
|    explained_variance   | 0.76       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.83       |
|    n_updates            | 4880       |
|    policy_gradient_loss | -0.148     |
|    std                  | 0.359      |
|    value_loss           | 56.6       |
----------------------------------------
---------------------------------------
| reward                  | -3.95     |
| reward_contact          | -0.00151  |
| reward_ctrl             | -0.0944   |
| reward_motion           | -0.089    |
| reward_torque           | -3.78     |
| reward_velocity         | 0.0191    |
| rollout/                |           |
|    ep_len_mean          | 61.2      |
|    ep_rew_mean          | -204      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 246       |
|    time_elapsed         | 886       |
|    total_timesteps      | 251904    |
| train/                  |           |
|    approx_kl            | 0.2645834 |
|    clip_fraction        | 0.461     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.2     |
|    explained_variance   | 0.595     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.52      |
|    n_updates            | 4900      |
|    policy_gradient_loss | -0.167    |
|    std                  | 0.359     |
|    value_loss           | 125       |
---------------------------------------
Num timesteps: 252000
Best mean reward: -68.51 - Last mean reward per episode: -204.44
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00208   |
| reward_ctrl             | -0.0944    |
| reward_motion           | -0.0774    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0221     |
| rollout/                |            |
|    ep_len_mean          | 69.6       |
|    ep_rew_mean          | -231       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 247        |
|    time_elapsed         | 890        |
|    total_timesteps      | 252928     |
| train/                  |            |
|    approx_kl            | 0.46001226 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.62       |
|    n_updates            | 4920       |
|    policy_gradient_loss | -0.197     |
|    std                  | 0.359      |
|    value_loss           | 107        |
----------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.0022    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0682    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.0163     |
| rollout/                |            |
|    ep_len_mean          | 55.7       |
|    ep_rew_mean          | -187       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 248        |
|    time_elapsed         | 894        |
|    total_timesteps      | 253952     |
| train/                  |            |
|    approx_kl            | 0.53318405 |
|    clip_fraction        | 0.571      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.65       |
|    n_updates            | 4940       |
|    policy_gradient_loss | -0.208     |
|    std                  | 0.359      |
|    value_loss           | 124        |
----------------------------------------
---------------------------------------
| reward                  | -3.93     |
| reward_contact          | -0.00242  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0744   |
| reward_torque           | -3.77     |
| reward_velocity         | 0.0142    |
| rollout/                |           |
|    ep_len_mean          | 35.7      |
|    ep_rew_mean          | -123      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 249       |
|    time_elapsed         | 897       |
|    total_timesteps      | 254976    |
| train/                  |           |
|    approx_kl            | 0.4965455 |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.805     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.34      |
|    n_updates            | 4960      |
|    policy_gradient_loss | -0.179    |
|    std                  | 0.359     |
|    value_loss           | 93.6      |
---------------------------------------
---------------------------------------
| reward                  | -3.94     |
| reward_contact          | -0.00209  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0774   |
| reward_torque           | -3.77     |
| reward_velocity         | 0.0115    |
| rollout/                |           |
|    ep_len_mean          | 33.3      |
|    ep_rew_mean          | -116      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 250       |
|    time_elapsed         | 901       |
|    total_timesteps      | 256000    |
| train/                  |           |
|    approx_kl            | 0.2736624 |
|    clip_fraction        | 0.471     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.823     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.2       |
|    n_updates            | 4980      |
|    policy_gradient_loss | -0.188    |
|    std                  | 0.359     |
|    value_loss           | 194       |
---------------------------------------
---------------------------------------
| reward                  | -3.92     |
| reward_contact          | -0.00191  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0728   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.0122    |
| rollout/                |           |
|    ep_len_mean          | 43.4      |
|    ep_rew_mean          | -148      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 251       |
|    time_elapsed         | 905       |
|    total_timesteps      | 257024    |
| train/                  |           |
|    approx_kl            | 0.4548053 |
|    clip_fraction        | 0.524     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.2     |
|    explained_variance   | 0.792     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.16      |
|    n_updates            | 5000      |
|    policy_gradient_loss | -0.165    |
|    std                  | 0.359     |
|    value_loss           | 73.2      |
---------------------------------------
Num timesteps: 258000
Best mean reward: -68.51 - Last mean reward per episode: -148.24
---------------------------------------
| reward                  | -3.91     |
| reward_contact          | -0.00208  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0694   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.0164    |
| rollout/                |           |
|    ep_len_mean          | 53.5      |
|    ep_rew_mean          | -180      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 252       |
|    time_elapsed         | 908       |
|    total_timesteps      | 258048    |
| train/                  |           |
|    approx_kl            | 0.1721398 |
|    clip_fraction        | 0.385     |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.9     |
|    explained_variance   | 0.618     |
|    learning_rate        | 0.0003    |
|    loss                 | 21.4      |
|    n_updates            | 5020      |
|    policy_gradient_loss | -0.159    |
|    std                  | 0.359     |
|    value_loss           | 160       |
---------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00139   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0816    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0142     |
| rollout/                |            |
|    ep_len_mean          | 47.1       |
|    ep_rew_mean          | -159       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 253        |
|    time_elapsed         | 912        |
|    total_timesteps      | 259072     |
| train/                  |            |
|    approx_kl            | 0.30751303 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.438      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.24       |
|    n_updates            | 5040       |
|    policy_gradient_loss | -0.145     |
|    std                  | 0.359      |
|    value_loss           | 77.8       |
----------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00067   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0718    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.0197     |
| rollout/                |            |
|    ep_len_mean          | 31.7       |
|    ep_rew_mean          | -109       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 254        |
|    time_elapsed         | 916        |
|    total_timesteps      | 260096     |
| train/                  |            |
|    approx_kl            | 0.33187163 |
|    clip_fraction        | 0.494      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.752      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.69       |
|    n_updates            | 5060       |
|    policy_gradient_loss | -0.217     |
|    std                  | 0.359      |
|    value_loss           | 202        |
----------------------------------------
----------------------------------------
| reward                  | -3.9       |
| reward_contact          | -0.0021    |
| reward_ctrl             | -0.0894    |
| reward_motion           | -0.0801    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0164     |
| rollout/                |            |
|    ep_len_mean          | 19.1       |
|    ep_rew_mean          | -70        |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 255        |
|    time_elapsed         | 920        |
|    total_timesteps      | 261120     |
| train/                  |            |
|    approx_kl            | 0.23440772 |
|    clip_fraction        | 0.421      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.739      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.16       |
|    n_updates            | 5080       |
|    policy_gradient_loss | -0.195     |
|    std                  | 0.358      |
|    value_loss           | 179        |
----------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00188   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0829    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.013      |
| rollout/                |            |
|    ep_len_mean          | 15.2       |
|    ep_rew_mean          | -57.1      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 256        |
|    time_elapsed         | 924        |
|    total_timesteps      | 262144     |
| train/                  |            |
|    approx_kl            | 0.22768447 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | -0.0481    |
|    learning_rate        | 0.0003     |
|    loss                 | 6.87       |
|    n_updates            | 5100       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.358      |
|    value_loss           | 226        |
----------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00177   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0668    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.00971    |
| rollout/                |            |
|    ep_len_mean          | 16.8       |
|    ep_rew_mean          | -63        |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 257        |
|    time_elapsed         | 927        |
|    total_timesteps      | 263168     |
| train/                  |            |
|    approx_kl            | 0.26715848 |
|    clip_fraction        | 0.404      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | -0.0852    |
|    learning_rate        | 0.0003     |
|    loss                 | 2.21       |
|    n_updates            | 5120       |
|    policy_gradient_loss | -0.195     |
|    std                  | 0.358      |
|    value_loss           | 144        |
----------------------------------------
Num timesteps: 264000
Best mean reward: -68.51 - Last mean reward per episode: -63.38
Saving new best model to rl/out_dir/models/exp74/best_model.zip
---------------------------------------
| reward                  | -3.95     |
| reward_contact          | -0.00224  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0748   |
| reward_torque           | -3.79     |
| reward_velocity         | 0.01      |
| rollout/                |           |
|    ep_len_mean          | 17        |
|    ep_rew_mean          | -63.4     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 258       |
|    time_elapsed         | 930       |
|    total_timesteps      | 264192    |
| train/                  |           |
|    approx_kl            | 0.3342979 |
|    clip_fraction        | 0.471     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.1     |
|    explained_variance   | 0.133     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.1       |
|    n_updates            | 5140      |
|    policy_gradient_loss | -0.214    |
|    std                  | 0.358     |
|    value_loss           | 128       |
---------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00184   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0622    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0106     |
| rollout/                |            |
|    ep_len_mean          | 26.9       |
|    ep_rew_mean          | -95.4      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 259        |
|    time_elapsed         | 933        |
|    total_timesteps      | 265216     |
| train/                  |            |
|    approx_kl            | 0.42672575 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.25       |
|    n_updates            | 5160       |
|    policy_gradient_loss | -0.216     |
|    std                  | 0.358      |
|    value_loss           | 113        |
----------------------------------------
---------------------------------------
| reward                  | -3.91     |
| reward_contact          | -0.00203  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0562   |
| reward_torque           | -3.77     |
| reward_velocity         | 0.0163    |
| rollout/                |           |
|    ep_len_mean          | 27        |
|    ep_rew_mean          | -95.8     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 260       |
|    time_elapsed         | 936       |
|    total_timesteps      | 266240    |
| train/                  |           |
|    approx_kl            | 0.2800152 |
|    clip_fraction        | 0.507     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.725     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.71      |
|    n_updates            | 5180      |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.358     |
|    value_loss           | 129       |
---------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00187   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0562    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0175     |
| rollout/                |            |
|    ep_len_mean          | 27         |
|    ep_rew_mean          | -95.7      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 261        |
|    time_elapsed         | 939        |
|    total_timesteps      | 267264     |
| train/                  |            |
|    approx_kl            | 0.27896422 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.048      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.42       |
|    n_updates            | 5200       |
|    policy_gradient_loss | -0.216     |
|    std                  | 0.358      |
|    value_loss           | 132        |
----------------------------------------
---------------------------------------
| reward                  | -3.93     |
| reward_contact          | -0.00206  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0644   |
| reward_torque           | -3.78     |
| reward_velocity         | 0.0168    |
| rollout/                |           |
|    ep_len_mean          | 27.4      |
|    ep_rew_mean          | -97.4     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 262       |
|    time_elapsed         | 942       |
|    total_timesteps      | 268288    |
| train/                  |           |
|    approx_kl            | 0.3802692 |
|    clip_fraction        | 0.541     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.746     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.01      |
|    n_updates            | 5220      |
|    policy_gradient_loss | -0.214    |
|    std                  | 0.358     |
|    value_loss           | 96.7      |
---------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00182   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0727    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.017      |
| rollout/                |            |
|    ep_len_mean          | 27.7       |
|    ep_rew_mean          | -98.6      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 263        |
|    time_elapsed         | 945        |
|    total_timesteps      | 269312     |
| train/                  |            |
|    approx_kl            | 0.23873353 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.737      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.28       |
|    n_updates            | 5240       |
|    policy_gradient_loss | -0.201     |
|    std                  | 0.358      |
|    value_loss           | 168        |
----------------------------------------
Num timesteps: 270000
Best mean reward: -63.38 - Last mean reward per episode: -132.70
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00199   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0855    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0121     |
| rollout/                |            |
|    ep_len_mean          | 38.2       |
|    ep_rew_mean          | -133       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 264        |
|    time_elapsed         | 948        |
|    total_timesteps      | 270336     |
| train/                  |            |
|    approx_kl            | 0.41716772 |
|    clip_fraction        | 0.488      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.685      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.32       |
|    n_updates            | 5260       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.358      |
|    value_loss           | 61.9       |
----------------------------------------
---------------------------------------
| reward                  | -3.89     |
| reward_contact          | -0.00194  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0593   |
| reward_torque           | -3.74     |
| reward_velocity         | 0.0116    |
| rollout/                |           |
|    ep_len_mean          | 48        |
|    ep_rew_mean          | -164      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 265       |
|    time_elapsed         | 952       |
|    total_timesteps      | 271360    |
| train/                  |           |
|    approx_kl            | 0.5198351 |
|    clip_fraction        | 0.585     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.662     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.42      |
|    n_updates            | 5280      |
|    policy_gradient_loss | -0.191    |
|    std                  | 0.358     |
|    value_loss           | 107       |
---------------------------------------
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.00194   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0593    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.0116     |
| rollout/                |            |
|    ep_len_mean          | 58.7       |
|    ep_rew_mean          | -199       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 266        |
|    time_elapsed         | 955        |
|    total_timesteps      | 272384     |
| train/                  |            |
|    approx_kl            | 0.58499473 |
|    clip_fraction        | 0.585      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.826      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.32       |
|    n_updates            | 5300       |
|    policy_gradient_loss | -0.229     |
|    std                  | 0.358      |
|    value_loss           | 113        |
----------------------------------------
----------------------------------------
| reward                  | -3.9       |
| reward_contact          | -0.00101   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0637    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.00926    |
| rollout/                |            |
|    ep_len_mean          | 48.3       |
|    ep_rew_mean          | -165       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 267        |
|    time_elapsed         | 958        |
|    total_timesteps      | 273408     |
| train/                  |            |
|    approx_kl            | 0.22189789 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.838      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.9       |
|    n_updates            | 5320       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.358      |
|    value_loss           | 161        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.000798  |
| reward_ctrl             | -0.0897    |
| reward_motion           | -0.0582    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0117     |
| rollout/                |            |
|    ep_len_mean          | 47.6       |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 268        |
|    time_elapsed         | 961        |
|    total_timesteps      | 274432     |
| train/                  |            |
|    approx_kl            | 0.34347674 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.45       |
|    n_updates            | 5340       |
|    policy_gradient_loss | -0.213     |
|    std                  | 0.358      |
|    value_loss           | 116        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.000798  |
| reward_ctrl             | -0.0897    |
| reward_motion           | -0.0582    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0119     |
| rollout/                |            |
|    ep_len_mean          | 57.6       |
|    ep_rew_mean          | -195       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 269        |
|    time_elapsed         | 964        |
|    total_timesteps      | 275456     |
| train/                  |            |
|    approx_kl            | 0.31875604 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.694      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.66       |
|    n_updates            | 5360       |
|    policy_gradient_loss | -0.209     |
|    std                  | 0.358      |
|    value_loss           | 174        |
----------------------------------------
Num timesteps: 276000
Best mean reward: -63.38 - Last mean reward per episode: -161.01
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00115   |
| reward_ctrl             | -0.0897    |
| reward_motion           | -0.0723    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0191     |
| rollout/                |            |
|    ep_len_mean          | 47.1       |
|    ep_rew_mean          | -161       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 270        |
|    time_elapsed         | 967        |
|    total_timesteps      | 276480     |
| train/                  |            |
|    approx_kl            | 0.23767808 |
|    clip_fraction        | 0.436      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.532      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.6       |
|    n_updates            | 5380       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.358      |
|    value_loss           | 101        |
----------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00173   |
| reward_ctrl             | -0.0897    |
| reward_motion           | -0.0653    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.0175     |
| rollout/                |            |
|    ep_len_mean          | 37.2       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 271        |
|    time_elapsed         | 970        |
|    total_timesteps      | 277504     |
| train/                  |            |
|    approx_kl            | 0.49354082 |
|    clip_fraction        | 0.595      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.35       |
|    n_updates            | 5400       |
|    policy_gradient_loss | -0.223     |
|    std                  | 0.358      |
|    value_loss           | 78.9       |
----------------------------------------
--------------------------------------
| reward                  | -3.95    |
| reward_contact          | -0.00189 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0808  |
| reward_torque           | -3.78    |
| reward_velocity         | 0.0126   |
| rollout/                |          |
|    ep_len_mean          | 15.9     |
|    ep_rew_mean          | -59.5    |
| time/                   |          |
|    fps                  | 286      |
|    iterations           | 272      |
|    time_elapsed         | 973      |
|    total_timesteps      | 278528   |
| train/                  |          |
|    approx_kl            | 0.680837 |
|    clip_fraction        | 0.608    |
|    clip_range           | 0.4      |
|    entropy_loss         | -21.6    |
|    explained_variance   | 0.769    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.952    |
|    n_updates            | 5420     |
|    policy_gradient_loss | -0.223   |
|    std                  | 0.358    |
|    value_loss           | 97.4     |
--------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00179   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0133     |
| rollout/                |            |
|    ep_len_mean          | 17.4       |
|    ep_rew_mean          | -64.9      |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 273        |
|    time_elapsed         | 976        |
|    total_timesteps      | 279552     |
| train/                  |            |
|    approx_kl            | 0.29229861 |
|    clip_fraction        | 0.459      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.0217     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.19       |
|    n_updates            | 5440       |
|    policy_gradient_loss | -0.208     |
|    std                  | 0.358      |
|    value_loss           | 117        |
----------------------------------------
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00072   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0927    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.00898    |
| rollout/                |            |
|    ep_len_mean          | 18.6       |
|    ep_rew_mean          | -69.1      |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 274        |
|    time_elapsed         | 979        |
|    total_timesteps      | 280576     |
| train/                  |            |
|    approx_kl            | 0.26341864 |
|    clip_fraction        | 0.482      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.108      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.65       |
|    n_updates            | 5460       |
|    policy_gradient_loss | -0.21      |
|    std                  | 0.358      |
|    value_loss           | 127        |
----------------------------------------
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00072   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0927    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.00861    |
| rollout/                |            |
|    ep_len_mean          | 24.1       |
|    ep_rew_mean          | -86.8      |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 275        |
|    time_elapsed         | 982        |
|    total_timesteps      | 281600     |
| train/                  |            |
|    approx_kl            | 0.48075184 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.303      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.49       |
|    n_updates            | 5480       |
|    policy_gradient_loss | -0.208     |
|    std                  | 0.358      |
|    value_loss           | 117        |
----------------------------------------
Num timesteps: 282000
Best mean reward: -63.38 - Last mean reward per episode: -86.50
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00048   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0927    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.00867    |
| rollout/                |            |
|    ep_len_mean          | 24         |
|    ep_rew_mean          | -86.5      |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 276        |
|    time_elapsed         | 985        |
|    total_timesteps      | 282624     |
| train/                  |            |
|    approx_kl            | 0.57373464 |
|    clip_fraction        | 0.559      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.807      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.08       |
|    n_updates            | 5500       |
|    policy_gradient_loss | -0.211     |
|    std                  | 0.358      |
|    value_loss           | 103        |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.000824  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0861    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.00921    |
| rollout/                |            |
|    ep_len_mean          | 33.5       |
|    ep_rew_mean          | -117       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 277        |
|    time_elapsed         | 988        |
|    total_timesteps      | 283648     |
| train/                  |            |
|    approx_kl            | 0.52857876 |
|    clip_fraction        | 0.544      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.554      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.61       |
|    n_updates            | 5520       |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.358      |
|    value_loss           | 56.8       |
----------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.000824  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.08      |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0093     |
| rollout/                |            |
|    ep_len_mean          | 43.6       |
|    ep_rew_mean          | -151       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 278        |
|    time_elapsed         | 992        |
|    total_timesteps      | 284672     |
| train/                  |            |
|    approx_kl            | 0.39017332 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.717      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.33       |
|    n_updates            | 5540       |
|    policy_gradient_loss | -0.16      |
|    std                  | 0.358      |
|    value_loss           | 110        |
----------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00106   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0637    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0143     |
| rollout/                |            |
|    ep_len_mean          | 53.3       |
|    ep_rew_mean          | -183       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 279        |
|    time_elapsed         | 995        |
|    total_timesteps      | 285696     |
| train/                  |            |
|    approx_kl            | 0.43813702 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.529      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.15       |
|    n_updates            | 5560       |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.358      |
|    value_loss           | 73.1       |
----------------------------------------
---------------------------------------
| reward                  | -3.91     |
| reward_contact          | -0.00123  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0682   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.0175    |
| rollout/                |           |
|    ep_len_mean          | 62.5      |
|    ep_rew_mean          | -213      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 280       |
|    time_elapsed         | 998       |
|    total_timesteps      | 286720    |
| train/                  |           |
|    approx_kl            | 0.5980768 |
|    clip_fraction        | 0.626     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.781     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.66      |
|    n_updates            | 5580      |
|    policy_gradient_loss | -0.19     |
|    std                  | 0.358     |
|    value_loss           | 130       |
---------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00156   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0657    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 72.6       |
|    ep_rew_mean          | -246       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 281        |
|    time_elapsed         | 1001       |
|    total_timesteps      | 287744     |
| train/                  |            |
|    approx_kl            | 0.32057574 |
|    clip_fraction        | 0.489      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.733      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.61       |
|    n_updates            | 5600       |
|    policy_gradient_loss | -0.195     |
|    std                  | 0.358      |
|    value_loss           | 189        |
----------------------------------------
Num timesteps: 288000
Best mean reward: -63.38 - Last mean reward per episode: -248.01
---------------------------------------
| reward                  | -3.9      |
| reward_contact          | -0.00156  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0657   |
| reward_torque           | -3.75     |
| reward_velocity         | 0.0183    |
| rollout/                |           |
|    ep_len_mean          | 74        |
|    ep_rew_mean          | -251      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 282       |
|    time_elapsed         | 1004      |
|    total_timesteps      | 288768    |
| train/                  |           |
|    approx_kl            | 0.9688711 |
|    clip_fraction        | 0.622     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.857     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.94      |
|    n_updates            | 5620      |
|    policy_gradient_loss | -0.173    |
|    std                  | 0.358     |
|    value_loss           | 65.5      |
---------------------------------------
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.00193   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0512    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0214     |
| rollout/                |            |
|    ep_len_mean          | 77.7       |
|    ep_rew_mean          | -263       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 283        |
|    time_elapsed         | 1007       |
|    total_timesteps      | 289792     |
| train/                  |            |
|    approx_kl            | 0.34954274 |
|    clip_fraction        | 0.473      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.2        |
|    n_updates            | 5640       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.358      |
|    value_loss           | 140        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00144   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0546    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0235     |
| rollout/                |            |
|    ep_len_mean          | 77.7       |
|    ep_rew_mean          | -263       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 284        |
|    time_elapsed         | 1010       |
|    total_timesteps      | 290816     |
| train/                  |            |
|    approx_kl            | 0.23329642 |
|    clip_fraction        | 0.441      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.61       |
|    n_updates            | 5660       |
|    policy_gradient_loss | -0.168     |
|    std                  | 0.357      |
|    value_loss           | 130        |
----------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00175   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0661    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 56.1       |
|    ep_rew_mean          | -191       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 285        |
|    time_elapsed         | 1013       |
|    total_timesteps      | 291840     |
| train/                  |            |
|    approx_kl            | 0.28268188 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.605      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.74       |
|    n_updates            | 5680       |
|    policy_gradient_loss | -0.15      |
|    std                  | 0.357      |
|    value_loss           | 87         |
----------------------------------------
---------------------------------------
| reward                  | -3.89     |
| reward_contact          | -0.00164  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0686   |
| reward_torque           | -3.74     |
| reward_velocity         | 0.0157    |
| rollout/                |           |
|    ep_len_mean          | 46.8      |
|    ep_rew_mean          | -162      |
| time/                   |           |
|    fps                  | 288       |
|    iterations           | 286       |
|    time_elapsed         | 1016      |
|    total_timesteps      | 292864    |
| train/                  |           |
|    approx_kl            | 0.4226592 |
|    clip_fraction        | 0.509     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.757     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.92      |
|    n_updates            | 5700      |
|    policy_gradient_loss | -0.209    |
|    std                  | 0.357     |
|    value_loss           | 146       |
---------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00158   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0633    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0161     |
| rollout/                |            |
|    ep_len_mean          | 56.9       |
|    ep_rew_mean          | -195       |
| time/                   |            |
|    fps                  | 288        |
|    iterations           | 287        |
|    time_elapsed         | 1019       |
|    total_timesteps      | 293888     |
| train/                  |            |
|    approx_kl            | 0.15495507 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.697      |
|    learning_rate        | 0.0003     |
|    loss                 | 25.7       |
|    n_updates            | 5720       |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.357      |
|    value_loss           | 257        |
----------------------------------------
Num timesteps: 294000
Best mean reward: -63.38 - Last mean reward per episode: -194.91
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00167   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.074     |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0159     |
| rollout/                |            |
|    ep_len_mean          | 55.1       |
|    ep_rew_mean          | -189       |
| time/                   |            |
|    fps                  | 288        |
|    iterations           | 288        |
|    time_elapsed         | 1023       |
|    total_timesteps      | 294912     |
| train/                  |            |
|    approx_kl            | 0.18868396 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.42       |
|    n_updates            | 5740       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.357      |
|    value_loss           | 130        |
----------------------------------------
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.00158  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0909   |
| reward_torque           | -3.78     |
| reward_velocity         | 0.0149    |
| rollout/                |           |
|    ep_len_mean          | 37.5      |
|    ep_rew_mean          | -132      |
| time/                   |           |
|    fps                  | 288       |
|    iterations           | 289       |
|    time_elapsed         | 1026      |
|    total_timesteps      | 295936    |
| train/                  |           |
|    approx_kl            | 0.4144636 |
|    clip_fraction        | 0.572     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.844     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.07      |
|    n_updates            | 5760      |
|    policy_gradient_loss | -0.207    |
|    std                  | 0.357     |
|    value_loss           | 138       |
---------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0879    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0145     |
| rollout/                |            |
|    ep_len_mean          | 28.8       |
|    ep_rew_mean          | -104       |
| time/                   |            |
|    fps                  | 288        |
|    iterations           | 290        |
|    time_elapsed         | 1030       |
|    total_timesteps      | 296960     |
| train/                  |            |
|    approx_kl            | 0.37940833 |
|    clip_fraction        | 0.497      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.105      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.51       |
|    n_updates            | 5780       |
|    policy_gradient_loss | -0.217     |
|    std                  | 0.357      |
|    value_loss           | 130        |
----------------------------------------
--------------------------------------
| reward                  | -3.93    |
| reward_contact          | -0.00172 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0889  |
| reward_torque           | -3.76    |
| reward_velocity         | 0.0128   |
| rollout/                |          |
|    ep_len_mean          | 28.8     |
|    ep_rew_mean          | -103     |
| time/                   |          |
|    fps                  | 288      |
|    iterations           | 291      |
|    time_elapsed         | 1034     |
|    total_timesteps      | 297984   |
| train/                  |          |
|    approx_kl            | 0.722342 |
|    clip_fraction        | 0.559    |
|    clip_range           | 0.4      |
|    entropy_loss         | -21.4    |
|    explained_variance   | 0.886    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.13     |
|    n_updates            | 5800     |
|    policy_gradient_loss | -0.194   |
|    std                  | 0.357    |
|    value_loss           | 82.1     |
--------------------------------------
---------------------------------------
| reward                  | -3.92     |
| reward_contact          | -0.00166  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0857   |
| reward_torque           | -3.75     |
| reward_velocity         | 0.0169    |
| rollout/                |           |
|    ep_len_mean          | 38.8      |
|    ep_rew_mean          | -136      |
| time/                   |           |
|    fps                  | 288       |
|    iterations           | 292       |
|    time_elapsed         | 1038      |
|    total_timesteps      | 299008    |
| train/                  |           |
|    approx_kl            | 0.7147992 |
|    clip_fraction        | 0.557     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.184     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.899     |
|    n_updates            | 5820      |
|    policy_gradient_loss | -0.135    |
|    std                  | 0.357     |
|    value_loss           | 49.8      |
---------------------------------------
Num timesteps: 300000
Best mean reward: -63.38 - Last mean reward per episode: -172.00
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00142   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0799    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0177     |
| rollout/                |            |
|    ep_len_mean          | 49.8       |
|    ep_rew_mean          | -172       |
| time/                   |            |
|    fps                  | 288        |
|    iterations           | 293        |
|    time_elapsed         | 1041       |
|    total_timesteps      | 300032     |
| train/                  |            |
|    approx_kl            | 0.30227596 |
|    clip_fraction        | 0.484      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.86       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.75       |
|    n_updates            | 5840       |
|    policy_gradient_loss | -0.18      |
|    std                  | 0.357      |
|    value_loss           | 134        |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.000719  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0882    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.0177     |
| rollout/                |            |
|    ep_len_mean          | 37.2       |
|    ep_rew_mean          | -130       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 294        |
|    time_elapsed         | 1045       |
|    total_timesteps      | 301056     |
| train/                  |            |
|    approx_kl            | 0.24325319 |
|    clip_fraction        | 0.472      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.05       |
|    n_updates            | 5860       |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.357      |
|    value_loss           | 227        |
----------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.00024  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0838   |
| reward_torque           | -3.83     |
| reward_velocity         | 0.0143    |
| rollout/                |           |
|    ep_len_mean          | 27.8      |
|    ep_rew_mean          | -99.4     |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 295       |
|    time_elapsed         | 1049      |
|    total_timesteps      | 302080    |
| train/                  |           |
|    approx_kl            | 0.3370017 |
|    clip_fraction        | 0.436     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.1     |
|    explained_variance   | -0.12     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.6       |
|    n_updates            | 5880      |
|    policy_gradient_loss | -0.204    |
|    std                  | 0.357     |
|    value_loss           | 198       |
---------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.000649  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0796    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0117     |
| rollout/                |            |
|    ep_len_mean          | 26.1       |
|    ep_rew_mean          | -92.8      |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 296        |
|    time_elapsed         | 1052       |
|    total_timesteps      | 303104     |
| train/                  |            |
|    approx_kl            | 0.24127942 |
|    clip_fraction        | 0.483      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.36       |
|    n_updates            | 5900       |
|    policy_gradient_loss | -0.172     |
|    std                  | 0.357      |
|    value_loss           | 105        |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00105   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0756    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0129     |
| rollout/                |            |
|    ep_len_mean          | 36.6       |
|    ep_rew_mean          | -128       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 297        |
|    time_elapsed         | 1056       |
|    total_timesteps      | 304128     |
| train/                  |            |
|    approx_kl            | 0.34115908 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.99       |
|    n_updates            | 5920       |
|    policy_gradient_loss | -0.218     |
|    std                  | 0.357      |
|    value_loss           | 177        |
----------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00111   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0776    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 25.9       |
|    ep_rew_mean          | -92.2      |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 298        |
|    time_elapsed         | 1060       |
|    total_timesteps      | 305152     |
| train/                  |            |
|    approx_kl            | 0.19175299 |
|    clip_fraction        | 0.429      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.93       |
|    n_updates            | 5940       |
|    policy_gradient_loss | -0.15      |
|    std                  | 0.357      |
|    value_loss           | 136        |
----------------------------------------
Num timesteps: 306000
Best mean reward: -63.38 - Last mean reward per episode: -125.45
---------------------------------------
| reward                  | -3.93     |
| reward_contact          | -0.00128  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0745   |
| reward_torque           | -3.77     |
| reward_velocity         | 0.0144    |
| rollout/                |           |
|    ep_len_mean          | 36        |
|    ep_rew_mean          | -125      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 299       |
|    time_elapsed         | 1063      |
|    total_timesteps      | 306176    |
| train/                  |           |
|    approx_kl            | 0.3205521 |
|    clip_fraction        | 0.498     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.676     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.34      |
|    n_updates            | 5960      |
|    policy_gradient_loss | -0.22     |
|    std                  | 0.357     |
|    value_loss           | 140       |
---------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00128   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0698    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0161     |
| rollout/                |            |
|    ep_len_mean          | 46.8       |
|    ep_rew_mean          | -161       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 300        |
|    time_elapsed         | 1067       |
|    total_timesteps      | 307200     |
| train/                  |            |
|    approx_kl            | 0.28683978 |
|    clip_fraction        | 0.405      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.428      |
|    learning_rate        | 0.0003     |
|    loss                 | 20.5       |
|    n_updates            | 5980       |
|    policy_gradient_loss | -0.14      |
|    std                  | 0.357      |
|    value_loss           | 179        |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00153   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0738    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0155     |
| rollout/                |            |
|    ep_len_mean          | 38         |
|    ep_rew_mean          | -132       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 301        |
|    time_elapsed         | 1071       |
|    total_timesteps      | 308224     |
| train/                  |            |
|    approx_kl            | 0.21886161 |
|    clip_fraction        | 0.419      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.541      |
|    learning_rate        | 0.0003     |
|    loss                 | 19.7       |
|    n_updates            | 6000       |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.357      |
|    value_loss           | 164        |
----------------------------------------
---------------------------------------
| reward                  | -3.95     |
| reward_contact          | -0.0015   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0705   |
| reward_torque           | -3.79     |
| reward_velocity         | 0.0164    |
| rollout/                |           |
|    ep_len_mean          | 48.2      |
|    ep_rew_mean          | -166      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 302       |
|    time_elapsed         | 1075      |
|    total_timesteps      | 309248    |
| train/                  |           |
|    approx_kl            | 0.5459168 |
|    clip_fraction        | 0.541     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.718     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.8       |
|    n_updates            | 6020      |
|    policy_gradient_loss | -0.219    |
|    std                  | 0.357     |
|    value_loss           | 164       |
---------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00245   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0675    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0175     |
| rollout/                |            |
|    ep_len_mean          | 50         |
|    ep_rew_mean          | -173       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 303        |
|    time_elapsed         | 1078       |
|    total_timesteps      | 310272     |
| train/                  |            |
|    approx_kl            | 0.75299984 |
|    clip_fraction        | 0.56       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.751      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.38       |
|    n_updates            | 6040       |
|    policy_gradient_loss | -0.203     |
|    std                  | 0.357      |
|    value_loss           | 111        |
----------------------------------------
---------------------------------------
| reward                  | -3.98     |
| reward_contact          | -0.00215  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0724   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0175    |
| rollout/                |           |
|    ep_len_mean          | 38.8      |
|    ep_rew_mean          | -136      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 304       |
|    time_elapsed         | 1082      |
|    total_timesteps      | 311296    |
| train/                  |           |
|    approx_kl            | 0.7593733 |
|    clip_fraction        | 0.612     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.827     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.32      |
|    n_updates            | 6060      |
|    policy_gradient_loss | -0.215    |
|    std                  | 0.357     |
|    value_loss           | 119       |
---------------------------------------
Num timesteps: 312000
Best mean reward: -63.38 - Last mean reward per episode: -135.54
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00215   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0724    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0172     |
| rollout/                |            |
|    ep_len_mean          | 48.5       |
|    ep_rew_mean          | -167       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 305        |
|    time_elapsed         | 1086       |
|    total_timesteps      | 312320     |
| train/                  |            |
|    approx_kl            | 0.98770946 |
|    clip_fraction        | 0.616      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.867      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.13       |
|    n_updates            | 6080       |
|    policy_gradient_loss | -0.186     |
|    std                  | 0.357      |
|    value_loss           | 95.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00195   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0724    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0167     |
| rollout/                |            |
|    ep_len_mean          | 47.5       |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 306        |
|    time_elapsed         | 1089       |
|    total_timesteps      | 313344     |
| train/                  |            |
|    approx_kl            | 0.26876247 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.48       |
|    n_updates            | 6100       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.357      |
|    value_loss           | 72.3       |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00161   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0793    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0174     |
| rollout/                |            |
|    ep_len_mean          | 47.5       |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 307        |
|    time_elapsed         | 1093       |
|    total_timesteps      | 314368     |
| train/                  |            |
|    approx_kl            | 0.55357516 |
|    clip_fraction        | 0.526      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.892      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.675      |
|    n_updates            | 6120       |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.357      |
|    value_loss           | 39         |
----------------------------------------
---------------------------------------
| reward                  | -3.98     |
| reward_contact          | -0.00223  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0779   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0219    |
| rollout/                |           |
|    ep_len_mean          | 58.1      |
|    ep_rew_mean          | -199      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 308       |
|    time_elapsed         | 1096      |
|    total_timesteps      | 315392    |
| train/                  |           |
|    approx_kl            | 0.3940823 |
|    clip_fraction        | 0.515     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.1     |
|    explained_variance   | 0.769     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.955     |
|    n_updates            | 6140      |
|    policy_gradient_loss | -0.18     |
|    std                  | 0.357     |
|    value_loss           | 205       |
---------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00195   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.07      |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0217     |
| rollout/                |            |
|    ep_len_mean          | 68.2       |
|    ep_rew_mean          | -232       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 309        |
|    time_elapsed         | 1100       |
|    total_timesteps      | 316416     |
| train/                  |            |
|    approx_kl            | 0.22938171 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.696      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.8       |
|    n_updates            | 6160       |
|    policy_gradient_loss | -0.16      |
|    std                  | 0.357      |
|    value_loss           | 161        |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00207   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0837    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.017      |
| rollout/                |            |
|    ep_len_mean          | 58.2       |
|    ep_rew_mean          | -199       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 310        |
|    time_elapsed         | 1104       |
|    total_timesteps      | 317440     |
| train/                  |            |
|    approx_kl            | 0.20780717 |
|    clip_fraction        | 0.443      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.761      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.1       |
|    n_updates            | 6180       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.357      |
|    value_loss           | 112        |
----------------------------------------
Num timesteps: 318000
Best mean reward: -63.38 - Last mean reward per episode: -230.59
--------------------------------------
| reward                  | -3.94    |
| reward_contact          | -0.00228 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0656  |
| reward_torque           | -3.79    |
| reward_velocity         | 0.0162   |
| rollout/                |          |
|    ep_len_mean          | 57.7     |
|    ep_rew_mean          | -198     |
| time/                   |          |
|    fps                  | 287      |
|    iterations           | 311      |
|    time_elapsed         | 1107     |
|    total_timesteps      | 318464   |
| train/                  |          |
|    approx_kl            | 0.817657 |
|    clip_fraction        | 0.556    |
|    clip_range           | 0.4      |
|    entropy_loss         | -22      |
|    explained_variance   | 0.865    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.08     |
|    n_updates            | 6200     |
|    policy_gradient_loss | -0.214   |
|    std                  | 0.357    |
|    value_loss           | 98.7     |
--------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00172   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0643    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0165     |
| rollout/                |            |
|    ep_len_mean          | 60.1       |
|    ep_rew_mean          | -206       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 312        |
|    time_elapsed         | 1111       |
|    total_timesteps      | 319488     |
| train/                  |            |
|    approx_kl            | 0.30783203 |
|    clip_fraction        | 0.514      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.72       |
|    learning_rate        | 0.0003     |
|    loss                 | 12.6       |
|    n_updates            | 6220       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.357      |
|    value_loss           | 140        |
----------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00192   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0643    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0162     |
| rollout/                |            |
|    ep_len_mean          | 60.6       |
|    ep_rew_mean          | -208       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 313        |
|    time_elapsed         | 1114       |
|    total_timesteps      | 320512     |
| train/                  |            |
|    approx_kl            | 0.37986365 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.81       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.5        |
|    n_updates            | 6240       |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.357      |
|    value_loss           | 120        |
----------------------------------------
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.00171  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0712   |
| reward_torque           | -3.8      |
| reward_velocity         | 0.0129    |
| rollout/                |           |
|    ep_len_mean          | 49.4      |
|    ep_rew_mean          | -171      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 314       |
|    time_elapsed         | 1118      |
|    total_timesteps      | 321536    |
| train/                  |           |
|    approx_kl            | 0.6377945 |
|    clip_fraction        | 0.562     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.888     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.25      |
|    n_updates            | 6260      |
|    policy_gradient_loss | -0.173    |
|    std                  | 0.357     |
|    value_loss           | 51.4      |
---------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00126   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0712    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0118     |
| rollout/                |            |
|    ep_len_mean          | 51         |
|    ep_rew_mean          | -177       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 315        |
|    time_elapsed         | 1121       |
|    total_timesteps      | 322560     |
| train/                  |            |
|    approx_kl            | 0.37177432 |
|    clip_fraction        | 0.503      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.717      |
|    learning_rate        | 0.0003     |
|    loss                 | 29.7       |
|    n_updates            | 6280       |
|    policy_gradient_loss | -0.217     |
|    std                  | 0.357      |
|    value_loss           | 202        |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00145   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0785    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0139     |
| rollout/                |            |
|    ep_len_mean          | 51         |
|    ep_rew_mean          | -176       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 316        |
|    time_elapsed         | 1125       |
|    total_timesteps      | 323584     |
| train/                  |            |
|    approx_kl            | 0.29948264 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.852      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.12       |
|    n_updates            | 6300       |
|    policy_gradient_loss | -0.186     |
|    std                  | 0.357      |
|    value_loss           | 108        |
----------------------------------------
Num timesteps: 324000
Best mean reward: -63.38 - Last mean reward per episode: -176.31
---------------------------------------
| reward                  | -3.97     |
| reward_contact          | -0.00145  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0753   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.0149    |
| rollout/                |           |
|    ep_len_mean          | 61.1      |
|    ep_rew_mean          | -209      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 317       |
|    time_elapsed         | 1129      |
|    total_timesteps      | 324608    |
| train/                  |           |
|    approx_kl            | 0.2741406 |
|    clip_fraction        | 0.483     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.679     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.49      |
|    n_updates            | 6320      |
|    policy_gradient_loss | -0.174    |
|    std                  | 0.357     |
|    value_loss           | 147       |
---------------------------------------
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.00165  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0705   |
| reward_torque           | -3.8      |
| reward_velocity         | 0.0186    |
| rollout/                |           |
|    ep_len_mean          | 71.1      |
|    ep_rew_mean          | -242      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 318       |
|    time_elapsed         | 1132      |
|    total_timesteps      | 325632    |
| train/                  |           |
|    approx_kl            | 0.3353169 |
|    clip_fraction        | 0.46      |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.678     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.33      |
|    n_updates            | 6340      |
|    policy_gradient_loss | -0.166    |
|    std                  | 0.357     |
|    value_loss           | 90.5      |
---------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00208   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0729    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0179     |
| rollout/                |            |
|    ep_len_mean          | 58.8       |
|    ep_rew_mean          | -201       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 319        |
|    time_elapsed         | 1136       |
|    total_timesteps      | 326656     |
| train/                  |            |
|    approx_kl            | 0.44978076 |
|    clip_fraction        | 0.499      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.27       |
|    n_updates            | 6360       |
|    policy_gradient_loss | -0.143     |
|    std                  | 0.357      |
|    value_loss           | 82.9       |
----------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00218   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0729    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.018      |
| rollout/                |            |
|    ep_len_mean          | 58         |
|    ep_rew_mean          | -198       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 320        |
|    time_elapsed         | 1140       |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.73575515 |
|    clip_fraction        | 0.609      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.34       |
|    n_updates            | 6380       |
|    policy_gradient_loss | -0.223     |
|    std                  | 0.357      |
|    value_loss           | 177        |
----------------------------------------
---------------------------------------
| reward                  | -3.94     |
| reward_contact          | -0.00332  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0714   |
| reward_torque           | -3.78     |
| reward_velocity         | 0.016     |
| rollout/                |           |
|    ep_len_mean          | 56.2      |
|    ep_rew_mean          | -192      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 321       |
|    time_elapsed         | 1143      |
|    total_timesteps      | 328704    |
| train/                  |           |
|    approx_kl            | 0.5643609 |
|    clip_fraction        | 0.643     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.841     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.33      |
|    n_updates            | 6400      |
|    policy_gradient_loss | -0.208    |
|    std                  | 0.356     |
|    value_loss           | 133       |
---------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00374   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0581    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.017      |
| rollout/                |            |
|    ep_len_mean          | 56.4       |
|    ep_rew_mean          | -193       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 322        |
|    time_elapsed         | 1147       |
|    total_timesteps      | 329728     |
| train/                  |            |
|    approx_kl            | 0.53742754 |
|    clip_fraction        | 0.542      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.867      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.49       |
|    n_updates            | 6420       |
|    policy_gradient_loss | -0.19      |
|    std                  | 0.356      |
|    value_loss           | 150        |
----------------------------------------
Num timesteps: 330000
Best mean reward: -63.38 - Last mean reward per episode: -127.61
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00318   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.082     |
| reward_torque           | -3.77      |
| reward_velocity         | 0.012      |
| rollout/                |            |
|    ep_len_mean          | 36.6       |
|    ep_rew_mean          | -128       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 323        |
|    time_elapsed         | 1151       |
|    total_timesteps      | 330752     |
| train/                  |            |
|    approx_kl            | 0.21094002 |
|    clip_fraction        | 0.428      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.79       |
|    learning_rate        | 0.0003     |
|    loss                 | 8.07       |
|    n_updates            | 6440       |
|    policy_gradient_loss | -0.144     |
|    std                  | 0.356      |
|    value_loss           | 158        |
----------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00334   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0752    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0126     |
| rollout/                |            |
|    ep_len_mean          | 46.5       |
|    ep_rew_mean          | -161       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 324        |
|    time_elapsed         | 1154       |
|    total_timesteps      | 331776     |
| train/                  |            |
|    approx_kl            | 0.54393625 |
|    clip_fraction        | 0.536      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.841      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.08       |
|    n_updates            | 6460       |
|    policy_gradient_loss | -0.185     |
|    std                  | 0.356      |
|    value_loss           | 94.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00374   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0752    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0136     |
| rollout/                |            |
|    ep_len_mean          | 57.3       |
|    ep_rew_mean          | -196       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 325        |
|    time_elapsed         | 1158       |
|    total_timesteps      | 332800     |
| train/                  |            |
|    approx_kl            | 0.30211753 |
|    clip_fraction        | 0.511      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.753      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.49       |
|    n_updates            | 6480       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.356      |
|    value_loss           | 78.8       |
----------------------------------------
---------------------------------------
| reward                  | -3.93     |
| reward_contact          | -0.00308  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0764   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.013     |
| rollout/                |           |
|    ep_len_mean          | 57.2      |
|    ep_rew_mean          | -196      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 326       |
|    time_elapsed         | 1162      |
|    total_timesteps      | 333824    |
| train/                  |           |
|    approx_kl            | 0.2800269 |
|    clip_fraction        | 0.447     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.516     |
|    learning_rate        | 0.0003    |
|    loss                 | 10.5      |
|    n_updates            | 6500      |
|    policy_gradient_loss | -0.2      |
|    std                  | 0.356     |
|    value_loss           | 347       |
---------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00263   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0764    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0136     |
| rollout/                |            |
|    ep_len_mean          | 58.7       |
|    ep_rew_mean          | -201       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 327        |
|    time_elapsed         | 1165       |
|    total_timesteps      | 334848     |
| train/                  |            |
|    approx_kl            | 0.34173077 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.807      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.95       |
|    n_updates            | 6520       |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.356      |
|    value_loss           | 108        |
----------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00263   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0764    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0138     |
| rollout/                |            |
|    ep_len_mean          | 68.8       |
|    ep_rew_mean          | -234       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 328        |
|    time_elapsed         | 1169       |
|    total_timesteps      | 335872     |
| train/                  |            |
|    approx_kl            | 0.34884518 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.703      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.53       |
|    n_updates            | 6540       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.356      |
|    value_loss           | 107        |
----------------------------------------
Num timesteps: 336000
Best mean reward: -63.38 - Last mean reward per episode: -234.16
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0892    |
| reward_torque           | -3.72      |
| reward_velocity         | 0.0129     |
| rollout/                |            |
|    ep_len_mean          | 47         |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 329        |
|    time_elapsed         | 1172       |
|    total_timesteps      | 336896     |
| train/                  |            |
|    approx_kl            | 0.39822656 |
|    clip_fraction        | 0.468      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.561      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.75       |
|    n_updates            | 6560       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.356      |
|    value_loss           | 68.4       |
----------------------------------------
---------------------------------------
| reward                  | -3.91     |
| reward_contact          | -0.00139  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0892   |
| reward_torque           | -3.73     |
| reward_velocity         | 0.0145    |
| rollout/                |           |
|    ep_len_mean          | 47        |
|    ep_rew_mean          | -162      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 330       |
|    time_elapsed         | 1176      |
|    total_timesteps      | 337920    |
| train/                  |           |
|    approx_kl            | 0.3280568 |
|    clip_fraction        | 0.468     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.1     |
|    explained_variance   | 0.648     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.4       |
|    n_updates            | 6580      |
|    policy_gradient_loss | -0.206    |
|    std                  | 0.356     |
|    value_loss           | 316       |
---------------------------------------
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.00159   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0875    |
| reward_torque           | -3.7       |
| reward_velocity         | 0.0168     |
| rollout/                |            |
|    ep_len_mean          | 57.6       |
|    ep_rew_mean          | -197       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 331        |
|    time_elapsed         | 1180       |
|    total_timesteps      | 338944     |
| train/                  |            |
|    approx_kl            | 0.24517041 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.77       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.15       |
|    n_updates            | 6600       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.356      |
|    value_loss           | 104        |
----------------------------------------
----------------------------------------
| reward                  | -3.87      |
| reward_contact          | -0.000919  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0831    |
| reward_torque           | -3.71      |
| reward_velocity         | 0.0167     |
| rollout/                |            |
|    ep_len_mean          | 38.2       |
|    ep_rew_mean          | -133       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 332        |
|    time_elapsed         | 1183       |
|    total_timesteps      | 339968     |
| train/                  |            |
|    approx_kl            | 0.26216576 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.33       |
|    n_updates            | 6620       |
|    policy_gradient_loss | -0.168     |
|    std                  | 0.356      |
|    value_loss           | 96.2       |
----------------------------------------
--------------------------------------
| reward                  | -3.87    |
| reward_contact          | -0.0014  |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0831  |
| reward_torque           | -3.71    |
| reward_velocity         | 0.0165   |
| rollout/                |          |
|    ep_len_mean          | 38.8     |
|    ep_rew_mean          | -135     |
| time/                   |          |
|    fps                  | 287      |
|    iterations           | 333      |
|    time_elapsed         | 1187     |
|    total_timesteps      | 340992   |
| train/                  |          |
|    approx_kl            | 0.31723  |
|    clip_fraction        | 0.484    |
|    clip_range           | 0.4      |
|    entropy_loss         | -22      |
|    explained_variance   | 0.729    |
|    learning_rate        | 0.0003   |
|    loss                 | 2.35     |
|    n_updates            | 6640     |
|    policy_gradient_loss | -0.211   |
|    std                  | 0.356    |
|    value_loss           | 172      |
--------------------------------------
Num timesteps: 342000
Best mean reward: -63.38 - Last mean reward per episode: -101.22
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00122   |
| reward_ctrl             | -0.0988    |
| reward_motion           | -0.0769    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0147     |
| rollout/                |            |
|    ep_len_mean          | 28.4       |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 334        |
|    time_elapsed         | 1191       |
|    total_timesteps      | 342016     |
| train/                  |            |
|    approx_kl            | 0.57439095 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.854      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.28       |
|    n_updates            | 6660       |
|    policy_gradient_loss | -0.22      |
|    std                  | 0.356      |
|    value_loss           | 124        |
----------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00214   |
| reward_ctrl             | -0.0988    |
| reward_motion           | -0.0868    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 27.5       |
|    ep_rew_mean          | -98.1      |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 335        |
|    time_elapsed         | 1194       |
|    total_timesteps      | 343040     |
| train/                  |            |
|    approx_kl            | 0.41933316 |
|    clip_fraction        | 0.569      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.67       |
|    n_updates            | 6680       |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.356      |
|    value_loss           | 106        |
----------------------------------------
---------------------------------------
| reward                  | -3.92     |
| reward_contact          | -0.00224  |
| reward_ctrl             | -0.0988   |
| reward_motion           | -0.083    |
| reward_torque           | -3.76     |
| reward_velocity         | 0.0168    |
| rollout/                |           |
|    ep_len_mean          | 37.4      |
|    ep_rew_mean          | -131      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 336       |
|    time_elapsed         | 1198      |
|    total_timesteps      | 344064    |
| train/                  |           |
|    approx_kl            | 0.7729536 |
|    clip_fraction        | 0.638     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.922     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.845     |
|    n_updates            | 6700      |
|    policy_gradient_loss | -0.203    |
|    std                  | 0.356     |
|    value_loss           | 74.5      |
---------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00241   |
| reward_ctrl             | -0.0988    |
| reward_motion           | -0.0788    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0185     |
| rollout/                |            |
|    ep_len_mean          | 47.3       |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 337        |
|    time_elapsed         | 1202       |
|    total_timesteps      | 345088     |
| train/                  |            |
|    approx_kl            | 0.42307824 |
|    clip_fraction        | 0.485      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.893      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.08       |
|    n_updates            | 6720       |
|    policy_gradient_loss | -0.156     |
|    std                  | 0.356      |
|    value_loss           | 127        |
----------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00217   |
| reward_ctrl             | -0.0988    |
| reward_motion           | -0.0698    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0207     |
| rollout/                |            |
|    ep_len_mean          | 49.1       |
|    ep_rew_mean          | -170       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 338        |
|    time_elapsed         | 1205       |
|    total_timesteps      | 346112     |
| train/                  |            |
|    approx_kl            | 0.29145288 |
|    clip_fraction        | 0.497      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.29       |
|    n_updates            | 6740       |
|    policy_gradient_loss | -0.203     |
|    std                  | 0.356      |
|    value_loss           | 141        |
----------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00169   |
| reward_ctrl             | -0.0988    |
| reward_motion           | -0.0606    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0212     |
| rollout/                |            |
|    ep_len_mean          | 60.5       |
|    ep_rew_mean          | -208       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 339        |
|    time_elapsed         | 1209       |
|    total_timesteps      | 347136     |
| train/                  |            |
|    approx_kl            | 0.32728803 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.98       |
|    n_updates            | 6760       |
|    policy_gradient_loss | -0.184     |
|    std                  | 0.356      |
|    value_loss           | 106        |
----------------------------------------
Num timesteps: 348000
Best mean reward: -63.38 - Last mean reward per episode: -207.64
---------------------------------------
| reward                  | -3.91     |
| reward_contact          | -0.00119  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0737   |
| reward_torque           | -3.75     |
| reward_velocity         | 0.0181    |
| rollout/                |           |
|    ep_len_mean          | 60        |
|    ep_rew_mean          | -207      |
| time/                   |           |
|    fps                  | 287       |
|    iterations           | 340       |
|    time_elapsed         | 1212      |
|    total_timesteps      | 348160    |
| train/                  |           |
|    approx_kl            | 0.3496292 |
|    clip_fraction        | 0.517     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.854     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.65      |
|    n_updates            | 6780      |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.356     |
|    value_loss           | 133       |
---------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00124   |
| reward_ctrl             | -0.099     |
| reward_motion           | -0.0714    |
| reward_torque           | -3.75      |
| reward_velocity         | 0.0237     |
| rollout/                |            |
|    ep_len_mean          | 69.3       |
|    ep_rew_mean          | -237       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 341        |
|    time_elapsed         | 1216       |
|    total_timesteps      | 349184     |
| train/                  |            |
|    approx_kl            | 0.55597585 |
|    clip_fraction        | 0.558      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.854      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.66       |
|    n_updates            | 6800       |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.356      |
|    value_loss           | 131        |
----------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.00037   |
| reward_ctrl             | -0.099     |
| reward_motion           | -0.0658    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0231     |
| rollout/                |            |
|    ep_len_mean          | 60.3       |
|    ep_rew_mean          | -208       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 342        |
|    time_elapsed         | 1220       |
|    total_timesteps      | 350208     |
| train/                  |            |
|    approx_kl            | 0.39476937 |
|    clip_fraction        | 0.496      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.848      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.33       |
|    n_updates            | 6820       |
|    policy_gradient_loss | -0.154     |
|    std                  | 0.356      |
|    value_loss           | 95.7       |
----------------------------------------
---------------------------------------
| reward                  | -3.93     |
| reward_contact          | -0.000344 |
| reward_ctrl             | -0.099    |
| reward_motion           | -0.0855   |
| reward_torque           | -3.77     |
| reward_velocity         | 0.0187    |
| rollout/                |           |
|    ep_len_mean          | 47.4      |
|    ep_rew_mean          | -165      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 343       |
|    time_elapsed         | 1224      |
|    total_timesteps      | 351232    |
| train/                  |           |
|    approx_kl            | 0.5752926 |
|    clip_fraction        | 0.606     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.881     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.19      |
|    n_updates            | 6840      |
|    policy_gradient_loss | -0.196    |
|    std                  | 0.356     |
|    value_loss           | 121       |
---------------------------------------
---------------------------------------
| reward                  | -3.93     |
| reward_contact          | -0.00044  |
| reward_ctrl             | -0.099    |
| reward_motion           | -0.0777   |
| reward_torque           | -3.77     |
| reward_velocity         | 0.0189    |
| rollout/                |           |
|    ep_len_mean          | 57.7      |
|    ep_rew_mean          | -199      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 344       |
|    time_elapsed         | 1227      |
|    total_timesteps      | 352256    |
| train/                  |           |
|    approx_kl            | 0.2618421 |
|    clip_fraction        | 0.485     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.825     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.23      |
|    n_updates            | 6860      |
|    policy_gradient_loss | -0.185    |
|    std                  | 0.356     |
|    value_loss           | 217       |
---------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00092   |
| reward_ctrl             | -0.099     |
| reward_motion           | -0.0777    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0188     |
| rollout/                |            |
|    ep_len_mean          | 46.6       |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 345        |
|    time_elapsed         | 1231       |
|    total_timesteps      | 353280     |
| train/                  |            |
|    approx_kl            | 0.36548486 |
|    clip_fraction        | 0.501      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.0003     |
|    loss                 | 43.8       |
|    n_updates            | 6880       |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.356      |
|    value_loss           | 122        |
----------------------------------------
Num timesteps: 354000
Best mean reward: -63.38 - Last mean reward per episode: -165.12
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00107   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0742    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0159     |
| rollout/                |            |
|    ep_len_mean          | 47.6       |
|    ep_rew_mean          | -165       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 346        |
|    time_elapsed         | 1235       |
|    total_timesteps      | 354304     |
| train/                  |            |
|    approx_kl            | 0.44719616 |
|    clip_fraction        | 0.548      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.57       |
|    n_updates            | 6900       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.356      |
|    value_loss           | 99.8       |
----------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00149   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0637    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0208     |
| rollout/                |            |
|    ep_len_mean          | 57.5       |
|    ep_rew_mean          | -198       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 347        |
|    time_elapsed         | 1238       |
|    total_timesteps      | 355328     |
| train/                  |            |
|    approx_kl            | 0.54739803 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.87       |
|    n_updates            | 6920       |
|    policy_gradient_loss | -0.169     |
|    std                  | 0.356      |
|    value_loss           | 127        |
----------------------------------------
---------------------------------------
| reward                  | -3.91     |
| reward_contact          | -0.00148  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0604   |
| reward_torque           | -3.77     |
| reward_velocity         | 0.0226    |
| rollout/                |           |
|    ep_len_mean          | 67        |
|    ep_rew_mean          | -229      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 348       |
|    time_elapsed         | 1242      |
|    total_timesteps      | 356352    |
| train/                  |           |
|    approx_kl            | 43.801178 |
|    clip_fraction        | 0.477     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.86      |
|    learning_rate        | 0.0003    |
|    loss                 | 3.61      |
|    n_updates            | 6940      |
|    policy_gradient_loss | -0.143    |
|    std                  | 0.356     |
|    value_loss           | 124       |
---------------------------------------
---------------------------------------
| reward                  | -3.9      |
| reward_contact          | -0.00145  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0804   |
| reward_torque           | -3.74     |
| reward_velocity         | 0.0188    |
| rollout/                |           |
|    ep_len_mean          | 48        |
|    ep_rew_mean          | -167      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 349       |
|    time_elapsed         | 1246      |
|    total_timesteps      | 357376    |
| train/                  |           |
|    approx_kl            | 0.8180683 |
|    clip_fraction        | 0.633     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.914     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.26      |
|    n_updates            | 6960      |
|    policy_gradient_loss | -0.186    |
|    std                  | 0.356     |
|    value_loss           | 77.6      |
---------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00145   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.072     |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0223     |
| rollout/                |            |
|    ep_len_mean          | 58.3       |
|    ep_rew_mean          | -201       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 350        |
|    time_elapsed         | 1249       |
|    total_timesteps      | 358400     |
| train/                  |            |
|    approx_kl            | 0.47330943 |
|    clip_fraction        | 0.482      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.59       |
|    n_updates            | 6980       |
|    policy_gradient_loss | -0.201     |
|    std                  | 0.356      |
|    value_loss           | 182        |
----------------------------------------
---------------------------------------
| reward                  | -3.89     |
| reward_contact          | -0.00187  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0778   |
| reward_torque           | -3.74     |
| reward_velocity         | 0.0201    |
| rollout/                |           |
|    ep_len_mean          | 57.8      |
|    ep_rew_mean          | -200      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 351       |
|    time_elapsed         | 1253      |
|    total_timesteps      | 359424    |
| train/                  |           |
|    approx_kl            | 1.1113491 |
|    clip_fraction        | 0.552     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.745     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.08      |
|    n_updates            | 7000      |
|    policy_gradient_loss | -0.112    |
|    std                  | 0.356     |
|    value_loss           | 75.9      |
---------------------------------------
Num timesteps: 360000
Best mean reward: -63.38 - Last mean reward per episode: -135.13
----------------------------------------
| reward                  | -3.9       |
| reward_contact          | -0.00135   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0808    |
| reward_torque           | -3.73      |
| reward_velocity         | 0.0141     |
| rollout/                |            |
|    ep_len_mean          | 38.3       |
|    ep_rew_mean          | -135       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 352        |
|    time_elapsed         | 1257       |
|    total_timesteps      | 360448     |
| train/                  |            |
|    approx_kl            | 0.32027358 |
|    clip_fraction        | 0.611      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.842      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.43       |
|    n_updates            | 7020       |
|    policy_gradient_loss | -0.147     |
|    std                  | 0.356      |
|    value_loss           | 129        |
----------------------------------------
----------------------------------------
| reward                  | -3.9       |
| reward_contact          | -0.00183   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0767    |
| reward_torque           | -3.74      |
| reward_velocity         | 0.0166     |
| rollout/                |            |
|    ep_len_mean          | 48.3       |
|    ep_rew_mean          | -169       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 353        |
|    time_elapsed         | 1260       |
|    total_timesteps      | 361472     |
| train/                  |            |
|    approx_kl            | 0.38496023 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.87       |
|    n_updates            | 7040       |
|    policy_gradient_loss | -0.198     |
|    std                  | 0.356      |
|    value_loss           | 142        |
----------------------------------------
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.00291  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0584   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0159    |
| rollout/                |           |
|    ep_len_mean          | 58.8      |
|    ep_rew_mean          | -204      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 354       |
|    time_elapsed         | 1264      |
|    total_timesteps      | 362496    |
| train/                  |           |
|    approx_kl            | 0.3748232 |
|    clip_fraction        | 0.467     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.819     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.93      |
|    n_updates            | 7060      |
|    policy_gradient_loss | -0.15     |
|    std                  | 0.356     |
|    value_loss           | 63.3      |
---------------------------------------
--------------------------------------
| reward                  | -3.99    |
| reward_contact          | -0.00315 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0584  |
| reward_torque           | -3.85    |
| reward_velocity         | 0.0178   |
| rollout/                |          |
|    ep_len_mean          | 59.9     |
|    ep_rew_mean          | -208     |
| time/                   |          |
|    fps                  | 286      |
|    iterations           | 355      |
|    time_elapsed         | 1267     |
|    total_timesteps      | 363520   |
| train/                  |          |
|    approx_kl            | 0.549195 |
|    clip_fraction        | 0.457    |
|    clip_range           | 0.4      |
|    entropy_loss         | -22.2    |
|    explained_variance   | 0.762    |
|    learning_rate        | 0.0003   |
|    loss                 | 5.49     |
|    n_updates            | 7080     |
|    policy_gradient_loss | -0.182   |
|    std                  | 0.356    |
|    value_loss           | 266      |
--------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00244   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0518    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0137     |
| rollout/                |            |
|    ep_len_mean          | 46.9       |
|    ep_rew_mean          | -163       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 356        |
|    time_elapsed         | 1271       |
|    total_timesteps      | 364544     |
| train/                  |            |
|    approx_kl            | 0.12948388 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.0003     |
|    loss                 | 20.2       |
|    n_updates            | 7100       |
|    policy_gradient_loss | -0.156     |
|    std                  | 0.356      |
|    value_loss           | 226        |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00196   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0569    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0112     |
| rollout/                |            |
|    ep_len_mean          | 36.7       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 357        |
|    time_elapsed         | 1275       |
|    total_timesteps      | 365568     |
| train/                  |            |
|    approx_kl            | 0.37696522 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.28       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.84       |
|    n_updates            | 7120       |
|    policy_gradient_loss | -0.218     |
|    std                  | 0.356      |
|    value_loss           | 234        |
----------------------------------------
Num timesteps: 366000
Best mean reward: -63.38 - Last mean reward per episode: -132.75
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00174   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0509    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0142     |
| rollout/                |            |
|    ep_len_mean          | 37.9       |
|    ep_rew_mean          | -133       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 358        |
|    time_elapsed         | 1278       |
|    total_timesteps      | 366592     |
| train/                  |            |
|    approx_kl            | 0.18941039 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.823      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.97       |
|    n_updates            | 7140       |
|    policy_gradient_loss | -0.156     |
|    std                  | 0.356      |
|    value_loss           | 130        |
----------------------------------------
----------------------------------------
| reward                  | -3.92      |
| reward_contact          | -0.00126   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0441    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.0179     |
| rollout/                |            |
|    ep_len_mean          | 47.8       |
|    ep_rew_mean          | -166       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 359        |
|    time_elapsed         | 1282       |
|    total_timesteps      | 367616     |
| train/                  |            |
|    approx_kl            | 0.33528614 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.801      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.35       |
|    n_updates            | 7160       |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.356      |
|    value_loss           | 190        |
----------------------------------------
----------------------------------------
| reward                  | -3.91      |
| reward_contact          | -0.0012    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0482    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.0212     |
| rollout/                |            |
|    ep_len_mean          | 47         |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 360        |
|    time_elapsed         | 1286       |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.29301536 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.782      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.5        |
|    n_updates            | 7180       |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.356      |
|    value_loss           | 156        |
----------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00109   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0449    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0229     |
| rollout/                |            |
|    ep_len_mean          | 57.9       |
|    ep_rew_mean          | -200       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 361        |
|    time_elapsed         | 1289       |
|    total_timesteps      | 369664     |
| train/                  |            |
|    approx_kl            | 0.24608983 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.801      |
|    learning_rate        | 0.0003     |
|    loss                 | 11         |
|    n_updates            | 7200       |
|    policy_gradient_loss | -0.172     |
|    std                  | 0.356      |
|    value_loss           | 180        |
----------------------------------------
----------------------------------------
| reward                  | -3.89      |
| reward_contact          | -0.00109   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0449    |
| reward_torque           | -3.77      |
| reward_velocity         | 0.0227     |
| rollout/                |            |
|    ep_len_mean          | 57.8       |
|    ep_rew_mean          | -200       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 362        |
|    time_elapsed         | 1293       |
|    total_timesteps      | 370688     |
| train/                  |            |
|    approx_kl            | 0.51944435 |
|    clip_fraction        | 0.491      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.81       |
|    n_updates            | 7220       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.356      |
|    value_loss           | 96.4       |
----------------------------------------
----------------------------------------
| reward                  | -3.86      |
| reward_contact          | -0.00107   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.055     |
| reward_torque           | -3.73      |
| reward_velocity         | 0.0239     |
| rollout/                |            |
|    ep_len_mean          | 58.8       |
|    ep_rew_mean          | -203       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 363        |
|    time_elapsed         | 1297       |
|    total_timesteps      | 371712     |
| train/                  |            |
|    approx_kl            | 0.23502257 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.669      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.03       |
|    n_updates            | 7240       |
|    policy_gradient_loss | -0.185     |
|    std                  | 0.356      |
|    value_loss           | 101        |
----------------------------------------
Num timesteps: 372000
Best mean reward: -63.38 - Last mean reward per episode: -203.43
----------------------------------------
| reward                  | -3.86      |
| reward_contact          | -0.00107   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.055     |
| reward_torque           | -3.73      |
| reward_velocity         | 0.0239     |
| rollout/                |            |
|    ep_len_mean          | 69.1       |
|    ep_rew_mean          | -238       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 364        |
|    time_elapsed         | 1300       |
|    total_timesteps      | 372736     |
| train/                  |            |
|    approx_kl            | 0.25955117 |
|    clip_fraction        | 0.44       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.127      |
|    learning_rate        | 0.0003     |
|    loss                 | 15         |
|    n_updates            | 7260       |
|    policy_gradient_loss | -0.205     |
|    std                  | 0.356      |
|    value_loss           | 425        |
----------------------------------------
----------------------------------------
| reward                  | -3.9       |
| reward_contact          | -0.000825  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0878    |
| reward_torque           | -3.73      |
| reward_velocity         | 0.0154     |
| rollout/                |            |
|    ep_len_mean          | 29         |
|    ep_rew_mean          | -105       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 365        |
|    time_elapsed         | 1304       |
|    total_timesteps      | 373760     |
| train/                  |            |
|    approx_kl            | 0.36993116 |
|    clip_fraction        | 0.512      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.99       |
|    n_updates            | 7280       |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.356      |
|    value_loss           | 69.4       |
----------------------------------------
---------------------------------------
| reward                  | -3.92     |
| reward_contact          | -0.000825 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0878   |
| reward_torque           | -3.74     |
| reward_velocity         | 0.0162    |
| rollout/                |           |
|    ep_len_mean          | 39.1      |
|    ep_rew_mean          | -138      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 366       |
|    time_elapsed         | 1307      |
|    total_timesteps      | 374784    |
| train/                  |           |
|    approx_kl            | 0.4815482 |
|    clip_fraction        | 0.476     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.3     |
|    explained_variance   | -0.195    |
|    learning_rate        | 0.0003    |
|    loss                 | 2.6       |
|    n_updates            | 7300      |
|    policy_gradient_loss | -0.21     |
|    std                  | 0.356     |
|    value_loss           | 231       |
---------------------------------------
---------------------------------------
| reward                  | -3.93     |
| reward_contact          | -0.000827 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0931   |
| reward_torque           | -3.76     |
| reward_velocity         | 0.0195    |
| rollout/                |           |
|    ep_len_mean          | 38.3      |
|    ep_rew_mean          | -135      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 367       |
|    time_elapsed         | 1311      |
|    total_timesteps      | 375808    |
| train/                  |           |
|    approx_kl            | 0.4816535 |
|    clip_fraction        | 0.399     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.51      |
|    learning_rate        | 0.0003    |
|    loss                 | 6.35      |
|    n_updates            | 7320      |
|    policy_gradient_loss | -0.147    |
|    std                  | 0.356     |
|    value_loss           | 110       |
---------------------------------------
---------------------------------------
| reward                  | -3.95     |
| reward_contact          | -0.00107  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0884   |
| reward_torque           | -3.78     |
| reward_velocity         | 0.0194    |
| rollout/                |           |
|    ep_len_mean          | 47.2      |
|    ep_rew_mean          | -164      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 368       |
|    time_elapsed         | 1315      |
|    total_timesteps      | 376832    |
| train/                  |           |
|    approx_kl            | 0.6146321 |
|    clip_fraction        | 0.554     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.1     |
|    explained_variance   | 0.848     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.57      |
|    n_updates            | 7340      |
|    policy_gradient_loss | -0.2      |
|    std                  | 0.355     |
|    value_loss           | 177       |
---------------------------------------
---------------------------------------
| reward                  | -3.97     |
| reward_contact          | -0.000859 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0737   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.015     |
| rollout/                |           |
|    ep_len_mean          | 47.1      |
|    ep_rew_mean          | -164      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 369       |
|    time_elapsed         | 1318      |
|    total_timesteps      | 377856    |
| train/                  |           |
|    approx_kl            | 0.2647634 |
|    clip_fraction        | 0.454     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.748     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.66      |
|    n_updates            | 7360      |
|    policy_gradient_loss | -0.157    |
|    std                  | 0.355     |
|    value_loss           | 116       |
---------------------------------------
Num timesteps: 378000
Best mean reward: -63.38 - Last mean reward per episode: -163.50
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.000859 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0737   |
| reward_torque           | -3.8      |
| reward_velocity         | 0.0151    |
| rollout/                |           |
|    ep_len_mean          | 57.1      |
|    ep_rew_mean          | -197      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 370       |
|    time_elapsed         | 1322      |
|    total_timesteps      | 378880    |
| train/                  |           |
|    approx_kl            | 0.2758844 |
|    clip_fraction        | 0.454     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.729     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.94      |
|    n_updates            | 7380      |
|    policy_gradient_loss | -0.187    |
|    std                  | 0.355     |
|    value_loss           | 254       |
---------------------------------------
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.0011   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0682   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.0195    |
| rollout/                |           |
|    ep_len_mean          | 57.3      |
|    ep_rew_mean          | -198      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 371       |
|    time_elapsed         | 1326      |
|    total_timesteps      | 379904    |
| train/                  |           |
|    approx_kl            | 0.2308224 |
|    clip_fraction        | 0.458     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.697     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.23      |
|    n_updates            | 7400      |
|    policy_gradient_loss | -0.172    |
|    std                  | 0.355     |
|    value_loss           | 107       |
---------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00122   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0642    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.021      |
| rollout/                |            |
|    ep_len_mean          | 67.5       |
|    ep_rew_mean          | -231       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 372        |
|    time_elapsed         | 1329       |
|    total_timesteps      | 380928     |
| train/                  |            |
|    approx_kl            | 0.65539503 |
|    clip_fraction        | 0.566      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.792      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.27       |
|    n_updates            | 7420       |
|    policy_gradient_loss | -0.138     |
|    std                  | 0.355      |
|    value_loss           | 33.2       |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.0019    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0384    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0149     |
| rollout/                |            |
|    ep_len_mean          | 67.1       |
|    ep_rew_mean          | -231       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 373        |
|    time_elapsed         | 1333       |
|    total_timesteps      | 381952     |
| train/                  |            |
|    approx_kl            | 0.34901255 |
|    clip_fraction        | 0.493      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.65       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.68       |
|    n_updates            | 7440       |
|    policy_gradient_loss | -0.148     |
|    std                  | 0.355      |
|    value_loss           | 138        |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00169   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0384    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.016      |
| rollout/                |            |
|    ep_len_mean          | 77.1       |
|    ep_rew_mean          | -263       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 374        |
|    time_elapsed         | 1337       |
|    total_timesteps      | 382976     |
| train/                  |            |
|    approx_kl            | 0.30204695 |
|    clip_fraction        | 0.493      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.64       |
|    n_updates            | 7460       |
|    policy_gradient_loss | -0.209     |
|    std                  | 0.355      |
|    value_loss           | 288        |
----------------------------------------
Num timesteps: 384000
Best mean reward: -63.38 - Last mean reward per episode: -194.90
---------------------------------------
| reward                  | -3.94     |
| reward_contact          | -0.00153  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0442   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.0154    |
| rollout/                |           |
|    ep_len_mean          | 56.5      |
|    ep_rew_mean          | -195      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 375       |
|    time_elapsed         | 1340      |
|    total_timesteps      | 384000    |
| train/                  |           |
|    approx_kl            | 0.4172854 |
|    clip_fraction        | 0.557     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.831     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.1       |
|    n_updates            | 7480      |
|    policy_gradient_loss | -0.169    |
|    std                  | 0.355     |
|    value_loss           | 112       |
---------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00176   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0701    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0143     |
| rollout/                |            |
|    ep_len_mean          | 34.4       |
|    ep_rew_mean          | -121       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 376        |
|    time_elapsed         | 1344       |
|    total_timesteps      | 385024     |
| train/                  |            |
|    approx_kl            | 0.67804694 |
|    clip_fraction        | 0.564      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.15       |
|    n_updates            | 7500       |
|    policy_gradient_loss | -0.181     |
|    std                  | 0.355      |
|    value_loss           | 52.7       |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.0017    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0839    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0164     |
| rollout/                |            |
|    ep_len_mean          | 45.4       |
|    ep_rew_mean          | -158       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 377        |
|    time_elapsed         | 1348       |
|    total_timesteps      | 386048     |
| train/                  |            |
|    approx_kl            | 0.41826862 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.881      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.78       |
|    n_updates            | 7520       |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.355      |
|    value_loss           | 176        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00173   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0839    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0155     |
| rollout/                |            |
|    ep_len_mean          | 46.4       |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 378        |
|    time_elapsed         | 1352       |
|    total_timesteps      | 387072     |
| train/                  |            |
|    approx_kl            | 0.18567243 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.833      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.5       |
|    n_updates            | 7540       |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.355      |
|    value_loss           | 261        |
----------------------------------------
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00187   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0881    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0146     |
| rollout/                |            |
|    ep_len_mean          | 47         |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 379        |
|    time_elapsed         | 1355       |
|    total_timesteps      | 388096     |
| train/                  |            |
|    approx_kl            | 0.42830938 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.58       |
|    n_updates            | 7560       |
|    policy_gradient_loss | -0.198     |
|    std                  | 0.355      |
|    value_loss           | 115        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00128   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0917    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0101     |
| rollout/                |            |
|    ep_len_mean          | 47.2       |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 380        |
|    time_elapsed         | 1359       |
|    total_timesteps      | 389120     |
| train/                  |            |
|    approx_kl            | 0.26628798 |
|    clip_fraction        | 0.473      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.14       |
|    n_updates            | 7580       |
|    policy_gradient_loss | -0.196     |
|    std                  | 0.355      |
|    value_loss           | 289        |
----------------------------------------
Num timesteps: 390000
Best mean reward: -63.38 - Last mean reward per episode: -130.86
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.00112  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0976   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.00747   |
| rollout/                |           |
|    ep_len_mean          | 37.2      |
|    ep_rew_mean          | -131      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 381       |
|    time_elapsed         | 1363      |
|    total_timesteps      | 390144    |
| train/                  |           |
|    approx_kl            | 0.2895919 |
|    clip_fraction        | 0.483     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.3     |
|    explained_variance   | 0.698     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.24      |
|    n_updates            | 7600      |
|    policy_gradient_loss | -0.197    |
|    std                  | 0.355     |
|    value_loss           | 171       |
---------------------------------------
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.00116  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.1      |
| reward_torque           | -3.8      |
| reward_velocity         | 0.00909   |
| rollout/                |           |
|    ep_len_mean          | 35.6      |
|    ep_rew_mean          | -125      |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 382       |
|    time_elapsed         | 1367      |
|    total_timesteps      | 391168    |
| train/                  |           |
|    approx_kl            | 0.5427918 |
|    clip_fraction        | 0.525     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.911     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.92      |
|    n_updates            | 7620      |
|    policy_gradient_loss | -0.203    |
|    std                  | 0.355     |
|    value_loss           | 112       |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.0014    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.82      |
| reward_velocity         | 0.00928    |
| rollout/                |            |
|    ep_len_mean          | 25.7       |
|    ep_rew_mean          | -92.6      |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 383        |
|    time_elapsed         | 1370       |
|    total_timesteps      | 392192     |
| train/                  |            |
|    approx_kl            | 0.28788298 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.3       |
|    n_updates            | 7640       |
|    policy_gradient_loss | -0.2       |
|    std                  | 0.355      |
|    value_loss           | 210        |
----------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00191   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0103     |
| rollout/                |            |
|    ep_len_mean          | 35.8       |
|    ep_rew_mean          | -126       |
| time/                   |            |
|    fps                  | 286        |
|    iterations           | 384        |
|    time_elapsed         | 1374       |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.25013903 |
|    clip_fraction        | 0.472      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.782      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.85       |
|    n_updates            | 7660       |
|    policy_gradient_loss | -0.199     |
|    std                  | 0.355      |
|    value_loss           | 148        |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00191   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0904    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0105     |
| rollout/                |            |
|    ep_len_mean          | 47.4       |
|    ep_rew_mean          | -165       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 385        |
|    time_elapsed         | 1378       |
|    total_timesteps      | 394240     |
| train/                  |            |
|    approx_kl            | 0.31216162 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.98       |
|    n_updates            | 7680       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.355      |
|    value_loss           | 149        |
----------------------------------------
---------------------------------------
| reward                  | -3.98     |
| reward_contact          | -0.00095  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0766   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.00873   |
| rollout/                |           |
|    ep_len_mean          | 37.8      |
|    ep_rew_mean          | -133      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 386       |
|    time_elapsed         | 1382      |
|    total_timesteps      | 395264    |
| train/                  |           |
|    approx_kl            | 0.4776835 |
|    clip_fraction        | 0.502     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.886     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.05      |
|    n_updates            | 7700      |
|    policy_gradient_loss | -0.189    |
|    std                  | 0.355     |
|    value_loss           | 127       |
---------------------------------------
Num timesteps: 396000
Best mean reward: -63.38 - Last mean reward per episode: -163.86
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00095   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0714    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.00935    |
| rollout/                |            |
|    ep_len_mean          | 47.1       |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 387        |
|    time_elapsed         | 1386       |
|    total_timesteps      | 396288     |
| train/                  |            |
|    approx_kl            | 0.49872696 |
|    clip_fraction        | 0.603      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.08       |
|    n_updates            | 7720       |
|    policy_gradient_loss | -0.209     |
|    std                  | 0.355      |
|    value_loss           | 128        |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00048   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0711    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.00899    |
| rollout/                |            |
|    ep_len_mean          | 31.2       |
|    ep_rew_mean          | -112       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 388        |
|    time_elapsed         | 1390       |
|    total_timesteps      | 397312     |
| train/                  |            |
|    approx_kl            | 0.33789963 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.07       |
|    n_updates            | 7740       |
|    policy_gradient_loss | -0.154     |
|    std                  | 0.355      |
|    value_loss           | 135        |
----------------------------------------
---------------------------------------
| reward                  | -3.97     |
| reward_contact          | -0.00048  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0626   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0108    |
| rollout/                |           |
|    ep_len_mean          | 40.5      |
|    ep_rew_mean          | -143      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 389       |
|    time_elapsed         | 1393      |
|    total_timesteps      | 398336    |
| train/                  |           |
|    approx_kl            | 0.5179801 |
|    clip_fraction        | 0.489     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.3     |
|    explained_variance   | 0.82      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.78      |
|    n_updates            | 7760      |
|    policy_gradient_loss | -0.203    |
|    std                  | 0.355     |
|    value_loss           | 172       |
---------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00048   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0704    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0107     |
| rollout/                |            |
|    ep_len_mean          | 41         |
|    ep_rew_mean          | -145       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 390        |
|    time_elapsed         | 1397       |
|    total_timesteps      | 399360     |
| train/                  |            |
|    approx_kl            | 0.30704182 |
|    clip_fraction        | 0.483      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.621      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.82       |
|    n_updates            | 7780       |
|    policy_gradient_loss | -0.172     |
|    std                  | 0.355      |
|    value_loss           | 145        |
----------------------------------------
---------------------------------------
| reward                  | -3.98     |
| reward_contact          | -0.00072  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0632   |
| reward_torque           | -3.83     |
| reward_velocity         | 0.0108    |
| rollout/                |           |
|    ep_len_mean          | 51.1      |
|    ep_rew_mean          | -178      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 391       |
|    time_elapsed         | 1401      |
|    total_timesteps      | 400384    |
| train/                  |           |
|    approx_kl            | 1.3394313 |
|    clip_fraction        | 0.542     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.826     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.84      |
|    n_updates            | 7800      |
|    policy_gradient_loss | -0.158    |
|    std                  | 0.355     |
|    value_loss           | 47        |
---------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00072   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0529    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0112     |
| rollout/                |            |
|    ep_len_mean          | 61.1       |
|    ep_rew_mean          | -211       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 392        |
|    time_elapsed         | 1404       |
|    total_timesteps      | 401408     |
| train/                  |            |
|    approx_kl            | 0.23714311 |
|    clip_fraction        | 0.491      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.62       |
|    n_updates            | 7820       |
|    policy_gradient_loss | -0.139     |
|    std                  | 0.355      |
|    value_loss           | 120        |
----------------------------------------
Num timesteps: 402000
Best mean reward: -63.38 - Last mean reward per episode: -246.33
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.000796  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0506    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0143     |
| rollout/                |            |
|    ep_len_mean          | 71.4       |
|    ep_rew_mean          | -246       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 393        |
|    time_elapsed         | 1408       |
|    total_timesteps      | 402432     |
| train/                  |            |
|    approx_kl            | 0.40642262 |
|    clip_fraction        | 0.516      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.62       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.65       |
|    n_updates            | 7840       |
|    policy_gradient_loss | -0.147     |
|    std                  | 0.355      |
|    value_loss           | 97.2       |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00128   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0367    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0162     |
| rollout/                |            |
|    ep_len_mean          | 65.8       |
|    ep_rew_mean          | -226       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 394        |
|    time_elapsed         | 1412       |
|    total_timesteps      | 403456     |
| train/                  |            |
|    approx_kl            | 0.37478182 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.615      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.02       |
|    n_updates            | 7860       |
|    policy_gradient_loss | -0.125     |
|    std                  | 0.355      |
|    value_loss           | 102        |
----------------------------------------
----------------------------------------
| reward                  | -3.88      |
| reward_contact          | -0.00128   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0336    |
| reward_torque           | -3.76      |
| reward_velocity         | 0.0171     |
| rollout/                |            |
|    ep_len_mean          | 75.7       |
|    ep_rew_mean          | -259       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 395        |
|    time_elapsed         | 1416       |
|    total_timesteps      | 404480     |
| train/                  |            |
|    approx_kl            | 0.52354705 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.767      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.17       |
|    n_updates            | 7880       |
|    policy_gradient_loss | -0.211     |
|    std                  | 0.355      |
|    value_loss           | 305        |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00048   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0525    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0114     |
| rollout/                |            |
|    ep_len_mean          | 24.6       |
|    ep_rew_mean          | -89        |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 396        |
|    time_elapsed         | 1419       |
|    total_timesteps      | 405504     |
| train/                  |            |
|    approx_kl            | 0.60849965 |
|    clip_fraction        | 0.561      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.893      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.51       |
|    n_updates            | 7900       |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.355      |
|    value_loss           | 106        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.000815  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0525    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0111     |
| rollout/                |            |
|    ep_len_mean          | 35         |
|    ep_rew_mean          | -124       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 397        |
|    time_elapsed         | 1423       |
|    total_timesteps      | 406528     |
| train/                  |            |
|    approx_kl            | 0.44664246 |
|    clip_fraction        | 0.492      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.807      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.53       |
|    n_updates            | 7920       |
|    policy_gradient_loss | -0.204     |
|    std                  | 0.355      |
|    value_loss           | 172        |
----------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.000335 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0749   |
| reward_torque           | -3.86     |
| reward_velocity         | 0.00905   |
| rollout/                |           |
|    ep_len_mean          | 26.9      |
|    ep_rew_mean          | -97.4     |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 398       |
|    time_elapsed         | 1427      |
|    total_timesteps      | 407552    |
| train/                  |           |
|    approx_kl            | 0.3157359 |
|    clip_fraction        | 0.491     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.878     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.02      |
|    n_updates            | 7940      |
|    policy_gradient_loss | -0.152    |
|    std                  | 0.355     |
|    value_loss           | 116       |
---------------------------------------
Num timesteps: 408000
Best mean reward: -63.38 - Last mean reward per episode: -97.40
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.000335 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0748   |
| reward_torque           | -3.85     |
| reward_velocity         | 0.00922   |
| rollout/                |           |
|    ep_len_mean          | 36.8      |
|    ep_rew_mean          | -130      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 399       |
|    time_elapsed         | 1430      |
|    total_timesteps      | 408576    |
| train/                  |           |
|    approx_kl            | 0.6823025 |
|    clip_fraction        | 0.506     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.897     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.02      |
|    n_updates            | 7960      |
|    policy_gradient_loss | -0.202    |
|    std                  | 0.355     |
|    value_loss           | 156       |
---------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.000863 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.09     |
| reward_torque           | -3.84     |
| reward_velocity         | 0.00965   |
| rollout/                |           |
|    ep_len_mean          | 26.6      |
|    ep_rew_mean          | -96       |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 400       |
|    time_elapsed         | 1434      |
|    total_timesteps      | 409600    |
| train/                  |           |
|    approx_kl            | 0.3025665 |
|    clip_fraction        | 0.491     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.895     |
|    learning_rate        | 0.0003    |
|    loss                 | 28        |
|    n_updates            | 7980      |
|    policy_gradient_loss | -0.143    |
|    std                  | 0.355     |
|    value_loss           | 262       |
---------------------------------------
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.00106   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0857    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.012      |
| rollout/                |            |
|    ep_len_mean          | 37         |
|    ep_rew_mean          | -131       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 401        |
|    time_elapsed         | 1438       |
|    total_timesteps      | 410624     |
| train/                  |            |
|    approx_kl            | 0.34966886 |
|    clip_fraction        | 0.453      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.43       |
|    n_updates            | 8000       |
|    policy_gradient_loss | -0.213     |
|    std                  | 0.355      |
|    value_loss           | 164        |
----------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.00192   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0828    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0148     |
| rollout/                |            |
|    ep_len_mean          | 45.5       |
|    ep_rew_mean          | -159       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 402        |
|    time_elapsed         | 1442       |
|    total_timesteps      | 411648     |
| train/                  |            |
|    approx_kl            | 0.27043265 |
|    clip_fraction        | 0.464      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.24       |
|    n_updates            | 8020       |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.355      |
|    value_loss           | 206        |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.0013    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0754    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0155     |
| rollout/                |            |
|    ep_len_mean          | 36.5       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 403        |
|    time_elapsed         | 1445       |
|    total_timesteps      | 412672     |
| train/                  |            |
|    approx_kl            | 0.42986935 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.21       |
|    n_updates            | 8040       |
|    policy_gradient_loss | -0.193     |
|    std                  | 0.355      |
|    value_loss           | 150        |
----------------------------------------
----------------------------------------
| reward                  | -3.95      |
| reward_contact          | -0.00024   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0825    |
| reward_torque           | -3.78      |
| reward_velocity         | 0.013      |
| rollout/                |            |
|    ep_len_mean          | 17.6       |
|    ep_rew_mean          | -66.5      |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 404        |
|    time_elapsed         | 1449       |
|    total_timesteps      | 413696     |
| train/                  |            |
|    approx_kl            | 0.48211375 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.174      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.85       |
|    n_updates            | 8060       |
|    policy_gradient_loss | -0.22      |
|    std                  | 0.355      |
|    value_loss           | 173        |
----------------------------------------
Num timesteps: 414000
Best mean reward: -63.38 - Last mean reward per episode: -66.49
---------------------------------------
| reward                  | -3.95     |
| reward_contact          | -0.000844 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0616   |
| reward_torque           | -3.8      |
| reward_velocity         | 0.0151    |
| rollout/                |           |
|    ep_len_mean          | 27.4      |
|    ep_rew_mean          | -99.1     |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 405       |
|    time_elapsed         | 1453      |
|    total_timesteps      | 414720    |
| train/                  |           |
|    approx_kl            | 1.3719039 |
|    clip_fraction        | 0.543     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.874     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.464     |
|    n_updates            | 8080      |
|    policy_gradient_loss | -0.22     |
|    std                  | 0.355     |
|    value_loss           | 113       |
---------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.000956  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0577    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0185     |
| rollout/                |            |
|    ep_len_mean          | 36.9       |
|    ep_rew_mean          | -130       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 406        |
|    time_elapsed         | 1456       |
|    total_timesteps      | 415744     |
| train/                  |            |
|    approx_kl            | 0.42250845 |
|    clip_fraction        | 0.534      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.77       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.78       |
|    n_updates            | 8100       |
|    policy_gradient_loss | -0.18      |
|    std                  | 0.355      |
|    value_loss           | 111        |
----------------------------------------
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.000956 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0668   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.0187    |
| rollout/                |           |
|    ep_len_mean          | 37.8      |
|    ep_rew_mean          | -134      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 407       |
|    time_elapsed         | 1460      |
|    total_timesteps      | 416768    |
| train/                  |           |
|    approx_kl            | 0.4643223 |
|    clip_fraction        | 0.534     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.864     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.94      |
|    n_updates            | 8120      |
|    policy_gradient_loss | -0.19     |
|    std                  | 0.355     |
|    value_loss           | 104       |
---------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.0012    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0604    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0171     |
| rollout/                |            |
|    ep_len_mean          | 47.9       |
|    ep_rew_mean          | -167       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 408        |
|    time_elapsed         | 1464       |
|    total_timesteps      | 417792     |
| train/                  |            |
|    approx_kl            | 0.54839766 |
|    clip_fraction        | 0.566      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.877      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.83       |
|    n_updates            | 8140       |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.355      |
|    value_loss           | 83.5       |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00202   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0615    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0165     |
| rollout/                |            |
|    ep_len_mean          | 56.7       |
|    ep_rew_mean          | -195       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 409        |
|    time_elapsed         | 1467       |
|    total_timesteps      | 418816     |
| train/                  |            |
|    approx_kl            | 0.43524492 |
|    clip_fraction        | 0.523      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.75       |
|    n_updates            | 8160       |
|    policy_gradient_loss | -0.182     |
|    std                  | 0.354      |
|    value_loss           | 112        |
----------------------------------------
---------------------------------------
| reward                  | -3.95     |
| reward_contact          | -0.00202  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0543   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.0189    |
| rollout/                |           |
|    ep_len_mean          | 67.3      |
|    ep_rew_mean          | -231      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 410       |
|    time_elapsed         | 1471      |
|    total_timesteps      | 419840    |
| train/                  |           |
|    approx_kl            | 0.3953567 |
|    clip_fraction        | 0.601     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.844     |
|    learning_rate        | 0.0003    |
|    loss                 | 12.7      |
|    n_updates            | 8180      |
|    policy_gradient_loss | -0.182    |
|    std                  | 0.354     |
|    value_loss           | 262       |
---------------------------------------
Num timesteps: 420000
Best mean reward: -63.38 - Last mean reward per episode: -228.74
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00189   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0586    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.016      |
| rollout/                |            |
|    ep_len_mean          | 56.8       |
|    ep_rew_mean          | -195       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 411        |
|    time_elapsed         | 1475       |
|    total_timesteps      | 420864     |
| train/                  |            |
|    approx_kl            | 0.42520824 |
|    clip_fraction        | 0.548      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.57       |
|    n_updates            | 8200       |
|    policy_gradient_loss | -0.181     |
|    std                  | 0.354      |
|    value_loss           | 83.6       |
----------------------------------------
--------------------------------------
| reward                  | -3.99    |
| reward_contact          | -0.00244 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0791  |
| reward_torque           | -3.82    |
| reward_velocity         | 0.0143   |
| rollout/                |          |
|    ep_len_mean          | 56.7     |
|    ep_rew_mean          | -196     |
| time/                   |          |
|    fps                  | 285      |
|    iterations           | 412      |
|    time_elapsed         | 1478     |
|    total_timesteps      | 421888   |
| train/                  |          |
|    approx_kl            | 0.950876 |
|    clip_fraction        | 0.617    |
|    clip_range           | 0.4      |
|    entropy_loss         | -21.8    |
|    explained_variance   | 0.915    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.12     |
|    n_updates            | 8220     |
|    policy_gradient_loss | -0.19    |
|    std                  | 0.354    |
|    value_loss           | 64.8     |
--------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00257   |
| reward_ctrl             | -0.0934    |
| reward_motion           | -0.0906    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0166     |
| rollout/                |            |
|    ep_len_mean          | 44.4       |
|    ep_rew_mean          | -155       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 413        |
|    time_elapsed         | 1482       |
|    total_timesteps      | 422912     |
| train/                  |            |
|    approx_kl            | 0.40403545 |
|    clip_fraction        | 0.581      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.864      |
|    learning_rate        | 0.0003     |
|    loss                 | 33.5       |
|    n_updates            | 8240       |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.354      |
|    value_loss           | 133        |
----------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00211   |
| reward_ctrl             | -0.0934    |
| reward_motion           | -0.0902    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0147     |
| rollout/                |            |
|    ep_len_mean          | 35.4       |
|    ep_rew_mean          | -125       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 414        |
|    time_elapsed         | 1486       |
|    total_timesteps      | 423936     |
| train/                  |            |
|    approx_kl            | 0.37594998 |
|    clip_fraction        | 0.464      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.88       |
|    n_updates            | 8260       |
|    policy_gradient_loss | -0.194     |
|    std                  | 0.354      |
|    value_loss           | 201        |
----------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00238   |
| reward_ctrl             | -0.0934    |
| reward_motion           | -0.0858    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0175     |
| rollout/                |            |
|    ep_len_mean          | 45.6       |
|    ep_rew_mean          | -159       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 415        |
|    time_elapsed         | 1490       |
|    total_timesteps      | 424960     |
| train/                  |            |
|    approx_kl            | 0.71219885 |
|    clip_fraction        | 0.567      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.45       |
|    n_updates            | 8280       |
|    policy_gradient_loss | -0.205     |
|    std                  | 0.354      |
|    value_loss           | 121        |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00203   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0842    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0179     |
| rollout/                |            |
|    ep_len_mean          | 36.5       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 416        |
|    time_elapsed         | 1493       |
|    total_timesteps      | 425984     |
| train/                  |            |
|    approx_kl            | 0.47726566 |
|    clip_fraction        | 0.507      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.56       |
|    n_updates            | 8300       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.354      |
|    value_loss           | 88.4       |
----------------------------------------
Num timesteps: 426000
Best mean reward: -63.38 - Last mean reward per episode: -129.87
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00179   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0842    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0184     |
| rollout/                |            |
|    ep_len_mean          | 36.4       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 417        |
|    time_elapsed         | 1497       |
|    total_timesteps      | 427008     |
| train/                  |            |
|    approx_kl            | 0.40723807 |
|    clip_fraction        | 0.606      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.87       |
|    learning_rate        | 0.0003     |
|    loss                 | 15.2       |
|    n_updates            | 8320       |
|    policy_gradient_loss | -0.208     |
|    std                  | 0.354      |
|    value_loss           | 313        |
----------------------------------------
---------------------------------------
| reward                  | -3.95     |
| reward_contact          | -0.00159  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0739   |
| reward_torque           | -3.79     |
| reward_velocity         | 0.0189    |
| rollout/                |           |
|    ep_len_mean          | 45.6      |
|    ep_rew_mean          | -158      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 418       |
|    time_elapsed         | 1501      |
|    total_timesteps      | 428032    |
| train/                  |           |
|    approx_kl            | 0.7864481 |
|    clip_fraction        | 0.56      |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.916     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.2       |
|    n_updates            | 8340      |
|    policy_gradient_loss | -0.162    |
|    std                  | 0.354     |
|    value_loss           | 67.6      |
---------------------------------------
----------------------------------------
| reward                  | -3.93      |
| reward_contact          | -0.00183   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0647    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.021      |
| rollout/                |            |
|    ep_len_mean          | 35.2       |
|    ep_rew_mean          | -124       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 419        |
|    time_elapsed         | 1505       |
|    total_timesteps      | 429056     |
| train/                  |            |
|    approx_kl            | 0.27581483 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.791      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.02       |
|    n_updates            | 8360       |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.354      |
|    value_loss           | 297        |
----------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.000952  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0827    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0134     |
| rollout/                |            |
|    ep_len_mean          | 25         |
|    ep_rew_mean          | -90.4      |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 420        |
|    time_elapsed         | 1508       |
|    total_timesteps      | 430080     |
| train/                  |            |
|    approx_kl            | 0.20145088 |
|    clip_fraction        | 0.394      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.877      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.6       |
|    n_updates            | 8380       |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.354      |
|    value_loss           | 138        |
----------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.00072  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0685   |
| reward_torque           | -3.87     |
| reward_velocity         | 0.0142    |
| rollout/                |           |
|    ep_len_mean          | 34.5      |
|    ep_rew_mean          | -122      |
| time/                   |           |
|    fps                  | 285       |
|    iterations           | 421       |
|    time_elapsed         | 1512      |
|    total_timesteps      | 431104    |
| train/                  |           |
|    approx_kl            | 0.5858379 |
|    clip_fraction        | 0.528     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.3     |
|    explained_variance   | 0.92      |
|    learning_rate        | 0.0003    |
|    loss                 | 6.54      |
|    n_updates            | 8400      |
|    policy_gradient_loss | -0.202    |
|    std                  | 0.354     |
|    value_loss           | 128       |
---------------------------------------
Num timesteps: 432000
Best mean reward: -63.38 - Last mean reward per episode: -122.33
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.000906  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0765    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.013      |
| rollout/                |            |
|    ep_len_mean          | 34.5       |
|    ep_rew_mean          | -122       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 422        |
|    time_elapsed         | 1515       |
|    total_timesteps      | 432128     |
| train/                  |            |
|    approx_kl            | 0.26940846 |
|    clip_fraction        | 0.516      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0003     |
|    loss                 | 32.5       |
|    n_updates            | 8420       |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.354      |
|    value_loss           | 289        |
----------------------------------------
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.001     |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0841    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0167     |
| rollout/                |            |
|    ep_len_mean          | 34.8       |
|    ep_rew_mean          | -123       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 423        |
|    time_elapsed         | 1519       |
|    total_timesteps      | 433152     |
| train/                  |            |
|    approx_kl            | 0.26015857 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.844      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.26       |
|    n_updates            | 8440       |
|    policy_gradient_loss | -0.202     |
|    std                  | 0.354      |
|    value_loss           | 128        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00211   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0809    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0182     |
| rollout/                |            |
|    ep_len_mean          | 45.5       |
|    ep_rew_mean          | -159       |
| time/                   |            |
|    fps                  | 285        |
|    iterations           | 424        |
|    time_elapsed         | 1523       |
|    total_timesteps      | 434176     |
| train/                  |            |
|    approx_kl            | 0.27896038 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.8       |
|    n_updates            | 8460       |
|    policy_gradient_loss | -0.161     |
|    std                  | 0.354      |
|    value_loss           | 143        |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00187   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0852    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0159     |
| rollout/                |            |
|    ep_len_mean          | 35.8       |
|    ep_rew_mean          | -127       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 425        |
|    time_elapsed         | 1527       |
|    total_timesteps      | 435200     |
| train/                  |            |
|    approx_kl            | 0.38560873 |
|    clip_fraction        | 0.495      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.18       |
|    n_updates            | 8480       |
|    policy_gradient_loss | -0.199     |
|    std                  | 0.354      |
|    value_loss           | 218        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00156   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0863    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.00776    |
| rollout/                |            |
|    ep_len_mean          | 26.7       |
|    ep_rew_mean          | -97.1      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 426        |
|    time_elapsed         | 1530       |
|    total_timesteps      | 436224     |
| train/                  |            |
|    approx_kl            | 0.29351357 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.5       |
|    n_updates            | 8500       |
|    policy_gradient_loss | -0.187     |
|    std                  | 0.354      |
|    value_loss           | 246        |
----------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.00174  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.085    |
| reward_torque           | -3.82     |
| reward_velocity         | 0.00803   |
| rollout/                |           |
|    ep_len_mean          | 36.1      |
|    ep_rew_mean          | -128      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 427       |
|    time_elapsed         | 1534      |
|    total_timesteps      | 437248    |
| train/                  |           |
|    approx_kl            | 0.3907338 |
|    clip_fraction        | 0.467     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.486     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.88      |
|    n_updates            | 8520      |
|    policy_gradient_loss | -0.21     |
|    std                  | 0.354     |
|    value_loss           | 215       |
---------------------------------------
Num timesteps: 438000
Best mean reward: -63.38 - Last mean reward per episode: -128.37
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00102   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.085     |
| reward_torque           | -3.8       |
| reward_velocity         | 0.00887    |
| rollout/                |            |
|    ep_len_mean          | 36.3       |
|    ep_rew_mean          | -128       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 428        |
|    time_elapsed         | 1538       |
|    total_timesteps      | 438272     |
| train/                  |            |
|    approx_kl            | 0.39633846 |
|    clip_fraction        | 0.534      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.834      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.25       |
|    n_updates            | 8540       |
|    policy_gradient_loss | -0.189     |
|    std                  | 0.354      |
|    value_loss           | 76.9       |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00121   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0809    |
| reward_torque           | -3.79      |
| reward_velocity         | 0.0112     |
| rollout/                |            |
|    ep_len_mean          | 46.1       |
|    ep_rew_mean          | -161       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 429        |
|    time_elapsed         | 1541       |
|    total_timesteps      | 439296     |
| train/                  |            |
|    approx_kl            | 0.80881566 |
|    clip_fraction        | 0.556      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.88       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.33       |
|    n_updates            | 8560       |
|    policy_gradient_loss | -0.202     |
|    std                  | 0.354      |
|    value_loss           | 149        |
----------------------------------------
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.00116  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.081    |
| reward_torque           | -3.79     |
| reward_velocity         | 0.0129    |
| rollout/                |           |
|    ep_len_mean          | 45.2      |
|    ep_rew_mean          | -157      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 430       |
|    time_elapsed         | 1545      |
|    total_timesteps      | 440320    |
| train/                  |           |
|    approx_kl            | 0.2752297 |
|    clip_fraction        | 0.475     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.818     |
|    learning_rate        | 0.0003    |
|    loss                 | 30.9      |
|    n_updates            | 8580      |
|    policy_gradient_loss | -0.147    |
|    std                  | 0.354     |
|    value_loss           | 175       |
---------------------------------------
---------------------------------------
| reward                  | -3.94     |
| reward_contact          | -0.00146  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0776   |
| reward_torque           | -3.78     |
| reward_velocity         | 0.0152    |
| rollout/                |           |
|    ep_len_mean          | 55.5      |
|    ep_rew_mean          | -191      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 431       |
|    time_elapsed         | 1549      |
|    total_timesteps      | 441344    |
| train/                  |           |
|    approx_kl            | 0.2689147 |
|    clip_fraction        | 0.489     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.777     |
|    learning_rate        | 0.0003    |
|    loss                 | 19.4      |
|    n_updates            | 8600      |
|    policy_gradient_loss | -0.155    |
|    std                  | 0.354     |
|    value_loss           | 181       |
---------------------------------------
---------------------------------------
| reward                  | -3.96     |
| reward_contact          | -0.00108  |
| reward_ctrl             | -0.0951   |
| reward_motion           | -0.0695   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.016     |
| rollout/                |           |
|    ep_len_mean          | 44.6      |
|    ep_rew_mean          | -155      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 432       |
|    time_elapsed         | 1553      |
|    total_timesteps      | 442368    |
| train/                  |           |
|    approx_kl            | 0.3003227 |
|    clip_fraction        | 0.524     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.796     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.69      |
|    n_updates            | 8620      |
|    policy_gradient_loss | -0.201    |
|    std                  | 0.354     |
|    value_loss           | 169       |
---------------------------------------
---------------------------------------
| reward                  | -3.94     |
| reward_contact          | -0.00156  |
| reward_ctrl             | -0.0951   |
| reward_motion           | -0.0656   |
| reward_torque           | -3.8      |
| reward_velocity         | 0.0192    |
| rollout/                |           |
|    ep_len_mean          | 55.1      |
|    ep_rew_mean          | -190      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 433       |
|    time_elapsed         | 1557      |
|    total_timesteps      | 443392    |
| train/                  |           |
|    approx_kl            | 1.2538021 |
|    clip_fraction        | 0.655     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.912     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.2       |
|    n_updates            | 8640      |
|    policy_gradient_loss | -0.194    |
|    std                  | 0.354     |
|    value_loss           | 86.7      |
---------------------------------------
Num timesteps: 444000
Best mean reward: -63.38 - Last mean reward per episode: -222.11
---------------------------------------
| reward                  | -3.94     |
| reward_contact          | -0.00156  |
| reward_ctrl             | -0.0951   |
| reward_motion           | -0.0616   |
| reward_torque           | -3.8      |
| reward_velocity         | 0.0217    |
| rollout/                |           |
|    ep_len_mean          | 64.9      |
|    ep_rew_mean          | -222      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 434       |
|    time_elapsed         | 1560      |
|    total_timesteps      | 444416    |
| train/                  |           |
|    approx_kl            | 1.5972466 |
|    clip_fraction        | 0.636     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.884     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.61      |
|    n_updates            | 8660      |
|    policy_gradient_loss | -0.161    |
|    std                  | 0.354     |
|    value_loss           | 106       |
---------------------------------------
----------------------------------------
| reward                  | -3.94      |
| reward_contact          | -0.00146   |
| reward_ctrl             | -0.0941    |
| reward_motion           | -0.064     |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0277     |
| rollout/                |            |
|    ep_len_mean          | 65.5       |
|    ep_rew_mean          | -225       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 435        |
|    time_elapsed         | 1564       |
|    total_timesteps      | 445440     |
| train/                  |            |
|    approx_kl            | 0.57625556 |
|    clip_fraction        | 0.678      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.12       |
|    n_updates            | 8680       |
|    policy_gradient_loss | -0.121     |
|    std                  | 0.354      |
|    value_loss           | 97.8       |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00129   |
| reward_ctrl             | -0.0941    |
| reward_motion           | -0.0703    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0244     |
| rollout/                |            |
|    ep_len_mean          | 44.9       |
|    ep_rew_mean          | -157       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 436        |
|    time_elapsed         | 1568       |
|    total_timesteps      | 446464     |
| train/                  |            |
|    approx_kl            | 0.23225716 |
|    clip_fraction        | 0.417      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | 19.9       |
|    n_updates            | 8700       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.354      |
|    value_loss           | 249        |
----------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00115   |
| reward_ctrl             | -0.099     |
| reward_motion           | -0.0783    |
| reward_torque           | -3.87      |
| reward_velocity         | 0.0258     |
| rollout/                |            |
|    ep_len_mean          | 54.4       |
|    ep_rew_mean          | -188       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 437        |
|    time_elapsed         | 1572       |
|    total_timesteps      | 447488     |
| train/                  |            |
|    approx_kl            | 0.88879985 |
|    clip_fraction        | 0.576      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.26       |
|    n_updates            | 8720       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.354      |
|    value_loss           | 84.9       |
----------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.000911  |
| reward_ctrl             | -0.099     |
| reward_motion           | -0.0921    |
| reward_torque           | -3.87      |
| reward_velocity         | 0.0215     |
| rollout/                |            |
|    ep_len_mean          | 52         |
|    ep_rew_mean          | -180       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 438        |
|    time_elapsed         | 1575       |
|    total_timesteps      | 448512     |
| train/                  |            |
|    approx_kl            | 0.29066724 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.28       |
|    n_updates            | 8740       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.354      |
|    value_loss           | 174        |
----------------------------------------
---------------------------------------
| reward                  | -4.03     |
| reward_contact          | -0.000911 |
| reward_ctrl             | -0.099    |
| reward_motion           | -0.0829   |
| reward_torque           | -3.87     |
| reward_velocity         | 0.0218    |
| rollout/                |           |
|    ep_len_mean          | 62.1      |
|    ep_rew_mean          | -214      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 439       |
|    time_elapsed         | 1579      |
|    total_timesteps      | 449536    |
| train/                  |           |
|    approx_kl            | 0.4085124 |
|    clip_fraction        | 0.474     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.847     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.05      |
|    n_updates            | 8760      |
|    policy_gradient_loss | -0.174    |
|    std                  | 0.354     |
|    value_loss           | 209       |
---------------------------------------
Num timesteps: 450000
Best mean reward: -63.38 - Last mean reward per episode: -248.53
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.000708 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0731   |
| reward_torque           | -3.85     |
| reward_velocity         | 0.0103    |
| rollout/                |           |
|    ep_len_mean          | 53.2      |
|    ep_rew_mean          | -185      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 440       |
|    time_elapsed         | 1583      |
|    total_timesteps      | 450560    |
| train/                  |           |
|    approx_kl            | 0.3786428 |
|    clip_fraction        | 0.471     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.596     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.7       |
|    n_updates            | 8780      |
|    policy_gradient_loss | -0.13     |
|    std                  | 0.354     |
|    value_loss           | 121       |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.000708  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0731    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0103     |
| rollout/                |            |
|    ep_len_mean          | 63.2       |
|    ep_rew_mean          | -219       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 441        |
|    time_elapsed         | 1586       |
|    total_timesteps      | 451584     |
| train/                  |            |
|    approx_kl            | 0.22186717 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.799      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.4       |
|    n_updates            | 8800       |
|    policy_gradient_loss | -0.186     |
|    std                  | 0.354      |
|    value_loss           | 355        |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.000764  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0652    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.00836    |
| rollout/                |            |
|    ep_len_mean          | 47.1       |
|    ep_rew_mean          | -166       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 442        |
|    time_elapsed         | 1590       |
|    total_timesteps      | 452608     |
| train/                  |            |
|    approx_kl            | 0.27236608 |
|    clip_fraction        | 0.481      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.541      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.9       |
|    n_updates            | 8820       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.354      |
|    value_loss           | 162        |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.000524  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0652    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.00844    |
| rollout/                |            |
|    ep_len_mean          | 47.5       |
|    ep_rew_mean          | -168       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 443        |
|    time_elapsed         | 1594       |
|    total_timesteps      | 453632     |
| train/                  |            |
|    approx_kl            | 0.33655977 |
|    clip_fraction        | 0.427      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | -0.159     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.93       |
|    n_updates            | 8840       |
|    policy_gradient_loss | -0.207     |
|    std                  | 0.354      |
|    value_loss           | 279        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.000401  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0802    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.00789    |
| rollout/                |            |
|    ep_len_mean          | 36.9       |
|    ep_rew_mean          | -132       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 444        |
|    time_elapsed         | 1597       |
|    total_timesteps      | 454656     |
| train/                  |            |
|    approx_kl            | 0.56706333 |
|    clip_fraction        | 0.552      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.86       |
|    n_updates            | 8860       |
|    policy_gradient_loss | -0.184     |
|    std                  | 0.354      |
|    value_loss           | 65.1       |
----------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.00048   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0961    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0101     |
| rollout/                |            |
|    ep_len_mean          | 37.4       |
|    ep_rew_mean          | -133       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 445        |
|    time_elapsed         | 1601       |
|    total_timesteps      | 455680     |
| train/                  |            |
|    approx_kl            | 0.26140085 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.0003     |
|    loss                 | 9.39       |
|    n_updates            | 8880       |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.354      |
|    value_loss           | 193        |
----------------------------------------
Num timesteps: 456000
Best mean reward: -63.38 - Last mean reward per episode: -133.47
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.000846 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.082    |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0108    |
| rollout/                |           |
|    ep_len_mean          | 47.1      |
|    ep_rew_mean          | -165      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 446       |
|    time_elapsed         | 1605      |
|    total_timesteps      | 456704    |
| train/                  |           |
|    approx_kl            | 0.7970977 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.711     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.13      |
|    n_updates            | 8900      |
|    policy_gradient_loss | -0.232    |
|    std                  | 0.354     |
|    value_loss           | 177       |
---------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.000982  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0742    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0115     |
| rollout/                |            |
|    ep_len_mean          | 57.1       |
|    ep_rew_mean          | -199       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 447        |
|    time_elapsed         | 1608       |
|    total_timesteps      | 457728     |
| train/                  |            |
|    approx_kl            | 0.35103822 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.859      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.15       |
|    n_updates            | 8920       |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.354      |
|    value_loss           | 145        |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00111   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0705    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0135     |
| rollout/                |            |
|    ep_len_mean          | 66.8       |
|    ep_rew_mean          | -231       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 448        |
|    time_elapsed         | 1612       |
|    total_timesteps      | 458752     |
| train/                  |            |
|    approx_kl            | 0.49459758 |
|    clip_fraction        | 0.546      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.794      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.02       |
|    n_updates            | 8940       |
|    policy_gradient_loss | -0.142     |
|    std                  | 0.354      |
|    value_loss           | 74.8       |
----------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.00111   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0705    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0135     |
| rollout/                |            |
|    ep_len_mean          | 76.5       |
|    ep_rew_mean          | -263       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 449        |
|    time_elapsed         | 1616       |
|    total_timesteps      | 459776     |
| train/                  |            |
|    approx_kl            | 0.45320842 |
|    clip_fraction        | 0.565      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.41       |
|    learning_rate        | 0.0003     |
|    loss                 | 8.3        |
|    n_updates            | 8960       |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.354      |
|    value_loss           | 133        |
----------------------------------------
----------------------------------------
| reward                  | -3.96      |
| reward_contact          | -0.00121   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0652    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0147     |
| rollout/                |            |
|    ep_len_mean          | 86.7       |
|    ep_rew_mean          | -297       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 450        |
|    time_elapsed         | 1619       |
|    total_timesteps      | 460800     |
| train/                  |            |
|    approx_kl            | 0.38039494 |
|    clip_fraction        | 0.545      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.655      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.66       |
|    n_updates            | 8980       |
|    policy_gradient_loss | -0.14      |
|    std                  | 0.354      |
|    value_loss           | 111        |
----------------------------------------
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.00173  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0645   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.0168    |
| rollout/                |           |
|    ep_len_mean          | 67.2      |
|    ep_rew_mean          | -231      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 451       |
|    time_elapsed         | 1623      |
|    total_timesteps      | 461824    |
| train/                  |           |
|    approx_kl            | 0.7007009 |
|    clip_fraction        | 0.598     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.396     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.33      |
|    n_updates            | 9000      |
|    policy_gradient_loss | -0.115    |
|    std                  | 0.354     |
|    value_loss           | 131       |
---------------------------------------
Num timesteps: 462000
Best mean reward: -63.38 - Last mean reward per episode: -233.30
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00217   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0645    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0168     |
| rollout/                |            |
|    ep_len_mean          | 67.7       |
|    ep_rew_mean          | -233       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 452        |
|    time_elapsed         | 1627       |
|    total_timesteps      | 462848     |
| train/                  |            |
|    approx_kl            | 0.25675064 |
|    clip_fraction        | 0.407      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.299      |
|    learning_rate        | 0.0003     |
|    loss                 | 21.6       |
|    n_updates            | 9020       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.354      |
|    value_loss           | 661        |
----------------------------------------
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00233   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0592    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0176     |
| rollout/                |            |
|    ep_len_mean          | 77.5       |
|    ep_rew_mean          | -267       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 453        |
|    time_elapsed         | 1630       |
|    total_timesteps      | 463872     |
| train/                  |            |
|    approx_kl            | 0.29339504 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.682      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.9        |
|    n_updates            | 9040       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.354      |
|    value_loss           | 189        |
----------------------------------------
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.00242  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0693   |
| reward_torque           | -3.83     |
| reward_velocity         | 0.0184    |
| rollout/                |           |
|    ep_len_mean          | 78        |
|    ep_rew_mean          | -269      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 454       |
|    time_elapsed         | 1634      |
|    total_timesteps      | 464896    |
| train/                  |           |
|    approx_kl            | 0.5218841 |
|    clip_fraction        | 0.53      |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.889     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.34      |
|    n_updates            | 9060      |
|    policy_gradient_loss | -0.169    |
|    std                  | 0.354     |
|    value_loss           | 75.1      |
---------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.00194   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0892    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 46.9       |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 455        |
|    time_elapsed         | 1638       |
|    total_timesteps      | 465920     |
| train/                  |            |
|    approx_kl            | 0.39825416 |
|    clip_fraction        | 0.562      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.746      |
|    learning_rate        | 0.0003     |
|    loss                 | 72.2       |
|    n_updates            | 9080       |
|    policy_gradient_loss | -0.188     |
|    std                  | 0.354      |
|    value_loss           | 263        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.0017    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0892    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0161     |
| rollout/                |            |
|    ep_len_mean          | 55         |
|    ep_rew_mean          | -191       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 456        |
|    time_elapsed         | 1641       |
|    total_timesteps      | 466944     |
| train/                  |            |
|    approx_kl            | 0.29533693 |
|    clip_fraction        | 0.532      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.846      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.55       |
|    n_updates            | 9100       |
|    policy_gradient_loss | -0.185     |
|    std                  | 0.354      |
|    value_loss           | 167        |
----------------------------------------
---------------------------------------
| reward                  | -3.98     |
| reward_contact          | -0.0015   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0892   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.0158    |
| rollout/                |           |
|    ep_len_mean          | 54.7      |
|    ep_rew_mean          | -190      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 457       |
|    time_elapsed         | 1645      |
|    total_timesteps      | 467968    |
| train/                  |           |
|    approx_kl            | 0.9043795 |
|    clip_fraction        | 0.629     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.889     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.84      |
|    n_updates            | 9120      |
|    policy_gradient_loss | -0.194    |
|    std                  | 0.353     |
|    value_loss           | 150       |
---------------------------------------
Num timesteps: 468000
Best mean reward: -63.38 - Last mean reward per episode: -189.98
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.00129  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0948   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.00936   |
| rollout/                |           |
|    ep_len_mean          | 34.8      |
|    ep_rew_mean          | -124      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 458       |
|    time_elapsed         | 1649      |
|    total_timesteps      | 468992    |
| train/                  |           |
|    approx_kl            | 1.3862952 |
|    clip_fraction        | 0.676     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.929     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.684     |
|    n_updates            | 9140      |
|    policy_gradient_loss | -0.165    |
|    std                  | 0.353     |
|    value_loss           | 45.7      |
---------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.00107   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0948    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.00804    |
| rollout/                |            |
|    ep_len_mean          | 25.7       |
|    ep_rew_mean          | -93.4      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 459        |
|    time_elapsed         | 1652       |
|    total_timesteps      | 470016     |
| train/                  |            |
|    approx_kl            | 0.26756197 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.93       |
|    n_updates            | 9160       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.353      |
|    value_loss           | 382        |
----------------------------------------
--------------------------------------
| reward                  | -4.02    |
| reward_contact          | -0.00107 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.0948  |
| reward_torque           | -3.83    |
| reward_velocity         | 0.0105   |
| rollout/                |          |
|    ep_len_mean          | 37.1     |
|    ep_rew_mean          | -132     |
| time/                   |          |
|    fps                  | 284      |
|    iterations           | 460      |
|    time_elapsed         | 1656     |
|    total_timesteps      | 471040   |
| train/                  |          |
|    approx_kl            | 0.722155 |
|    clip_fraction        | 0.561    |
|    clip_range           | 0.4      |
|    entropy_loss         | -21.8    |
|    explained_variance   | 0.918    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.44     |
|    n_updates            | 9180     |
|    policy_gradient_loss | -0.179   |
|    std                  | 0.353    |
|    value_loss           | 65.6     |
--------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.00154   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0912    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0108     |
| rollout/                |            |
|    ep_len_mean          | 38.1       |
|    ep_rew_mean          | -136       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 461        |
|    time_elapsed         | 1659       |
|    total_timesteps      | 472064     |
| train/                  |            |
|    approx_kl            | 0.45894516 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.47       |
|    n_updates            | 9200       |
|    policy_gradient_loss | -0.148     |
|    std                  | 0.353      |
|    value_loss           | 170        |
----------------------------------------
---------------------------------------
| reward                  | -4.07     |
| reward_contact          | -0.0013   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0912   |
| reward_torque           | -3.88     |
| reward_velocity         | 0.00987   |
| rollout/                |           |
|    ep_len_mean          | 37.6      |
|    ep_rew_mean          | -134      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 462       |
|    time_elapsed         | 1663      |
|    total_timesteps      | 473088    |
| train/                  |           |
|    approx_kl            | 0.6468083 |
|    clip_fraction        | 0.559     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.854     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.39      |
|    n_updates            | 9220      |
|    policy_gradient_loss | -0.208    |
|    std                  | 0.353     |
|    value_loss           | 257       |
---------------------------------------
Num timesteps: 474000
Best mean reward: -63.38 - Last mean reward per episode: -181.78
----------------------------------------
| reward                  | -4.05      |
| reward_contact          | -0.00146   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0838    |
| reward_torque           | -3.88      |
| reward_velocity         | 0.0122     |
| rollout/                |            |
|    ep_len_mean          | 51.4       |
|    ep_rew_mean          | -182       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 463        |
|    time_elapsed         | 1667       |
|    total_timesteps      | 474112     |
| train/                  |            |
|    approx_kl            | 0.54084027 |
|    clip_fraction        | 0.517      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.867      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.23       |
|    n_updates            | 9240       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.353      |
|    value_loss           | 76         |
----------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.00126   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0838    |
| reward_torque           | -3.87      |
| reward_velocity         | 0.0113     |
| rollout/                |            |
|    ep_len_mean          | 61.3       |
|    ep_rew_mean          | -215       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 464        |
|    time_elapsed         | 1671       |
|    total_timesteps      | 475136     |
| train/                  |            |
|    approx_kl            | 0.42845312 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.25       |
|    n_updates            | 9260       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.353      |
|    value_loss           | 238        |
----------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.00116   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0926    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0115     |
| rollout/                |            |
|    ep_len_mean          | 39.3       |
|    ep_rew_mean          | -141       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 465        |
|    time_elapsed         | 1674       |
|    total_timesteps      | 476160     |
| train/                  |            |
|    approx_kl            | 0.36577994 |
|    clip_fraction        | 0.513      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.888      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.46       |
|    n_updates            | 9280       |
|    policy_gradient_loss | -0.178     |
|    std                  | 0.353      |
|    value_loss           | 188        |
----------------------------------------
---------------------------------------
| reward                  | -4.03     |
| reward_contact          | -0.00141  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.085    |
| reward_torque           | -3.85     |
| reward_velocity         | 0.00908   |
| rollout/                |           |
|    ep_len_mean          | 37.4      |
|    ep_rew_mean          | -133      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 466       |
|    time_elapsed         | 1678      |
|    total_timesteps      | 477184    |
| train/                  |           |
|    approx_kl            | 2.5015154 |
|    clip_fraction        | 0.472     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.859     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.78      |
|    n_updates            | 9300      |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.353     |
|    value_loss           | 183       |
---------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.00141  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.085    |
| reward_torque           | -3.85     |
| reward_velocity         | 0.00973   |
| rollout/                |           |
|    ep_len_mean          | 38.8      |
|    ep_rew_mean          | -138      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 467       |
|    time_elapsed         | 1682      |
|    total_timesteps      | 478208    |
| train/                  |           |
|    approx_kl            | 0.5588834 |
|    clip_fraction        | 0.618     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.918     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.56      |
|    n_updates            | 9320      |
|    policy_gradient_loss | -0.171    |
|    std                  | 0.353     |
|    value_loss           | 167       |
---------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.00185   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0742    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0103     |
| rollout/                |            |
|    ep_len_mean          | 48.2       |
|    ep_rew_mean          | -169       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 468        |
|    time_elapsed         | 1685       |
|    total_timesteps      | 479232     |
| train/                  |            |
|    approx_kl            | 0.88191164 |
|    clip_fraction        | 0.572      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.883      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.832      |
|    n_updates            | 9340       |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.353      |
|    value_loss           | 43.5       |
----------------------------------------
Num timesteps: 480000
Best mean reward: -63.38 - Last mean reward per episode: -165.75
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00196   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0644    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0101     |
| rollout/                |            |
|    ep_len_mean          | 46.6       |
|    ep_rew_mean          | -163       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 469        |
|    time_elapsed         | 1689       |
|    total_timesteps      | 480256     |
| train/                  |            |
|    approx_kl            | 0.22767824 |
|    clip_fraction        | 0.506      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.8       |
|    n_updates            | 9360       |
|    policy_gradient_loss | -0.142     |
|    std                  | 0.353      |
|    value_loss           | 219        |
----------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.000701  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.85      |
| reward_velocity         | 0.00627    |
| rollout/                |            |
|    ep_len_mean          | 15.8       |
|    ep_rew_mean          | -60.4      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 470        |
|    time_elapsed         | 1692       |
|    total_timesteps      | 481280     |
| train/                  |            |
|    approx_kl            | 0.29996347 |
|    clip_fraction        | 0.463      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.699      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.37       |
|    n_updates            | 9380       |
|    policy_gradient_loss | -0.202     |
|    std                  | 0.353      |
|    value_loss           | 278        |
----------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.000873  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0904    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.00983    |
| rollout/                |            |
|    ep_len_mean          | 15.1       |
|    ep_rew_mean          | -57.8      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 471        |
|    time_elapsed         | 1696       |
|    total_timesteps      | 482304     |
| train/                  |            |
|    approx_kl            | 0.30862546 |
|    clip_fraction        | 0.394      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23        |
|    explained_variance   | -0.068     |
|    learning_rate        | 0.0003     |
|    loss                 | 3.88       |
|    n_updates            | 9400       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.353      |
|    value_loss           | 241        |
----------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.000873 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0811   |
| reward_torque           | -3.85     |
| reward_velocity         | 0.0121    |
| rollout/                |           |
|    ep_len_mean          | 27.1      |
|    ep_rew_mean          | -98.7     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 472       |
|    time_elapsed         | 1700      |
|    total_timesteps      | 483328    |
| train/                  |           |
|    approx_kl            | 0.6673261 |
|    clip_fraction        | 0.511     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.7     |
|    explained_variance   | 0.867     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.31      |
|    n_updates            | 9420      |
|    policy_gradient_loss | -0.208    |
|    std                  | 0.353     |
|    value_loss           | 132       |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00111   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0811    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0124     |
| rollout/                |            |
|    ep_len_mean          | 27.2       |
|    ep_rew_mean          | -99.1      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 473        |
|    time_elapsed         | 1703       |
|    total_timesteps      | 484352     |
| train/                  |            |
|    approx_kl            | 0.69099355 |
|    clip_fraction        | 0.508      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.878      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.5       |
|    n_updates            | 9440       |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.353      |
|    value_loss           | 127        |
----------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.000643  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0742    |
| reward_torque           | -3.88      |
| reward_velocity         | 0.0165     |
| rollout/                |            |
|    ep_len_mean          | 37.5       |
|    ep_rew_mean          | -134       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 474        |
|    time_elapsed         | 1707       |
|    total_timesteps      | 485376     |
| train/                  |            |
|    approx_kl            | 0.30361605 |
|    clip_fraction        | 0.472      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.36       |
|    n_updates            | 9460       |
|    policy_gradient_loss | -0.186     |
|    std                  | 0.353      |
|    value_loss           | 120        |
----------------------------------------
Num timesteps: 486000
Best mean reward: -63.38 - Last mean reward per episode: -92.43
----------------------------------------
| reward                  | -4.06      |
| reward_contact          | -0.000695  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.87      |
| reward_velocity         | 0.00879    |
| rollout/                |            |
|    ep_len_mean          | 14.1       |
|    ep_rew_mean          | -54.1      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 475        |
|    time_elapsed         | 1711       |
|    total_timesteps      | 486400     |
| train/                  |            |
|    approx_kl            | 0.29325378 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.13       |
|    n_updates            | 9480       |
|    policy_gradient_loss | -0.199     |
|    std                  | 0.353      |
|    value_loss           | 234        |
----------------------------------------
---------------------------------------
| reward                  | -4.05     |
| reward_contact          | -0.000695 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0938   |
| reward_torque           | -3.86     |
| reward_velocity         | 0.0101    |
| rollout/                |           |
|    ep_len_mean          | 24.3      |
|    ep_rew_mean          | -88.2     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 476       |
|    time_elapsed         | 1715      |
|    total_timesteps      | 487424    |
| train/                  |           |
|    approx_kl            | 0.6818845 |
|    clip_fraction        | 0.463     |
|    clip_range           | 0.4       |
|    entropy_loss         | -23.1     |
|    explained_variance   | 0.613     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.6       |
|    n_updates            | 9500      |
|    policy_gradient_loss | -0.208    |
|    std                  | 0.353     |
|    value_loss           | 168       |
---------------------------------------
----------------------------------------
| reward                  | -4.05      |
| reward_contact          | -0.000695  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0938    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0101     |
| rollout/                |            |
|    ep_len_mean          | 34.4       |
|    ep_rew_mean          | -122       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 477        |
|    time_elapsed         | 1718       |
|    total_timesteps      | 488448     |
| train/                  |            |
|    approx_kl            | 0.59273034 |
|    clip_fraction        | 0.544      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.766      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.21       |
|    n_updates            | 9520       |
|    policy_gradient_loss | -0.145     |
|    std                  | 0.353      |
|    value_loss           | 119        |
----------------------------------------
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.00143   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.079     |
| reward_torque           | -3.86      |
| reward_velocity         | 0.00953    |
| rollout/                |            |
|    ep_len_mean          | 36.3       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 478        |
|    time_elapsed         | 1722       |
|    total_timesteps      | 489472     |
| train/                  |            |
|    approx_kl            | 0.31759673 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.673      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.8        |
|    n_updates            | 9540       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.353      |
|    value_loss           | 101        |
----------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.00143  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.072    |
| reward_torque           | -3.86     |
| reward_velocity         | 0.0133    |
| rollout/                |           |
|    ep_len_mean          | 47        |
|    ep_rew_mean          | -166      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 479       |
|    time_elapsed         | 1726      |
|    total_timesteps      | 490496    |
| train/                  |           |
|    approx_kl            | 0.8690135 |
|    clip_fraction        | 0.519     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.6     |
|    explained_variance   | 0.414     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.5       |
|    n_updates            | 9560      |
|    policy_gradient_loss | -0.206    |
|    std                  | 0.353     |
|    value_loss           | 164       |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00143   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0685    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0125     |
| rollout/                |            |
|    ep_len_mean          | 56.7       |
|    ep_rew_mean          | -198       |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 480        |
|    time_elapsed         | 1730       |
|    total_timesteps      | 491520     |
| train/                  |            |
|    approx_kl            | 0.22620931 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.1       |
|    n_updates            | 9580       |
|    policy_gradient_loss | -0.202     |
|    std                  | 0.353      |
|    value_loss           | 202        |
----------------------------------------
Num timesteps: 492000
Best mean reward: -63.38 - Last mean reward per episode: -199.45
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.00119  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0685   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.0134    |
| rollout/                |           |
|    ep_len_mean          | 57        |
|    ep_rew_mean          | -199      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 481       |
|    time_elapsed         | 1733      |
|    total_timesteps      | 492544    |
| train/                  |           |
|    approx_kl            | 0.3660801 |
|    clip_fraction        | 0.515     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.775     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.05      |
|    n_updates            | 9600      |
|    policy_gradient_loss | -0.197    |
|    std                  | 0.353     |
|    value_loss           | 104       |
---------------------------------------
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.00071  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0605   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.0125    |
| rollout/                |           |
|    ep_len_mean          | 46.9      |
|    ep_rew_mean          | -166      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 482       |
|    time_elapsed         | 1737      |
|    total_timesteps      | 493568    |
| train/                  |           |
|    approx_kl            | 1.0251296 |
|    clip_fraction        | 0.624     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.93      |
|    learning_rate        | 0.0003    |
|    loss                 | 5.92      |
|    n_updates            | 9620      |
|    policy_gradient_loss | -0.215    |
|    std                  | 0.353     |
|    value_loss           | 86.5      |
---------------------------------------
---------------------------------------
| reward                  | -3.98     |
| reward_contact          | -0.0002   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0642   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0129    |
| rollout/                |           |
|    ep_len_mean          | 45.2      |
|    ep_rew_mean          | -159      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 483       |
|    time_elapsed         | 1741      |
|    total_timesteps      | 494592    |
| train/                  |           |
|    approx_kl            | 0.7006027 |
|    clip_fraction        | 0.57      |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.86      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.816     |
|    n_updates            | 9640      |
|    policy_gradient_loss | -0.169    |
|    std                  | 0.353     |
|    value_loss           | 104       |
---------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.00057  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0646   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.00807   |
| rollout/                |           |
|    ep_len_mean          | 34.6      |
|    ep_rew_mean          | -123      |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 484       |
|    time_elapsed         | 1744      |
|    total_timesteps      | 495616    |
| train/                  |           |
|    approx_kl            | 0.6338114 |
|    clip_fraction        | 0.619     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.2     |
|    explained_variance   | 0.922     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.98      |
|    n_updates            | 9660      |
|    policy_gradient_loss | -0.204    |
|    std                  | 0.353     |
|    value_loss           | 107       |
---------------------------------------
---------------------------------------
| reward                  | -4.05     |
| reward_contact          | -0.00133  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0739   |
| reward_torque           | -3.88     |
| reward_velocity         | 0.0107    |
| rollout/                |           |
|    ep_len_mean          | 23.9      |
|    ep_rew_mean          | -86.9     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 485       |
|    time_elapsed         | 1748      |
|    total_timesteps      | 496640    |
| train/                  |           |
|    approx_kl            | 0.8359202 |
|    clip_fraction        | 0.609     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.4     |
|    explained_variance   | 0.926     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.73      |
|    n_updates            | 9680      |
|    policy_gradient_loss | -0.205    |
|    std                  | 0.353     |
|    value_loss           | 86.4      |
---------------------------------------
----------------------------------------
| reward                  | -4.06      |
| reward_contact          | -0.00166   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0839    |
| reward_torque           | -3.89      |
| reward_velocity         | 0.0107     |
| rollout/                |            |
|    ep_len_mean          | 26.3       |
|    ep_rew_mean          | -94.6      |
| time/                   |            |
|    fps                  | 284        |
|    iterations           | 486        |
|    time_elapsed         | 1752       |
|    total_timesteps      | 497664     |
| train/                  |            |
|    approx_kl            | 0.42835408 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.04       |
|    n_updates            | 9700       |
|    policy_gradient_loss | -0.193     |
|    std                  | 0.353      |
|    value_loss           | 135        |
----------------------------------------
Num timesteps: 498000
Best mean reward: -63.38 - Last mean reward per episode: -63.05
Saving new best model to rl/out_dir/models/exp74/best_model.zip
---------------------------------------
| reward                  | -4.05     |
| reward_contact          | -0.00164  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0799   |
| reward_torque           | -3.88     |
| reward_velocity         | 0.0106    |
| rollout/                |           |
|    ep_len_mean          | 16.8      |
|    ep_rew_mean          | -63.1     |
| time/                   |           |
|    fps                  | 284       |
|    iterations           | 487       |
|    time_elapsed         | 1755      |
|    total_timesteps      | 498688    |
| train/                  |           |
|    approx_kl            | 0.5428996 |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.7     |
|    explained_variance   | 0.902     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.49      |
|    n_updates            | 9720      |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.353     |
|    value_loss           | 154       |
---------------------------------------
----------------------------------------
| reward                  | -4.06      |
| reward_contact          | -0.00193   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0847    |
| reward_torque           | -3.88      |
| reward_velocity         | 0.0087     |
| rollout/                |            |
|    ep_len_mean          | 28.1       |
|    ep_rew_mean          | -102       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 488        |
|    time_elapsed         | 1759       |
|    total_timesteps      | 499712     |
| train/                  |            |
|    approx_kl            | 0.99617803 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.58       |
|    n_updates            | 9740       |
|    policy_gradient_loss | -0.2       |
|    std                  | 0.352      |
|    value_loss           | 66.3       |
----------------------------------------
----------------------------------------
| reward                  | -4.06      |
| reward_contact          | -0.000909  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.87      |
| reward_velocity         | 0.00672    |
| rollout/                |            |
|    ep_len_mean          | 14         |
|    ep_rew_mean          | -54        |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 489        |
|    time_elapsed         | 1763       |
|    total_timesteps      | 500736     |
| train/                  |            |
|    approx_kl            | 0.29248554 |
|    clip_fraction        | 0.532      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23        |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.73       |
|    n_updates            | 9760       |
|    policy_gradient_loss | -0.208     |
|    std                  | 0.352      |
|    value_loss           | 231        |
----------------------------------------
---------------------------------------
| reward                  | -4.09     |
| reward_contact          | -0.00112  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.09     |
| reward_torque           | -3.91     |
| reward_velocity         | 0.00941   |
| rollout/                |           |
|    ep_len_mean          | 11.9      |
|    ep_rew_mean          | -46.1     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 490       |
|    time_elapsed         | 1766      |
|    total_timesteps      | 501760    |
| train/                  |           |
|    approx_kl            | 0.6285322 |
|    clip_fraction        | 0.508     |
|    clip_range           | 0.4       |
|    entropy_loss         | -23.3     |
|    explained_variance   | 0.0428    |
|    learning_rate        | 0.0003    |
|    loss                 | 1.23      |
|    n_updates            | 9780      |
|    policy_gradient_loss | -0.22     |
|    std                  | 0.352     |
|    value_loss           | 95.9      |
---------------------------------------
----------------------------------------
| reward                  | -4.07      |
| reward_contact          | -0.00155   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0769    |
| reward_torque           | -3.9       |
| reward_velocity         | 0.0114     |
| rollout/                |            |
|    ep_len_mean          | 22.8       |
|    ep_rew_mean          | -82.5      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 491        |
|    time_elapsed         | 1770       |
|    total_timesteps      | 502784     |
| train/                  |            |
|    approx_kl            | 0.60537595 |
|    clip_fraction        | 0.498      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.8      |
|    explained_variance   | 0.873      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.01       |
|    n_updates            | 9800       |
|    policy_gradient_loss | -0.206     |
|    std                  | 0.352      |
|    value_loss           | 103        |
----------------------------------------
----------------------------------------
| reward                  | -4.07      |
| reward_contact          | -0.00131   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0769    |
| reward_torque           | -3.9       |
| reward_velocity         | 0.0118     |
| rollout/                |            |
|    ep_len_mean          | 22.1       |
|    ep_rew_mean          | -80        |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 492        |
|    time_elapsed         | 1774       |
|    total_timesteps      | 503808     |
| train/                  |            |
|    approx_kl            | 0.25271356 |
|    clip_fraction        | 0.497      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.859      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.9       |
|    n_updates            | 9820       |
|    policy_gradient_loss | -0.183     |
|    std                  | 0.352      |
|    value_loss           | 163        |
----------------------------------------
Num timesteps: 504000
Best mean reward: -63.05 - Last mean reward per episode: -80.00
---------------------------------------
| reward                  | -4.05     |
| reward_contact          | -0.00151  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0788   |
| reward_torque           | -3.88     |
| reward_velocity         | 0.012     |
| rollout/                |           |
|    ep_len_mean          | 32.5      |
|    ep_rew_mean          | -115      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 493       |
|    time_elapsed         | 1778      |
|    total_timesteps      | 504832    |
| train/                  |           |
|    approx_kl            | 0.9998336 |
|    clip_fraction        | 0.588     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.1     |
|    explained_variance   | 0.955     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.948     |
|    n_updates            | 9840      |
|    policy_gradient_loss | -0.186    |
|    std                  | 0.352     |
|    value_loss           | 55.1      |
---------------------------------------
---------------------------------------
| reward                  | -4.04     |
| reward_contact          | -0.00173  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.075    |
| reward_torque           | -3.87     |
| reward_velocity         | 0.0133    |
| rollout/                |           |
|    ep_len_mean          | 43.4      |
|    ep_rew_mean          | -152      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 494       |
|    time_elapsed         | 1781      |
|    total_timesteps      | 505856    |
| train/                  |           |
|    approx_kl            | 0.3880131 |
|    clip_fraction        | 0.542     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.3     |
|    explained_variance   | 0.913     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.04      |
|    n_updates            | 9860      |
|    policy_gradient_loss | -0.154    |
|    std                  | 0.352     |
|    value_loss           | 216       |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00149   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0693    |
| reward_torque           | -3.87      |
| reward_velocity         | 0.0181     |
| rollout/                |            |
|    ep_len_mean          | 44         |
|    ep_rew_mean          | -155       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 495        |
|    time_elapsed         | 1785       |
|    total_timesteps      | 506880     |
| train/                  |            |
|    approx_kl            | 0.34061325 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.2       |
|    n_updates            | 9880       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.352      |
|    value_loss           | 264        |
----------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.00024   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0943    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0145     |
| rollout/                |            |
|    ep_len_mean          | 23.9       |
|    ep_rew_mean          | -87        |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 496        |
|    time_elapsed         | 1788       |
|    total_timesteps      | 507904     |
| train/                  |            |
|    approx_kl            | 0.17631595 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.771      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.6       |
|    n_updates            | 9900       |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.352      |
|    value_loss           | 211        |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00044   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0898    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0148     |
| rollout/                |            |
|    ep_len_mean          | 33.9       |
|    ep_rew_mean          | -121       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 497        |
|    time_elapsed         | 1792       |
|    total_timesteps      | 508928     |
| train/                  |            |
|    approx_kl            | 0.42041576 |
|    clip_fraction        | 0.468      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23.3      |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.555      |
|    n_updates            | 9920       |
|    policy_gradient_loss | -0.21      |
|    std                  | 0.352      |
|    value_loss           | 155        |
----------------------------------------
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0868    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0058     |
| rollout/                |            |
|    ep_len_mean          | 22.4       |
|    ep_rew_mean          | -81.2      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 498        |
|    time_elapsed         | 1796       |
|    total_timesteps      | 509952     |
| train/                  |            |
|    approx_kl            | 0.47609308 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.29       |
|    n_updates            | 9940       |
|    policy_gradient_loss | -0.15      |
|    std                  | 0.352      |
|    value_loss           | 148        |
----------------------------------------
Num timesteps: 510000
Best mean reward: -63.05 - Last mean reward per episode: -81.17
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0821    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0086     |
| rollout/                |            |
|    ep_len_mean          | 32.3       |
|    ep_rew_mean          | -114       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 499        |
|    time_elapsed         | 1799       |
|    total_timesteps      | 510976     |
| train/                  |            |
|    approx_kl            | 0.40394205 |
|    clip_fraction        | 0.476      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23.1      |
|    explained_variance   | 0.703      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.33       |
|    n_updates            | 9960       |
|    policy_gradient_loss | -0.209     |
|    std                  | 0.352      |
|    value_loss           | 131        |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.0014    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0755    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0114     |
| rollout/                |            |
|    ep_len_mean          | 22.6       |
|    ep_rew_mean          | -81.4      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 500        |
|    time_elapsed         | 1803       |
|    total_timesteps      | 512000     |
| train/                  |            |
|    approx_kl            | 0.46649462 |
|    clip_fraction        | 0.63       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.8       |
|    n_updates            | 9980       |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.352      |
|    value_loss           | 173        |
----------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.0014    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0755    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0116     |
| rollout/                |            |
|    ep_len_mean          | 22.8       |
|    ep_rew_mean          | -82.6      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 501        |
|    time_elapsed         | 1807       |
|    total_timesteps      | 513024     |
| train/                  |            |
|    approx_kl            | 0.59766114 |
|    clip_fraction        | 0.543      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23.3      |
|    explained_variance   | -0.0232    |
|    learning_rate        | 0.0003     |
|    loss                 | 2.2        |
|    n_updates            | 10000      |
|    policy_gradient_loss | -0.225     |
|    std                  | 0.352      |
|    value_loss           | 81.2       |
----------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.00102  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.098    |
| reward_torque           | -3.82     |
| reward_velocity         | 0.014     |
| rollout/                |           |
|    ep_len_mean          | 24.4      |
|    ep_rew_mean          | -88.7     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 502       |
|    time_elapsed         | 1810      |
|    total_timesteps      | 514048    |
| train/                  |           |
|    approx_kl            | 1.0312245 |
|    clip_fraction        | 0.631     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.922     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.1       |
|    n_updates            | 10020     |
|    policy_gradient_loss | -0.17     |
|    std                  | 0.352     |
|    value_loss           | 38.7      |
---------------------------------------
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.000466 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.1      |
| reward_torque           | -3.8      |
| reward_velocity         | 0.0101    |
| rollout/                |           |
|    ep_len_mean          | 17.5      |
|    ep_rew_mean          | -66.9     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 503       |
|    time_elapsed         | 1814      |
|    total_timesteps      | 515072    |
| train/                  |           |
|    approx_kl            | 0.2969876 |
|    clip_fraction        | 0.451     |
|    clip_range           | 0.4       |
|    entropy_loss         | -23       |
|    explained_variance   | 0.44      |
|    learning_rate        | 0.0003    |
|    loss                 | 5.41      |
|    n_updates            | 10040     |
|    policy_gradient_loss | -0.184    |
|    std                  | 0.352     |
|    value_loss           | 288       |
---------------------------------------
Num timesteps: 516000
Best mean reward: -63.05 - Last mean reward per episode: -66.93
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.000466  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0101     |
| rollout/                |            |
|    ep_len_mean          | 27.7       |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 504        |
|    time_elapsed         | 1818       |
|    total_timesteps      | 516096     |
| train/                  |            |
|    approx_kl            | 0.33233848 |
|    clip_fraction        | 0.434      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23        |
|    explained_variance   | 0.526      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.53       |
|    n_updates            | 10060      |
|    policy_gradient_loss | -0.195     |
|    std                  | 0.352      |
|    value_loss           | 286        |
----------------------------------------
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00102   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0888    |
| reward_torque           | -3.8       |
| reward_velocity         | 0.0108     |
| rollout/                |            |
|    ep_len_mean          | 37.5       |
|    ep_rew_mean          | -134       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 505        |
|    time_elapsed         | 1821       |
|    total_timesteps      | 517120     |
| train/                  |            |
|    approx_kl            | 0.16740772 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.869      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.8       |
|    n_updates            | 10080      |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.352      |
|    value_loss           | 164        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00143   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0888    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.00966    |
| rollout/                |            |
|    ep_len_mean          | 42.3       |
|    ep_rew_mean          | -150       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 506        |
|    time_elapsed         | 1825       |
|    total_timesteps      | 518144     |
| train/                  |            |
|    approx_kl            | 0.31997707 |
|    clip_fraction        | 0.51       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.771      |
|    learning_rate        | 0.0003     |
|    loss                 | 17.4       |
|    n_updates            | 10100      |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.352      |
|    value_loss           | 163        |
----------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.0018   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0888   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0142    |
| rollout/                |           |
|    ep_len_mean          | 44        |
|    ep_rew_mean          | -156      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 507       |
|    time_elapsed         | 1829      |
|    total_timesteps      | 519168    |
| train/                  |           |
|    approx_kl            | 0.6466951 |
|    clip_fraction        | 0.494     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.869     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.31      |
|    n_updates            | 10120     |
|    policy_gradient_loss | -0.19     |
|    std                  | 0.352     |
|    value_loss           | 216       |
---------------------------------------
---------------------------------------
| reward                  | -3.99     |
| reward_contact          | -0.00194  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0854   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0157    |
| rollout/                |           |
|    ep_len_mean          | 53.6      |
|    ep_rew_mean          | -189      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 508       |
|    time_elapsed         | 1832      |
|    total_timesteps      | 520192    |
| train/                  |           |
|    approx_kl            | 0.3536955 |
|    clip_fraction        | 0.505     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.8     |
|    explained_variance   | 0.811     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.28      |
|    n_updates            | 10140     |
|    policy_gradient_loss | -0.214    |
|    std                  | 0.352     |
|    value_loss           | 209       |
---------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00218   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0854    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0158     |
| rollout/                |            |
|    ep_len_mean          | 63.5       |
|    ep_rew_mean          | -222       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 509        |
|    time_elapsed         | 1836       |
|    total_timesteps      | 521216     |
| train/                  |            |
|    approx_kl            | 0.29888153 |
|    clip_fraction        | 0.472      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.08       |
|    n_updates            | 10160      |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.352      |
|    value_loss           | 174        |
----------------------------------------
Num timesteps: 522000
Best mean reward: -63.05 - Last mean reward per episode: -135.32
---------------------------------------
| reward                  | -4.04     |
| reward_contact          | -0.00143  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0966   |
| reward_torque           | -3.85     |
| reward_velocity         | 0.0134    |
| rollout/                |           |
|    ep_len_mean          | 37.7      |
|    ep_rew_mean          | -135      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 510       |
|    time_elapsed         | 1840      |
|    total_timesteps      | 522240    |
| train/                  |           |
|    approx_kl            | 0.3722145 |
|    clip_fraction        | 0.476     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.1     |
|    explained_variance   | 0.611     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.68      |
|    n_updates            | 10180     |
|    policy_gradient_loss | -0.156    |
|    std                  | 0.352     |
|    value_loss           | 122       |
---------------------------------------
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.00143   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0898    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 48.1       |
|    ep_rew_mean          | -171       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 511        |
|    time_elapsed         | 1843       |
|    total_timesteps      | 523264     |
| train/                  |            |
|    approx_kl            | 0.45173562 |
|    clip_fraction        | 0.558      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.7      |
|    explained_variance   | 0.85       |
|    learning_rate        | 0.0003     |
|    loss                 | 3.25       |
|    n_updates            | 10200      |
|    policy_gradient_loss | -0.21      |
|    std                  | 0.352      |
|    value_loss           | 185        |
----------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.0015    |
| reward_ctrl             | -0.0914    |
| reward_motion           | -0.087     |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0179     |
| rollout/                |            |
|    ep_len_mean          | 58.2       |
|    ep_rew_mean          | -204       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 512        |
|    time_elapsed         | 1847       |
|    total_timesteps      | 524288     |
| train/                  |            |
|    approx_kl            | 0.33306018 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.94       |
|    n_updates            | 10220      |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.352      |
|    value_loss           | 138        |
----------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.0014    |
| reward_ctrl             | -0.0914    |
| reward_motion           | -0.0842    |
| reward_torque           | -3.88      |
| reward_velocity         | 0.016      |
| rollout/                |            |
|    ep_len_mean          | 66.2       |
|    ep_rew_mean          | -230       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 513        |
|    time_elapsed         | 1851       |
|    total_timesteps      | 525312     |
| train/                  |            |
|    approx_kl            | 0.23584895 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | 62.3       |
|    n_updates            | 10240      |
|    policy_gradient_loss | -0.15      |
|    std                  | 0.352      |
|    value_loss           | 245        |
----------------------------------------
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.0014    |
| reward_ctrl             | -0.0914    |
| reward_motion           | -0.0842    |
| reward_torque           | -3.87      |
| reward_velocity         | 0.0153     |
| rollout/                |            |
|    ep_len_mean          | 66.3       |
|    ep_rew_mean          | -230       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 514        |
|    time_elapsed         | 1855       |
|    total_timesteps      | 526336     |
| train/                  |            |
|    approx_kl            | 0.36110473 |
|    clip_fraction        | 0.537      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.784      |
|    learning_rate        | 0.0003     |
|    loss                 | 14         |
|    n_updates            | 10260      |
|    policy_gradient_loss | -0.215     |
|    std                  | 0.352      |
|    value_loss           | 241        |
----------------------------------------
---------------------------------------
| reward                  | -4.06     |
| reward_contact          | -0.000569 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0902   |
| reward_torque           | -3.88     |
| reward_velocity         | 0.0109    |
| rollout/                |           |
|    ep_len_mean          | 24.7      |
|    ep_rew_mean          | -89.9     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 515       |
|    time_elapsed         | 1858      |
|    total_timesteps      | 527360    |
| train/                  |           |
|    approx_kl            | 1.1207352 |
|    clip_fraction        | 0.582     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.9     |
|    explained_variance   | 0.837     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.4       |
|    n_updates            | 10280     |
|    policy_gradient_loss | -0.206    |
|    std                  | 0.352     |
|    value_loss           | 56        |
---------------------------------------
Num timesteps: 528000
Best mean reward: -63.05 - Last mean reward per episode: -89.11
---------------------------------------
| reward                  | -4.05     |
| reward_contact          | -0.00024  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.093    |
| reward_torque           | -3.86     |
| reward_velocity         | 0.0107    |
| rollout/                |           |
|    ep_len_mean          | 24.5      |
|    ep_rew_mean          | -89.1     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 516       |
|    time_elapsed         | 1862      |
|    total_timesteps      | 528384    |
| train/                  |           |
|    approx_kl            | 0.5009669 |
|    clip_fraction        | 0.428     |
|    clip_range           | 0.4       |
|    entropy_loss         | -23.1     |
|    explained_variance   | -0.352    |
|    learning_rate        | 0.0003    |
|    loss                 | 7.45      |
|    n_updates            | 10300     |
|    policy_gradient_loss | -0.189    |
|    std                  | 0.352     |
|    value_loss           | 402       |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00024   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.08      |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0118     |
| rollout/                |            |
|    ep_len_mean          | 24.9       |
|    ep_rew_mean          | -90.9      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 517        |
|    time_elapsed         | 1866       |
|    total_timesteps      | 529408     |
| train/                  |            |
|    approx_kl            | 0.77265966 |
|    clip_fraction        | 0.576      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.937      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.38       |
|    n_updates            | 10320      |
|    policy_gradient_loss | -0.197     |
|    std                  | 0.352      |
|    value_loss           | 77.3       |
----------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00135   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0698    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0104     |
| rollout/                |            |
|    ep_len_mean          | 35.6       |
|    ep_rew_mean          | -127       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 518        |
|    time_elapsed         | 1870       |
|    total_timesteps      | 530432     |
| train/                  |            |
|    approx_kl            | 0.32941997 |
|    clip_fraction        | 0.532      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.44       |
|    n_updates            | 10340      |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.351      |
|    value_loss           | 186        |
----------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.00135  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0698   |
| reward_torque           | -3.86     |
| reward_velocity         | 0.0112    |
| rollout/                |           |
|    ep_len_mean          | 35.9      |
|    ep_rew_mean          | -128      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 519       |
|    time_elapsed         | 1873      |
|    total_timesteps      | 531456    |
| train/                  |           |
|    approx_kl            | 0.7109261 |
|    clip_fraction        | 0.531     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.865     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.41      |
|    n_updates            | 10360     |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.351     |
|    value_loss           | 160       |
---------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.0019    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0643    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0108     |
| rollout/                |            |
|    ep_len_mean          | 45.1       |
|    ep_rew_mean          | -158       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 520        |
|    time_elapsed         | 1877       |
|    total_timesteps      | 532480     |
| train/                  |            |
|    approx_kl            | 0.51359904 |
|    clip_fraction        | 0.512      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.47       |
|    n_updates            | 10380      |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.351      |
|    value_loss           | 95.3       |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00235   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0766    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0131     |
| rollout/                |            |
|    ep_len_mean          | 46.5       |
|    ep_rew_mean          | -163       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 521        |
|    time_elapsed         | 1881       |
|    total_timesteps      | 533504     |
| train/                  |            |
|    approx_kl            | 0.34905317 |
|    clip_fraction        | 0.522      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.884      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.52       |
|    n_updates            | 10400      |
|    policy_gradient_loss | -0.179     |
|    std                  | 0.351      |
|    value_loss           | 213        |
----------------------------------------
Num timesteps: 534000
Best mean reward: -63.05 - Last mean reward per episode: -128.79
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.00125  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0867   |
| reward_torque           | -3.83     |
| reward_velocity         | 0.0125    |
| rollout/                |           |
|    ep_len_mean          | 40.3      |
|    ep_rew_mean          | -143      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 522       |
|    time_elapsed         | 1885      |
|    total_timesteps      | 534528    |
| train/                  |           |
|    approx_kl            | 0.3295213 |
|    clip_fraction        | 0.538     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.908     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.14      |
|    n_updates            | 10420     |
|    policy_gradient_loss | -0.195    |
|    std                  | 0.351     |
|    value_loss           | 167       |
---------------------------------------
----------------------------------------
| reward                  | -4.05      |
| reward_contact          | -0.00114   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.85      |
| reward_velocity         | 0.00852    |
| rollout/                |            |
|    ep_len_mean          | 21.2       |
|    ep_rew_mean          | -78.9      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 523        |
|    time_elapsed         | 1889       |
|    total_timesteps      | 535552     |
| train/                  |            |
|    approx_kl            | 0.65951025 |
|    clip_fraction        | 0.577      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.7      |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.91       |
|    n_updates            | 10440      |
|    policy_gradient_loss | -0.205     |
|    std                  | 0.351      |
|    value_loss           | 145        |
----------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.000901  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0855    |
| reward_torque           | -3.87      |
| reward_velocity         | 0.0114     |
| rollout/                |            |
|    ep_len_mean          | 31.8       |
|    ep_rew_mean          | -115       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 524        |
|    time_elapsed         | 1892       |
|    total_timesteps      | 536576     |
| train/                  |            |
|    approx_kl            | 0.92099726 |
|    clip_fraction        | 0.546      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.7      |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.35       |
|    n_updates            | 10460      |
|    policy_gradient_loss | -0.201     |
|    std                  | 0.351      |
|    value_loss           | 150        |
----------------------------------------
----------------------------------------
| reward                  | -4.05      |
| reward_contact          | -0.000682  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0855    |
| reward_torque           | -3.87      |
| reward_velocity         | 0.0102     |
| rollout/                |            |
|    ep_len_mean          | 33         |
|    ep_rew_mean          | -119       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 525        |
|    time_elapsed         | 1896       |
|    total_timesteps      | 537600     |
| train/                  |            |
|    approx_kl            | 0.41897923 |
|    clip_fraction        | 0.531      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.77       |
|    n_updates            | 10480      |
|    policy_gradient_loss | -0.183     |
|    std                  | 0.351      |
|    value_loss           | 166        |
----------------------------------------
---------------------------------------
| reward                  | -4.03     |
| reward_contact          | -0.000802 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0824   |
| reward_torque           | -3.86     |
| reward_velocity         | 0.0109    |
| rollout/                |           |
|    ep_len_mean          | 37.8      |
|    ep_rew_mean          | -135      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 526       |
|    time_elapsed         | 1900      |
|    total_timesteps      | 538624    |
| train/                  |           |
|    approx_kl            | 0.9505757 |
|    clip_fraction        | 0.592     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.3     |
|    explained_variance   | 0.924     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.04      |
|    n_updates            | 10500     |
|    policy_gradient_loss | -0.195    |
|    std                  | 0.351     |
|    value_loss           | 82        |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.000667  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0787    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0125     |
| rollout/                |            |
|    ep_len_mean          | 47.9       |
|    ep_rew_mean          | -169       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 527        |
|    time_elapsed         | 1903       |
|    total_timesteps      | 539648     |
| train/                  |            |
|    approx_kl            | 0.56767243 |
|    clip_fraction        | 0.537      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.89       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.1        |
|    n_updates            | 10520      |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.351      |
|    value_loss           | 216        |
----------------------------------------
Num timesteps: 540000
Best mean reward: -63.05 - Last mean reward per episode: -130.31
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00137   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.083     |
| reward_torque           | -3.81      |
| reward_velocity         | 0.00979    |
| rollout/                |            |
|    ep_len_mean          | 34.8       |
|    ep_rew_mean          | -124       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 528        |
|    time_elapsed         | 1907       |
|    total_timesteps      | 540672     |
| train/                  |            |
|    approx_kl            | 0.28936315 |
|    clip_fraction        | 0.411      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.5      |
|    explained_variance   | 0.87       |
|    learning_rate        | 0.0003     |
|    loss                 | 5.5        |
|    n_updates            | 10540      |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.351      |
|    value_loss           | 174        |
----------------------------------------
---------------------------------------
| reward                  | -3.98     |
| reward_contact          | -0.00137  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0804   |
| reward_torque           | -3.81     |
| reward_velocity         | 0.0126    |
| rollout/                |           |
|    ep_len_mean          | 44.4      |
|    ep_rew_mean          | -156      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 529       |
|    time_elapsed         | 1911      |
|    total_timesteps      | 541696    |
| train/                  |           |
|    approx_kl            | 0.6286762 |
|    clip_fraction        | 0.603     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.8     |
|    explained_variance   | 0.925     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.14      |
|    n_updates            | 10560     |
|    policy_gradient_loss | -0.215    |
|    std                  | 0.351     |
|    value_loss           | 140       |
---------------------------------------
----------------------------------------
| reward                  | -3.98      |
| reward_contact          | -0.00138   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0782    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0147     |
| rollout/                |            |
|    ep_len_mean          | 44.3       |
|    ep_rew_mean          | -155       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 530        |
|    time_elapsed         | 1914       |
|    total_timesteps      | 542720     |
| train/                  |            |
|    approx_kl            | 0.35119474 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | 48         |
|    n_updates            | 10580      |
|    policy_gradient_loss | -0.153     |
|    std                  | 0.351      |
|    value_loss           | 285        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00128   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0818    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0148     |
| rollout/                |            |
|    ep_len_mean          | 34.8       |
|    ep_rew_mean          | -124       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 531        |
|    time_elapsed         | 1918       |
|    total_timesteps      | 543744     |
| train/                  |            |
|    approx_kl            | 0.84457564 |
|    clip_fraction        | 0.587      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.897      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.8       |
|    n_updates            | 10600      |
|    policy_gradient_loss | -0.19      |
|    std                  | 0.351      |
|    value_loss           | 174        |
----------------------------------------
--------------------------------------
| reward                  | -3.99    |
| reward_contact          | -0.00149 |
| reward_ctrl             | -0.1     |
| reward_motion           | -0.09    |
| reward_torque           | -3.81    |
| reward_velocity         | 0.0162   |
| rollout/                |          |
|    ep_len_mean          | 45.3     |
|    ep_rew_mean          | -159     |
| time/                   |          |
|    fps                  | 283      |
|    iterations           | 532      |
|    time_elapsed         | 1922     |
|    total_timesteps      | 544768   |
| train/                  |          |
|    approx_kl            | 0.574094 |
|    clip_fraction        | 0.617    |
|    clip_range           | 0.4      |
|    entropy_loss         | -22.4    |
|    explained_variance   | 0.913    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.58     |
|    n_updates            | 10620    |
|    policy_gradient_loss | -0.196   |
|    std                  | 0.351    |
|    value_loss           | 106      |
--------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00158   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0838    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0188     |
| rollout/                |            |
|    ep_len_mean          | 55.8       |
|    ep_rew_mean          | -194       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 533        |
|    time_elapsed         | 1925       |
|    total_timesteps      | 545792     |
| train/                  |            |
|    approx_kl            | 0.62303174 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.6      |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.3        |
|    n_updates            | 10640      |
|    policy_gradient_loss | -0.194     |
|    std                  | 0.351      |
|    value_loss           | 235        |
----------------------------------------
Num timesteps: 546000
Best mean reward: -63.05 - Last mean reward per episode: -200.38
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00113   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0864    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.0179     |
| rollout/                |            |
|    ep_len_mean          | 47.9       |
|    ep_rew_mean          | -168       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 534        |
|    time_elapsed         | 1929       |
|    total_timesteps      | 546816     |
| train/                  |            |
|    approx_kl            | 0.26566616 |
|    clip_fraction        | 0.472      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.903      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.4       |
|    n_updates            | 10660      |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.351      |
|    value_loss           | 138        |
----------------------------------------
----------------------------------------
| reward                  | -4.01      |
| reward_contact          | -0.000993  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0897    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0178     |
| rollout/                |            |
|    ep_len_mean          | 47.7       |
|    ep_rew_mean          | -167       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 535        |
|    time_elapsed         | 1932       |
|    total_timesteps      | 547840     |
| train/                  |            |
|    approx_kl            | 0.59053326 |
|    clip_fraction        | 0.573      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.893      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.43       |
|    n_updates            | 10680      |
|    policy_gradient_loss | -0.197     |
|    std                  | 0.351      |
|    value_loss           | 116        |
----------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.00113   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.081     |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0171     |
| rollout/                |            |
|    ep_len_mean          | 48.6       |
|    ep_rew_mean          | -171       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 536        |
|    time_elapsed         | 1936       |
|    total_timesteps      | 548864     |
| train/                  |            |
|    approx_kl            | 0.57912946 |
|    clip_fraction        | 0.633      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.876      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.35       |
|    n_updates            | 10700      |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.351      |
|    value_loss           | 85.8       |
----------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.000352 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0892   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.0127    |
| rollout/                |           |
|    ep_len_mean          | 38.3      |
|    ep_rew_mean          | -136      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 537       |
|    time_elapsed         | 1940      |
|    total_timesteps      | 549888    |
| train/                  |           |
|    approx_kl            | 1.1979327 |
|    clip_fraction        | 0.606     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.6     |
|    explained_variance   | 0.913     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.04      |
|    n_updates            | 10720     |
|    policy_gradient_loss | -0.203    |
|    std                  | 0.351     |
|    value_loss           | 141       |
---------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.000473 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0833   |
| reward_torque           | -3.85     |
| reward_velocity         | 0.014     |
| rollout/                |           |
|    ep_len_mean          | 48.1      |
|    ep_rew_mean          | -169      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 538       |
|    time_elapsed         | 1943      |
|    total_timesteps      | 550912    |
| train/                  |           |
|    approx_kl            | 1.0421927 |
|    clip_fraction        | 0.65      |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.928     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.902     |
|    n_updates            | 10740     |
|    policy_gradient_loss | -0.201    |
|    std                  | 0.351     |
|    value_loss           | 112       |
---------------------------------------
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.000338  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0889    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0138     |
| rollout/                |            |
|    ep_len_mean          | 35.8       |
|    ep_rew_mean          | -127       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 539        |
|    time_elapsed         | 1947       |
|    total_timesteps      | 551936     |
| train/                  |            |
|    approx_kl            | 0.34594598 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.857      |
|    learning_rate        | 0.0003     |
|    loss                 | 20.6       |
|    n_updates            | 10760      |
|    policy_gradient_loss | -0.139     |
|    std                  | 0.351      |
|    value_loss           | 170        |
----------------------------------------
Num timesteps: 552000
Best mean reward: -63.05 - Last mean reward per episode: -128.74
----------------------------------------
| reward                  | -4.03      |
| reward_contact          | -0.000338  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0889    |
| reward_torque           | -3.86      |
| reward_velocity         | 0.0139     |
| rollout/                |            |
|    ep_len_mean          | 35.8       |
|    ep_rew_mean          | -127       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 540        |
|    time_elapsed         | 1951       |
|    total_timesteps      | 552960     |
| train/                  |            |
|    approx_kl            | 0.85380745 |
|    clip_fraction        | 0.667      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.4      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.9        |
|    n_updates            | 10780      |
|    policy_gradient_loss | -0.209     |
|    std                  | 0.351      |
|    value_loss           | 126        |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.000948  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.076     |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0187     |
| rollout/                |            |
|    ep_len_mean          | 44.3       |
|    ep_rew_mean          | -155       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 541        |
|    time_elapsed         | 1954       |
|    total_timesteps      | 553984     |
| train/                  |            |
|    approx_kl            | 0.68285906 |
|    clip_fraction        | 0.609      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.14       |
|    n_updates            | 10800      |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.351      |
|    value_loss           | 53.2       |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.0014    |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0871    |
| reward_torque           | -3.83      |
| reward_velocity         | 0.0164     |
| rollout/                |            |
|    ep_len_mean          | 33.9       |
|    ep_rew_mean          | -120       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 542        |
|    time_elapsed         | 1958       |
|    total_timesteps      | 555008     |
| train/                  |            |
|    approx_kl            | 0.79760426 |
|    clip_fraction        | 0.66       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.7      |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.6        |
|    n_updates            | 10820      |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.351      |
|    value_loss           | 220        |
----------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.001    |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0827   |
| reward_torque           | -3.83     |
| reward_velocity         | 0.0114    |
| rollout/                |           |
|    ep_len_mean          | 24.7      |
|    ep_rew_mean          | -89.3     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 543       |
|    time_elapsed         | 1962      |
|    total_timesteps      | 556032    |
| train/                  |           |
|    approx_kl            | 0.3138877 |
|    clip_fraction        | 0.556     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.931     |
|    learning_rate        | 0.0003    |
|    loss                 | 28.7      |
|    n_updates            | 10840     |
|    policy_gradient_loss | -0.179    |
|    std                  | 0.351     |
|    value_loss           | 210       |
---------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.00112  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0872   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0148    |
| rollout/                |           |
|    ep_len_mean          | 35.2      |
|    ep_rew_mean          | -125      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 544       |
|    time_elapsed         | 1966      |
|    total_timesteps      | 557056    |
| train/                  |           |
|    approx_kl            | 0.4367705 |
|    clip_fraction        | 0.469     |
|    clip_range           | 0.4       |
|    entropy_loss         | -23.1     |
|    explained_variance   | 0.72      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.84      |
|    n_updates            | 10860     |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.351     |
|    value_loss           | 198       |
---------------------------------------
Num timesteps: 558000
Best mean reward: -63.05 - Last mean reward per episode: -124.75
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00112   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0872    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.015      |
| rollout/                |            |
|    ep_len_mean          | 45.4       |
|    ep_rew_mean          | -159       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 545        |
|    time_elapsed         | 1969       |
|    total_timesteps      | 558080     |
| train/                  |            |
|    approx_kl            | 0.31359047 |
|    clip_fraction        | 0.506      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.0003     |
|    loss                 | 23         |
|    n_updates            | 10880      |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.351      |
|    value_loss           | 135        |
----------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.000586 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.079    |
| reward_torque           | -3.83     |
| reward_velocity         | 0.0127    |
| rollout/                |           |
|    ep_len_mean          | 36.3      |
|    ep_rew_mean          | -129      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 546       |
|    time_elapsed         | 1973      |
|    total_timesteps      | 559104    |
| train/                  |           |
|    approx_kl            | 1.2112174 |
|    clip_fraction        | 0.592     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | -0.0695   |
|    learning_rate        | 0.0003    |
|    loss                 | 1.97      |
|    n_updates            | 10900     |
|    policy_gradient_loss | -0.139    |
|    std                  | 0.351     |
|    value_loss           | 85.8      |
---------------------------------------
----------------------------------------
| reward                  | -3.97      |
| reward_contact          | -0.000972  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0766    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0159     |
| rollout/                |            |
|    ep_len_mean          | 46.2       |
|    ep_rew_mean          | -163       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 547        |
|    time_elapsed         | 1977       |
|    total_timesteps      | 560128     |
| train/                  |            |
|    approx_kl            | 0.44535783 |
|    clip_fraction        | 0.585      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.823      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.17       |
|    n_updates            | 10920      |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.351      |
|    value_loss           | 175        |
----------------------------------------
---------------------------------------
| reward                  | -4        |
| reward_contact          | -0.00115  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0954   |
| reward_torque           | -3.82     |
| reward_velocity         | 0.0103    |
| rollout/                |           |
|    ep_len_mean          | 15.6      |
|    ep_rew_mean          | -59.7     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 548       |
|    time_elapsed         | 1980      |
|    total_timesteps      | 561152    |
| train/                  |           |
|    approx_kl            | 0.3666895 |
|    clip_fraction        | 0.488     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.7     |
|    explained_variance   | 0.725     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.25      |
|    n_updates            | 10940     |
|    policy_gradient_loss | -0.199    |
|    std                  | 0.351     |
|    value_loss           | 273       |
---------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00115   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0954    |
| reward_torque           | -3.81      |
| reward_velocity         | 0.0102     |
| rollout/                |            |
|    ep_len_mean          | 15.7       |
|    ep_rew_mean          | -60        |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 549        |
|    time_elapsed         | 1984       |
|    total_timesteps      | 562176     |
| train/                  |            |
|    approx_kl            | 0.63639164 |
|    clip_fraction        | 0.488      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23.2      |
|    explained_variance   | 0.0406     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.12       |
|    n_updates            | 10960      |
|    policy_gradient_loss | -0.212     |
|    std                  | 0.351      |
|    value_loss           | 184        |
----------------------------------------
----------------------------------------
| reward                  | -4         |
| reward_contact          | -0.00163   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0954    |
| reward_torque           | -3.82      |
| reward_velocity         | 0.00963    |
| rollout/                |            |
|    ep_len_mean          | 26.4       |
|    ep_rew_mean          | -96.2      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 550        |
|    time_elapsed         | 1988       |
|    total_timesteps      | 563200     |
| train/                  |            |
|    approx_kl            | 0.26546812 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.584      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.9       |
|    n_updates            | 10980      |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.351      |
|    value_loss           | 140        |
----------------------------------------
Num timesteps: 564000
Best mean reward: -63.05 - Last mean reward per episode: -125.46
---------------------------------------
| reward                  | -4.05     |
| reward_contact          | -0.00229  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0942   |
| reward_torque           | -3.87     |
| reward_velocity         | 0.0113    |
| rollout/                |           |
|    ep_len_mean          | 35.6      |
|    ep_rew_mean          | -127      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 551       |
|    time_elapsed         | 1991      |
|    total_timesteps      | 564224    |
| train/                  |           |
|    approx_kl            | 0.4346162 |
|    clip_fraction        | 0.51      |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.831     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.21      |
|    n_updates            | 11000     |
|    policy_gradient_loss | -0.195    |
|    std                  | 0.351     |
|    value_loss           | 145       |
---------------------------------------
----------------------------------------
| reward                  | -3.99      |
| reward_contact          | -0.000746  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0588    |
| reward_torque           | -3.85      |
| reward_velocity         | 0.015      |
| rollout/                |            |
|    ep_len_mean          | 14.4       |
|    ep_rew_mean          | -54.9      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 552        |
|    time_elapsed         | 1995       |
|    total_timesteps      | 565248     |
| train/                  |            |
|    approx_kl            | 0.40083927 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.9      |
|    explained_variance   | -0.285     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.99       |
|    n_updates            | 11020      |
|    policy_gradient_loss | -0.218     |
|    std                  | 0.351      |
|    value_loss           | 225        |
----------------------------------------
---------------------------------------
| reward                  | -3.98     |
| reward_contact          | -0.000853 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0539   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.0166    |
| rollout/                |           |
|    ep_len_mean          | 25        |
|    ep_rew_mean          | -90.8     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 553       |
|    time_elapsed         | 1999      |
|    total_timesteps      | 566272    |
| train/                  |           |
|    approx_kl            | 0.5287496 |
|    clip_fraction        | 0.557     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.9     |
|    explained_variance   | 0.708     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.48      |
|    n_updates            | 11040     |
|    policy_gradient_loss | -0.226    |
|    std                  | 0.351     |
|    value_loss           | 152       |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.000107  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.0852    |
| reward_torque           | -3.84      |
| reward_velocity         | 0.0128     |
| rollout/                |            |
|    ep_len_mean          | 24.1       |
|    ep_rew_mean          | -88.1      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 554        |
|    time_elapsed         | 2002       |
|    total_timesteps      | 567296     |
| train/                  |            |
|    approx_kl            | 0.46956775 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.09       |
|    n_updates            | 11060      |
|    policy_gradient_loss | -0.189     |
|    std                  | 0.351      |
|    value_loss           | 103        |
----------------------------------------
----------------------------------------
| reward                  | -4.04      |
| reward_contact          | -0.000221  |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.1       |
| reward_torque           | -3.85      |
| reward_velocity         | 0.00996    |
| rollout/                |            |
|    ep_len_mean          | 21.8       |
|    ep_rew_mean          | -80.6      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 555        |
|    time_elapsed         | 2006       |
|    total_timesteps      | 568320     |
| train/                  |            |
|    approx_kl            | 0.54161096 |
|    clip_fraction        | 0.553      |
|    clip_range           | 0.4        |
|    entropy_loss         | -23.2      |
|    explained_variance   | 0.561      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.69       |
|    n_updates            | 11080      |
|    policy_gradient_loss | -0.227     |
|    std                  | 0.351      |
|    value_loss           | 132        |
----------------------------------------
---------------------------------------
| reward                  | -4.04     |
| reward_contact          | -0.00118  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0822   |
| reward_torque           | -3.87     |
| reward_velocity         | 0.00746   |
| rollout/                |           |
|    ep_len_mean          | 22.3      |
|    ep_rew_mean          | -82.2     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 556       |
|    time_elapsed         | 2010      |
|    total_timesteps      | 569344    |
| train/                  |           |
|    approx_kl            | 0.6290469 |
|    clip_fraction        | 0.579     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.3     |
|    explained_variance   | 0.911     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.876     |
|    n_updates            | 11100     |
|    policy_gradient_loss | -0.195    |
|    std                  | 0.351     |
|    value_loss           | 90.3      |
---------------------------------------
Num timesteps: 570000
Best mean reward: -63.05 - Last mean reward per episode: -82.16
---------------------------------------
| reward                  | -4.03     |
| reward_contact          | -0.0013   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0768   |
| reward_torque           | -3.86     |
| reward_velocity         | 0.00836   |
| rollout/                |           |
|    ep_len_mean          | 32.3      |
|    ep_rew_mean          | -116      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 557       |
|    time_elapsed         | 2014      |
|    total_timesteps      | 570368    |
| train/                  |           |
|    approx_kl            | 0.6175133 |
|    clip_fraction        | 0.556     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.9     |
|    explained_variance   | 0.769     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.04      |
|    n_updates            | 11120     |
|    policy_gradient_loss | -0.206    |
|    std                  | 0.35      |
|    value_loss           | 158       |
---------------------------------------
---------------------------------------
| reward                  | -4.03     |
| reward_contact          | -0.0013   |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0692   |
| reward_torque           | -3.87     |
| reward_velocity         | 0.00987   |
| rollout/                |           |
|    ep_len_mean          | 42.5      |
|    ep_rew_mean          | -150      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 558       |
|    time_elapsed         | 2017      |
|    total_timesteps      | 571392    |
| train/                  |           |
|    approx_kl            | 0.4204683 |
|    clip_fraction        | 0.568     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.815     |
|    learning_rate        | 0.0003    |
|    loss                 | 6.05      |
|    n_updates            | 11140     |
|    policy_gradient_loss | -0.154    |
|    std                  | 0.35      |
|    value_loss           | 121       |
---------------------------------------
----------------------------------------
| reward                  | -4.02      |
| reward_contact          | -0.00108   |
| reward_ctrl             | -0.1       |
| reward_motion           | -0.087     |
| reward_torque           | -3.85      |
| reward_velocity         | 0.0107     |
| rollout/                |            |
|    ep_len_mean          | 35.5       |
|    ep_rew_mean          | -126       |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 559        |
|    time_elapsed         | 2021       |
|    total_timesteps      | 572416     |
| train/                  |            |
|    approx_kl            | 0.36268115 |
|    clip_fraction        | 0.552      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.884      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.46       |
|    n_updates            | 11160      |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.35       |
|    value_loss           | 90.6       |
----------------------------------------
---------------------------------------
| reward                  | -4.03     |
| reward_contact          | -0.000951 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0906   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.00828   |
| rollout/                |           |
|    ep_len_mean          | 16.8      |
|    ep_rew_mean          | -64       |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 560       |
|    time_elapsed         | 2025      |
|    total_timesteps      | 573440    |
| train/                  |           |
|    approx_kl            | 1.5559456 |
|    clip_fraction        | 0.556     |
|    clip_range           | 0.4       |
|    entropy_loss         | -23.1     |
|    explained_variance   | -0.0884   |
|    learning_rate        | 0.0003    |
|    loss                 | 1.83      |
|    n_updates            | 11180     |
|    policy_gradient_loss | -0.227    |
|    std                  | 0.35      |
|    value_loss           | 118       |
---------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.000951 |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0906   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.00845   |
| rollout/                |           |
|    ep_len_mean          | 26.4      |
|    ep_rew_mean          | -96.4     |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 561       |
|    time_elapsed         | 2029      |
|    total_timesteps      | 574464    |
| train/                  |           |
|    approx_kl            | 1.0972482 |
|    clip_fraction        | 0.569     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.9     |
|    explained_variance   | 0.751     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.841     |
|    n_updates            | 11200     |
|    policy_gradient_loss | -0.225    |
|    std                  | 0.35      |
|    value_loss           | 137       |
---------------------------------------
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.001    |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0855   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.00858   |
| rollout/                |           |
|    ep_len_mean          | 36.8      |
|    ep_rew_mean          | -131      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 562       |
|    time_elapsed         | 2032      |
|    total_timesteps      | 575488    |
| train/                  |           |
|    approx_kl            | 1.5520428 |
|    clip_fraction        | 0.529     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.848     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.99      |
|    n_updates            | 11220     |
|    policy_gradient_loss | -0.184    |
|    std                  | 0.35      |
|    value_loss           | 101       |
---------------------------------------
Num timesteps: 576000
Best mean reward: -63.05 - Last mean reward per episode: -130.79
---------------------------------------
| reward                  | -4.02     |
| reward_contact          | -0.00139  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0855   |
| reward_torque           | -3.84     |
| reward_velocity         | 0.00893   |
| rollout/                |           |
|    ep_len_mean          | 46.9      |
|    ep_rew_mean          | -165      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 563       |
|    time_elapsed         | 2036      |
|    total_timesteps      | 576512    |
| train/                  |           |
|    approx_kl            | 1.0211638 |
|    clip_fraction        | 0.637     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.893     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.47      |
|    n_updates            | 11240     |
|    policy_gradient_loss | -0.16     |
|    std                  | 0.35      |
|    value_loss           | 78.1      |
---------------------------------------
---------------------------------------
| reward                  | -4.03     |
| reward_contact          | -0.00139  |
| reward_ctrl             | -0.1      |
| reward_motion           | -0.0855   |
| reward_torque           | -3.85     |
| reward_velocity         | 0.00686   |
| rollout/                |           |
|    ep_len_mean          | 46.4      |
|    ep_rew_mean          | -163      |
| time/                   |           |
|    fps                  | 283       |
|    iterations           | 564       |
|    time_elapsed         | 2040      |
|    total_timesteps      | 577536    |
| train/                  |           |
|    approx_kl            | 0.6500547 |
|    clip_fraction        | 0.559     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.366     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.75      |
|    n_updates            | 11260     |
|    policy_gradient_loss | -0.163    |
|    std                  | 0.35      |
|    value_loss           | 116       |
---------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_9
---------------------------------
| reward             | -3.22    |
| reward_contact     | 0        |
| reward_ctrl        | 0        |
| reward_motion      | 0        |
| reward_torque      | -3.23    |
| reward_velocity    | 0.0152   |
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | -75.8    |
| time/              |          |
|    fps             | 282      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | -3.26      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.27      |
| reward_velocity         | 0.0191     |
| rollout/                |            |
|    ep_len_mean          | 81.2       |
|    ep_rew_mean          | -264       |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 2          |
|    time_elapsed         | 8          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.10685694 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.00577    |
|    learning_rate        | 0.0003     |
|    loss                 | 69         |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.127     |
|    std                  | 0.367      |
|    value_loss           | 309        |
----------------------------------------
----------------------------------------
| reward                  | -3.33      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.35      |
| reward_velocity         | 0.0177     |
| rollout/                |            |
|    ep_len_mean          | 124        |
|    ep_rew_mean          | -407       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 3          |
|    time_elapsed         | 13         |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.09222409 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | 18.9       |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0994    |
|    std                  | 0.367      |
|    value_loss           | 298        |
----------------------------------------
----------------------------------------
| reward                  | -3.39      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.41      |
| reward_velocity         | 0.0176     |
| rollout/                |            |
|    ep_len_mean          | 97.8       |
|    ep_rew_mean          | -321       |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 4          |
|    time_elapsed         | 17         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.12846133 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.219      |
|    learning_rate        | 0.0003     |
|    loss                 | 45.7       |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.367      |
|    value_loss           | 221        |
----------------------------------------
----------------------------------------
| reward                  | -3.39      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.41      |
| reward_velocity         | 0.0171     |
| rollout/                |            |
|    ep_len_mean          | 112        |
|    ep_rew_mean          | -370       |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 5          |
|    time_elapsed         | 22         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.09951311 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | -0.0808    |
|    learning_rate        | 0.0003     |
|    loss                 | 40.6       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.367      |
|    value_loss           | 164        |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -402.75
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.38      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.4       |
| reward_velocity         | 0.0174     |
| rollout/                |            |
|    ep_len_mean          | 122        |
|    ep_rew_mean          | -403       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 6          |
|    time_elapsed         | 28         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.07804628 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0003     |
|    loss                 | 51.4       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0756    |
|    std                  | 0.367      |
|    value_loss           | 263        |
----------------------------------------
----------------------------------------
| reward                  | -3.38      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.4       |
| reward_velocity         | 0.0202     |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | -424       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 7          |
|    time_elapsed         | 32         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.11590248 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.398      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.1       |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.367      |
|    value_loss           | 210        |
----------------------------------------
-----------------------------------------
| reward                  | -3.42       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.44       |
| reward_velocity         | 0.0237      |
| rollout/                |             |
|    ep_len_mean          | 106         |
|    ep_rew_mean          | -352        |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 8           |
|    time_elapsed         | 37          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.073391944 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.483       |
|    learning_rate        | 0.0003      |
|    loss                 | 90.9        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.077      |
|    std                  | 0.367       |
|    value_loss           | 340         |
-----------------------------------------
----------------------------------------
| reward                  | -3.4       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.43      |
| reward_velocity         | 0.0273     |
| rollout/                |            |
|    ep_len_mean          | 117        |
|    ep_rew_mean          | -389       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 9          |
|    time_elapsed         | 42         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.07272075 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.745      |
|    learning_rate        | 0.0003     |
|    loss                 | 98.3       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0948    |
|    std                  | 0.367      |
|    value_loss           | 381        |
----------------------------------------
-----------------------------------------
| reward                  | -3.4        |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.43       |
| reward_velocity         | 0.0268      |
| rollout/                |             |
|    ep_len_mean          | 117         |
|    ep_rew_mean          | -391        |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 10          |
|    time_elapsed         | 47          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.112510115 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.96        |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0958     |
|    std                  | 0.367       |
|    value_loss           | 125         |
-----------------------------------------
----------------------------------------
| reward                  | -3.4       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.43      |
| reward_velocity         | 0.0253     |
| rollout/                |            |
|    ep_len_mean          | 107        |
|    ep_rew_mean          | -357       |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 11         |
|    time_elapsed         | 52         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.08184364 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.519      |
|    learning_rate        | 0.0003     |
|    loss                 | 31.5       |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0951    |
|    std                  | 0.367      |
|    value_loss           | 213        |
----------------------------------------
Num timesteps: 12000
Best mean reward: -402.75 - Last mean reward per episode: -395.19
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.4       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.43      |
| reward_velocity         | 0.0277     |
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -339       |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 12         |
|    time_elapsed         | 57         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.08700475 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.306      |
|    learning_rate        | 0.0003     |
|    loss                 | 36.8       |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0985    |
|    std                  | 0.367      |
|    value_loss           | 220        |
----------------------------------------
----------------------------------------
| reward                  | -3.4       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.43      |
| reward_velocity         | 0.0309     |
| rollout/                |            |
|    ep_len_mean          | 69.2       |
|    ep_rew_mean          | -231       |
| time/                   |            |
|    fps                  | 212        |
|    iterations           | 13         |
|    time_elapsed         | 62         |
|    total_timesteps      | 13312      |
| train/                  |            |
|    approx_kl            | 0.06795293 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.355      |
|    learning_rate        | 0.0003     |
|    loss                 | 45.1       |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.1       |
|    std                  | 0.367      |
|    value_loss           | 207        |
----------------------------------------
-----------------------------------------
| reward                  | -3.4        |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.42       |
| reward_velocity         | 0.0232      |
| rollout/                |             |
|    ep_len_mean          | 64.2        |
|    ep_rew_mean          | -215        |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 14          |
|    time_elapsed         | 67          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.069691315 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.7       |
|    explained_variance   | -0.0594     |
|    learning_rate        | 0.0003      |
|    loss                 | 97.6        |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0907     |
|    std                  | 0.367       |
|    value_loss           | 403         |
-----------------------------------------
----------------------------------------
| reward                  | -3.4       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.42      |
| reward_velocity         | 0.0202     |
| rollout/                |            |
|    ep_len_mean          | 44.7       |
|    ep_rew_mean          | -149       |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 15         |
|    time_elapsed         | 72         |
|    total_timesteps      | 15360      |
| train/                  |            |
|    approx_kl            | 0.06312081 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.9      |
|    explained_variance   | 0.176      |
|    learning_rate        | 0.0003     |
|    loss                 | 42.1       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0966    |
|    std                  | 0.367      |
|    value_loss           | 262        |
----------------------------------------
----------------------------------------
| reward                  | -3.4       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.42      |
| reward_velocity         | 0.0223     |
| rollout/                |            |
|    ep_len_mean          | 54.7       |
|    ep_rew_mean          | -183       |
| time/                   |            |
|    fps                  | 210        |
|    iterations           | 16         |
|    time_elapsed         | 77         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.08332546 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.7       |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0938    |
|    std                  | 0.366      |
|    value_loss           | 116        |
----------------------------------------
--------------------------------------
| reward                  | -3.41    |
| reward_contact          | 0        |
| reward_ctrl             | 0        |
| reward_motion           | 0        |
| reward_torque           | -3.43    |
| reward_velocity         | 0.0234   |
| rollout/                |          |
|    ep_len_mean          | 65.5     |
|    ep_rew_mean          | -220     |
| time/                   |          |
|    fps                  | 210      |
|    iterations           | 17       |
|    time_elapsed         | 82       |
|    total_timesteps      | 17408    |
| train/                  |          |
|    approx_kl            | 0.072109 |
|    clip_fraction        | 0.176    |
|    clip_range           | 0.4      |
|    entropy_loss         | -22.2    |
|    explained_variance   | 0.765    |
|    learning_rate        | 0.0003   |
|    loss                 | 23.3     |
|    n_updates            | 320      |
|    policy_gradient_loss | -0.0753  |
|    std                  | 0.366    |
|    value_loss           | 152      |
--------------------------------------
Num timesteps: 18000
Best mean reward: -395.19 - Last mean reward per episode: -252.97
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | -3.4        |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.42       |
| reward_velocity         | 0.0236      |
| rollout/                |             |
|    ep_len_mean          | 75.4        |
|    ep_rew_mean          | -253        |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 18          |
|    time_elapsed         | 88          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.060054056 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.3       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | 38.9        |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0819     |
|    std                  | 0.366       |
|    value_loss           | 219         |
-----------------------------------------
---------------------------------------
| reward                  | -3.41     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0         |
| reward_torque           | -3.43     |
| reward_velocity         | 0.0253    |
| rollout/                |           |
|    ep_len_mean          | 85.4      |
|    ep_rew_mean          | -287      |
| time/                   |           |
|    fps                  | 208       |
|    iterations           | 19        |
|    time_elapsed         | 93        |
|    total_timesteps      | 19456     |
| train/                  |           |
|    approx_kl            | 0.0652843 |
|    clip_fraction        | 0.145     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.5     |
|    explained_variance   | 0.883     |
|    learning_rate        | 0.0003    |
|    loss                 | 46.1      |
|    n_updates            | 360       |
|    policy_gradient_loss | -0.0702   |
|    std                  | 0.366     |
|    value_loss           | 195       |
---------------------------------------
----------------------------------------
| reward                  | -3.4       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.43      |
| reward_velocity         | 0.0264     |
| rollout/                |            |
|    ep_len_mean          | 95.4       |
|    ep_rew_mean          | -322       |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 20         |
|    time_elapsed         | 98         |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.10353299 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.201      |
|    learning_rate        | 0.0003     |
|    loss                 | 24         |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.099     |
|    std                  | 0.366      |
|    value_loss           | 152        |
----------------------------------------
----------------------------------------
| reward                  | -3.42      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0288     |
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | -356       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 21         |
|    time_elapsed         | 103        |
|    total_timesteps      | 21504      |
| train/                  |            |
|    approx_kl            | 0.15301812 |
|    clip_fraction        | 0.354      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.3      |
|    explained_variance   | 0.383      |
|    learning_rate        | 0.0003     |
|    loss                 | 42.8       |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.103     |
|    std                  | 0.366      |
|    value_loss           | 103        |
----------------------------------------
-----------------------------------------
| reward                  | -3.42       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.45       |
| reward_velocity         | 0.0268      |
| rollout/                |             |
|    ep_len_mean          | 103         |
|    ep_rew_mean          | -350        |
| time/                   |             |
|    fps                  | 208         |
|    iterations           | 22          |
|    time_elapsed         | 108         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.048719645 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.2       |
|    explained_variance   | 0.519       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.4        |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0751     |
|    std                  | 0.366       |
|    value_loss           | 174         |
-----------------------------------------
----------------------------------------
| reward                  | -3.44      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0262     |
| rollout/                |            |
|    ep_len_mean          | 108        |
|    ep_rew_mean          | -365       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 23         |
|    time_elapsed         | 113        |
|    total_timesteps      | 23552      |
| train/                  |            |
|    approx_kl            | 0.06391521 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.23       |
|    learning_rate        | 0.0003     |
|    loss                 | 38.8       |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0748    |
|    std                  | 0.366      |
|    value_loss           | 274        |
----------------------------------------
Num timesteps: 24000
Best mean reward: -252.97 - Last mean reward per episode: -398.95
----------------------------------------
| reward                  | -3.41      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.43      |
| reward_velocity         | 0.028      |
| rollout/                |            |
|    ep_len_mean          | 119        |
|    ep_rew_mean          | -402       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 24         |
|    time_elapsed         | 118        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.10754076 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.434      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.81       |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0999    |
|    std                  | 0.366      |
|    value_loss           | 109        |
----------------------------------------
----------------------------------------
| reward                  | -3.44      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0235     |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | -435       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 25         |
|    time_elapsed         | 123        |
|    total_timesteps      | 25600      |
| train/                  |            |
|    approx_kl            | 0.06762931 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.508      |
|    learning_rate        | 0.0003     |
|    loss                 | 35.5       |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0988    |
|    std                  | 0.366      |
|    value_loss           | 269        |
----------------------------------------
----------------------------------------
| reward                  | -3.44      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0234     |
| rollout/                |            |
|    ep_len_mean          | 138        |
|    ep_rew_mean          | -466       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 26         |
|    time_elapsed         | 128        |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.06698659 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.479      |
|    learning_rate        | 0.0003     |
|    loss                 | 30.7       |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.0869    |
|    std                  | 0.366      |
|    value_loss           | 258        |
----------------------------------------
----------------------------------------
| reward                  | -3.46      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0249     |
| rollout/                |            |
|    ep_len_mean          | 139        |
|    ep_rew_mean          | -473       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 27         |
|    time_elapsed         | 133        |
|    total_timesteps      | 27648      |
| train/                  |            |
|    approx_kl            | 0.03745903 |
|    clip_fraction        | 0.0746     |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0003     |
|    loss                 | 38.4       |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.0618    |
|    std                  | 0.366      |
|    value_loss           | 290        |
----------------------------------------
----------------------------------------
| reward                  | -3.45      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0324     |
| rollout/                |            |
|    ep_len_mean          | 149        |
|    ep_rew_mean          | -507       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 28         |
|    time_elapsed         | 138        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.09587663 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.182      |
|    learning_rate        | 0.0003     |
|    loss                 | 29.7       |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.366      |
|    value_loss           | 300        |
----------------------------------------
-----------------------------------------
| reward                  | -3.44       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.47       |
| reward_velocity         | 0.0342      |
| rollout/                |             |
|    ep_len_mean          | 150         |
|    ep_rew_mean          | -509        |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 29          |
|    time_elapsed         | 143         |
|    total_timesteps      | 29696       |
| train/                  |             |
|    approx_kl            | 0.082485154 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.4        |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0873     |
|    std                  | 0.366       |
|    value_loss           | 235         |
-----------------------------------------
Num timesteps: 30000
Best mean reward: -252.97 - Last mean reward per episode: -543.54
----------------------------------------
| reward                  | -3.45      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0395     |
| rollout/                |            |
|    ep_len_mean          | 159        |
|    ep_rew_mean          | -539       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 30         |
|    time_elapsed         | 148        |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.08126942 |
|    clip_fraction        | 0.222      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.68       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.48       |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.366      |
|    value_loss           | 79.7       |
----------------------------------------
----------------------------------------
| reward                  | -3.47      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0389     |
| rollout/                |            |
|    ep_len_mean          | 140        |
|    ep_rew_mean          | -475       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 31         |
|    time_elapsed         | 153        |
|    total_timesteps      | 31744      |
| train/                  |            |
|    approx_kl            | 0.08574103 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.357      |
|    learning_rate        | 0.0003     |
|    loss                 | 13         |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.366      |
|    value_loss           | 194        |
----------------------------------------
----------------------------------------
| reward                  | -3.47      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.5       |
| reward_velocity         | 0.0331     |
| rollout/                |            |
|    ep_len_mean          | 96.1       |
|    ep_rew_mean          | -326       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 32         |
|    time_elapsed         | 158        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.06671844 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.3       |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0904    |
|    std                  | 0.366      |
|    value_loss           | 193        |
----------------------------------------
-----------------------------------------
| reward                  | -3.47       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.5        |
| reward_velocity         | 0.0335      |
| rollout/                |             |
|    ep_len_mean          | 106         |
|    ep_rew_mean          | -360        |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 33          |
|    time_elapsed         | 163         |
|    total_timesteps      | 33792       |
| train/                  |             |
|    approx_kl            | 0.062012903 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.3        |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.0928     |
|    std                  | 0.366       |
|    value_loss           | 222         |
-----------------------------------------
----------------------------------------
| reward                  | -3.49      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0324     |
| rollout/                |            |
|    ep_len_mean          | 94.8       |
|    ep_rew_mean          | -322       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 34         |
|    time_elapsed         | 168        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.07803719 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.4       |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.366      |
|    value_loss           | 91.7       |
----------------------------------------
-----------------------------------------
| reward                  | -3.47       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.5        |
| reward_velocity         | 0.0325      |
| rollout/                |             |
|    ep_len_mean          | 92.6        |
|    ep_rew_mean          | -315        |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 35          |
|    time_elapsed         | 173         |
|    total_timesteps      | 35840       |
| train/                  |             |
|    approx_kl            | 0.099081606 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.35        |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.109      |
|    std                  | 0.366       |
|    value_loss           | 140         |
-----------------------------------------
Num timesteps: 36000
Best mean reward: -252.97 - Last mean reward per episode: -315.38
---------------------------------------
| reward                  | -3.46     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0         |
| reward_torque           | -3.49     |
| reward_velocity         | 0.0323    |
| rollout/                |           |
|    ep_len_mean          | 102       |
|    ep_rew_mean          | -349      |
| time/                   |           |
|    fps                  | 206       |
|    iterations           | 36        |
|    time_elapsed         | 178       |
|    total_timesteps      | 36864     |
| train/                  |           |
|    approx_kl            | 0.1101397 |
|    clip_fraction        | 0.194     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.214     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.82      |
|    n_updates            | 700       |
|    policy_gradient_loss | -0.0943   |
|    std                  | 0.366     |
|    value_loss           | 136       |
---------------------------------------
----------------------------------------
| reward                  | -3.45      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0207     |
| rollout/                |            |
|    ep_len_mean          | 78.2       |
|    ep_rew_mean          | -267       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 37         |
|    time_elapsed         | 183        |
|    total_timesteps      | 37888      |
| train/                  |            |
|    approx_kl            | 0.04298848 |
|    clip_fraction        | 0.0799     |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.862      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.8       |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.0629    |
|    std                  | 0.366      |
|    value_loss           | 157        |
----------------------------------------
-----------------------------------------
| reward                  | -3.45       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.48       |
| reward_velocity         | 0.0258      |
| rollout/                |             |
|    ep_len_mean          | 87.9        |
|    ep_rew_mean          | -300        |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 38          |
|    time_elapsed         | 188         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.067519605 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | 0.0662      |
|    learning_rate        | 0.0003      |
|    loss                 | 42.3        |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.096      |
|    std                  | 0.366       |
|    value_loss           | 346         |
-----------------------------------------
----------------------------------------
| reward                  | -3.45      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.48      |
| reward_velocity         | 0.025      |
| rollout/                |            |
|    ep_len_mean          | 87.7       |
|    ep_rew_mean          | -299       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 39         |
|    time_elapsed         | 193        |
|    total_timesteps      | 39936      |
| train/                  |            |
|    approx_kl            | 0.08896073 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.801      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.78       |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.366      |
|    value_loss           | 136        |
----------------------------------------
-----------------------------------------
| reward                  | -3.44       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.46       |
| reward_velocity         | 0.0226      |
| rollout/                |             |
|    ep_len_mean          | 87.9        |
|    ep_rew_mean          | -300        |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 40          |
|    time_elapsed         | 198         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.091612816 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.2       |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.54        |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.117      |
|    std                  | 0.366       |
|    value_loss           | 68.3        |
-----------------------------------------
-----------------------------------------
| reward                  | -3.43       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.46       |
| reward_velocity         | 0.0254      |
| rollout/                |             |
|    ep_len_mean          | 95.5        |
|    ep_rew_mean          | -326        |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 41          |
|    time_elapsed         | 203         |
|    total_timesteps      | 41984       |
| train/                  |             |
|    approx_kl            | 0.091683194 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.222       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.63        |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.112      |
|    std                  | 0.366       |
|    value_loss           | 151         |
-----------------------------------------
Num timesteps: 42000
Best mean reward: -252.97 - Last mean reward per episode: -326.50
----------------------------------------
| reward                  | -3.45      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0252     |
| rollout/                |            |
|    ep_len_mean          | 79.3       |
|    ep_rew_mean          | -271       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 42         |
|    time_elapsed         | 208        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.13295096 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.544      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.2       |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.127     |
|    std                  | 0.366      |
|    value_loss           | 224        |
----------------------------------------
----------------------------------------
| reward                  | -3.44      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0256     |
| rollout/                |            |
|    ep_len_mean          | 76.4       |
|    ep_rew_mean          | -262       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 43         |
|    time_elapsed         | 213        |
|    total_timesteps      | 44032      |
| train/                  |            |
|    approx_kl            | 0.08034168 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.655      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.64       |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.366      |
|    value_loss           | 159        |
----------------------------------------
----------------------------------------
| reward                  | -3.47      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0265     |
| rollout/                |            |
|    ep_len_mean          | 80.4       |
|    ep_rew_mean          | -276       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 44         |
|    time_elapsed         | 218        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.09620194 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.745      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.19       |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.366      |
|    value_loss           | 109        |
----------------------------------------
----------------------------------------
| reward                  | -3.46      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0276     |
| rollout/                |            |
|    ep_len_mean          | 91.1       |
|    ep_rew_mean          | -312       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 45         |
|    time_elapsed         | 223        |
|    total_timesteps      | 46080      |
| train/                  |            |
|    approx_kl            | 0.08886904 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.578      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.53       |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.366      |
|    value_loss           | 141        |
----------------------------------------
-----------------------------------------
| reward                  | -3.48       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.51       |
| reward_velocity         | 0.0278      |
| rollout/                |             |
|    ep_len_mean          | 90.7        |
|    ep_rew_mean          | -311        |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 46          |
|    time_elapsed         | 228         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.105513476 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.544       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.3         |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.105      |
|    std                  | 0.366       |
|    value_loss           | 118         |
-----------------------------------------
Num timesteps: 48000
Best mean reward: -252.97 - Last mean reward per episode: -348.42
---------------------------------------
| reward                  | -3.49     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0         |
| reward_torque           | -3.52     |
| reward_velocity         | 0.0278    |
| rollout/                |           |
|    ep_len_mean          | 102       |
|    ep_rew_mean          | -348      |
| time/                   |           |
|    fps                  | 206       |
|    iterations           | 47        |
|    time_elapsed         | 233       |
|    total_timesteps      | 48128     |
| train/                  |           |
|    approx_kl            | 0.1558862 |
|    clip_fraction        | 0.31      |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.654     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.34      |
|    n_updates            | 920       |
|    policy_gradient_loss | -0.135    |
|    std                  | 0.366     |
|    value_loss           | 171       |
---------------------------------------
----------------------------------------
| reward                  | -3.48      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0282     |
| rollout/                |            |
|    ep_len_mean          | 114        |
|    ep_rew_mean          | -390       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 48         |
|    time_elapsed         | 238        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.06145618 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.0003     |
|    loss                 | 25.5       |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.0911    |
|    std                  | 0.366      |
|    value_loss           | 190        |
----------------------------------------
----------------------------------------
| reward                  | -3.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0232     |
| rollout/                |            |
|    ep_len_mean          | 93.1       |
|    ep_rew_mean          | -319       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 49         |
|    time_elapsed         | 243        |
|    total_timesteps      | 50176      |
| train/                  |            |
|    approx_kl            | 0.07513807 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.456      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.8       |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.093     |
|    std                  | 0.366      |
|    value_loss           | 157        |
----------------------------------------
----------------------------------------
| reward                  | -3.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0243     |
| rollout/                |            |
|    ep_len_mean          | 103        |
|    ep_rew_mean          | -351       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 50         |
|    time_elapsed         | 248        |
|    total_timesteps      | 51200      |
| train/                  |            |
|    approx_kl            | 0.13576367 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.9       |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.127     |
|    std                  | 0.366      |
|    value_loss           | 136        |
----------------------------------------
----------------------------------------
| reward                  | -3.54      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0252     |
| rollout/                |            |
|    ep_len_mean          | 102        |
|    ep_rew_mean          | -348       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 51         |
|    time_elapsed         | 253        |
|    total_timesteps      | 52224      |
| train/                  |            |
|    approx_kl            | 0.11762303 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.561      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.66       |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.366      |
|    value_loss           | 104        |
----------------------------------------
----------------------------------------
| reward                  | -3.53      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.56      |
| reward_velocity         | 0.027      |
| rollout/                |            |
|    ep_len_mean          | 112        |
|    ep_rew_mean          | -383       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 52         |
|    time_elapsed         | 258        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.08652225 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 30.1       |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.366      |
|    value_loss           | 255        |
----------------------------------------
Num timesteps: 54000
Best mean reward: -252.97 - Last mean reward per episode: -417.31
-----------------------------------------
| reward                  | -3.55       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.58       |
| reward_velocity         | 0.029       |
| rollout/                |             |
|    ep_len_mean          | 122         |
|    ep_rew_mean          | -419        |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 53          |
|    time_elapsed         | 263         |
|    total_timesteps      | 54272       |
| train/                  |             |
|    approx_kl            | 0.099555954 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.1       |
|    explained_variance   | 0.605       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.5        |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.107      |
|    std                  | 0.366       |
|    value_loss           | 171         |
-----------------------------------------
----------------------------------------
| reward                  | -3.55      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0293     |
| rollout/                |            |
|    ep_len_mean          | 133        |
|    ep_rew_mean          | -454       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 54         |
|    time_elapsed         | 268        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.06288915 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.9       |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.101     |
|    std                  | 0.366      |
|    value_loss           | 192        |
----------------------------------------
-----------------------------------------
| reward                  | -3.52       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.55       |
| reward_velocity         | 0.03        |
| rollout/                |             |
|    ep_len_mean          | 132         |
|    ep_rew_mean          | -453        |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 55          |
|    time_elapsed         | 273         |
|    total_timesteps      | 56320       |
| train/                  |             |
|    approx_kl            | 0.057649896 |
|    clip_fraction        | 0.0987      |
|    clip_range           | 0.4         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.8        |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0702     |
|    std                  | 0.366       |
|    value_loss           | 261         |
-----------------------------------------
----------------------------------------
| reward                  | -3.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.55      |
| reward_velocity         | 0.037      |
| rollout/                |            |
|    ep_len_mean          | 143        |
|    ep_rew_mean          | -488       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 56         |
|    time_elapsed         | 278        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.10201044 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.253      |
|    learning_rate        | 0.0003     |
|    loss                 | 18         |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.0933    |
|    std                  | 0.366      |
|    value_loss           | 266        |
----------------------------------------
----------------------------------------
| reward                  | -3.54      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0385     |
| rollout/                |            |
|    ep_len_mean          | 122        |
|    ep_rew_mean          | -419       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 57         |
|    time_elapsed         | 283        |
|    total_timesteps      | 58368      |
| train/                  |            |
|    approx_kl            | 0.06344026 |
|    clip_fraction        | 0.131      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.01       |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.0913    |
|    std                  | 0.366      |
|    value_loss           | 116        |
----------------------------------------
---------------------------------------
| reward                  | -3.54     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0         |
| reward_torque           | -3.58     |
| reward_velocity         | 0.0416    |
| rollout/                |           |
|    ep_len_mean          | 133       |
|    ep_rew_mean          | -454      |
| time/                   |           |
|    fps                  | 206       |
|    iterations           | 58        |
|    time_elapsed         | 287       |
|    total_timesteps      | 59392     |
| train/                  |           |
|    approx_kl            | 0.0898159 |
|    clip_fraction        | 0.182     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.8     |
|    explained_variance   | 0.635     |
|    learning_rate        | 0.0003    |
|    loss                 | 13.1      |
|    n_updates            | 1140      |
|    policy_gradient_loss | -0.106    |
|    std                  | 0.366     |
|    value_loss           | 234       |
---------------------------------------
Num timesteps: 60000
Best mean reward: -252.97 - Last mean reward per episode: -456.03
----------------------------------------
| reward                  | -3.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0414     |
| rollout/                |            |
|    ep_len_mean          | 133        |
|    ep_rew_mean          | -456       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 59         |
|    time_elapsed         | 292        |
|    total_timesteps      | 60416      |
| train/                  |            |
|    approx_kl            | 0.12097858 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.56       |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.12      |
|    std                  | 0.365      |
|    value_loss           | 89         |
----------------------------------------
----------------------------------------
| reward                  | -3.48      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0424     |
| rollout/                |            |
|    ep_len_mean          | 121        |
|    ep_rew_mean          | -415       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 60         |
|    time_elapsed         | 297        |
|    total_timesteps      | 61440      |
| train/                  |            |
|    approx_kl            | 0.14020307 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.87       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.18       |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.127     |
|    std                  | 0.365      |
|    value_loss           | 85         |
----------------------------------------
----------------------------------------
| reward                  | -3.49      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.53      |
| reward_velocity         | 0.0424     |
| rollout/                |            |
|    ep_len_mean          | 132        |
|    ep_rew_mean          | -454       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 61         |
|    time_elapsed         | 302        |
|    total_timesteps      | 62464      |
| train/                  |            |
|    approx_kl            | 0.08372341 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.697      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.2       |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.365      |
|    value_loss           | 226        |
----------------------------------------
-----------------------------------------
| reward                  | -3.49       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.53       |
| reward_velocity         | 0.0356      |
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | -362        |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 62          |
|    time_elapsed         | 307         |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.080171436 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.4         |
|    entropy_loss         | -22         |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.71        |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.115      |
|    std                  | 0.365       |
|    value_loss           | 114         |
-----------------------------------------
----------------------------------------
| reward                  | -3.47      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.5       |
| reward_velocity         | 0.0288     |
| rollout/                |            |
|    ep_len_mean          | 84.9       |
|    ep_rew_mean          | -292       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 63         |
|    time_elapsed         | 311        |
|    total_timesteps      | 64512      |
| train/                  |            |
|    approx_kl            | 0.08373365 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | -0.0373    |
|    learning_rate        | 0.0003     |
|    loss                 | 27.5       |
|    n_updates            | 1240       |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.365      |
|    value_loss           | 333        |
----------------------------------------
----------------------------------------
| reward                  | -3.47      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0191     |
| rollout/                |            |
|    ep_len_mean          | 74.1       |
|    ep_rew_mean          | -254       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 64         |
|    time_elapsed         | 316        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.09768624 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.526      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.97       |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.365      |
|    value_loss           | 225        |
----------------------------------------
Num timesteps: 66000
Best mean reward: -252.97 - Last mean reward per episode: -254.16
----------------------------------------
| reward                  | -3.46      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0199     |
| rollout/                |            |
|    ep_len_mean          | 74.1       |
|    ep_rew_mean          | -254       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 65         |
|    time_elapsed         | 321        |
|    total_timesteps      | 66560      |
| train/                  |            |
|    approx_kl            | 0.11038068 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.442      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.4       |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.365      |
|    value_loss           | 186        |
----------------------------------------
-----------------------------------------
| reward                  | -3.52       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.54       |
| reward_velocity         | 0.0145      |
| rollout/                |             |
|    ep_len_mean          | 64.7        |
|    ep_rew_mean          | -223        |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 66          |
|    time_elapsed         | 326         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.109496385 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.555       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.86        |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.118      |
|    std                  | 0.365       |
|    value_loss           | 93.9        |
-----------------------------------------
----------------------------------------
| reward                  | -3.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.51      |
| reward_velocity         | 0.015      |
| rollout/                |            |
|    ep_len_mean          | 54.8       |
|    ep_rew_mean          | -189       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 67         |
|    time_elapsed         | 331        |
|    total_timesteps      | 68608      |
| train/                  |            |
|    approx_kl            | 0.09371218 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.101      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.5       |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.365      |
|    value_loss           | 299        |
----------------------------------------
----------------------------------------
| reward                  | -3.49      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0153     |
| rollout/                |            |
|    ep_len_mean          | 54.3       |
|    ep_rew_mean          | -187       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 68         |
|    time_elapsed         | 336        |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.09444392 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.302      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.5       |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.365      |
|    value_loss           | 244        |
----------------------------------------
----------------------------------------
| reward                  | -3.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0176     |
| rollout/                |            |
|    ep_len_mean          | 53.8       |
|    ep_rew_mean          | -186       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 69         |
|    time_elapsed         | 340        |
|    total_timesteps      | 70656      |
| train/                  |            |
|    approx_kl            | 0.14548701 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.74       |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.128     |
|    std                  | 0.365      |
|    value_loss           | 102        |
----------------------------------------
----------------------------------------
| reward                  | -3.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0184     |
| rollout/                |            |
|    ep_len_mean          | 63.9       |
|    ep_rew_mean          | -220       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 70         |
|    time_elapsed         | 345        |
|    total_timesteps      | 71680      |
| train/                  |            |
|    approx_kl            | 0.08314982 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.0003     |
|    loss                 | 24.1       |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.118     |
|    std                  | 0.365      |
|    value_loss           | 279        |
----------------------------------------
Num timesteps: 72000
Best mean reward: -252.97 - Last mean reward per episode: -179.34
Saving new best model to rl/out_dir/models/exp74/best_model.zip
-----------------------------------------
| reward                  | -3.51       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.52       |
| reward_velocity         | 0.0191      |
| rollout/                |             |
|    ep_len_mean          | 51.3        |
|    ep_rew_mean          | -177        |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 71          |
|    time_elapsed         | 351         |
|    total_timesteps      | 72704       |
| train/                  |             |
|    approx_kl            | 0.096345484 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.5       |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.5        |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.127      |
|    std                  | 0.365       |
|    value_loss           | 126         |
-----------------------------------------
----------------------------------------
| reward                  | -3.48      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.5       |
| reward_velocity         | 0.019      |
| rollout/                |            |
|    ep_len_mean          | 61.7       |
|    ep_rew_mean          | -212       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 72         |
|    time_elapsed         | 356        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.15074712 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.768      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.71       |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.152     |
|    std                  | 0.365      |
|    value_loss           | 131        |
----------------------------------------
----------------------------------------
| reward                  | -3.49      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0191     |
| rollout/                |            |
|    ep_len_mean          | 71.7       |
|    ep_rew_mean          | -246       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 73         |
|    time_elapsed         | 361        |
|    total_timesteps      | 74752      |
| train/                  |            |
|    approx_kl            | 0.13299297 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.648      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.73       |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.141     |
|    std                  | 0.365      |
|    value_loss           | 117        |
----------------------------------------
----------------------------------------
| reward                  | -3.45      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0221     |
| rollout/                |            |
|    ep_len_mean          | 81         |
|    ep_rew_mean          | -278       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 74         |
|    time_elapsed         | 365        |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.08469623 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.0003     |
|    loss                 | 15         |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.365      |
|    value_loss           | 219        |
----------------------------------------
----------------------------------------
| reward                  | -3.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0191     |
| rollout/                |            |
|    ep_len_mean          | 79.4       |
|    ep_rew_mean          | -273       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 75         |
|    time_elapsed         | 370        |
|    total_timesteps      | 76800      |
| train/                  |            |
|    approx_kl            | 0.16653484 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.72       |
|    learning_rate        | 0.0003     |
|    loss                 | 6.43       |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.138     |
|    std                  | 0.365      |
|    value_loss           | 134        |
----------------------------------------
-----------------------------------------
| reward                  | -3.55       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.57       |
| reward_velocity         | 0.014       |
| rollout/                |             |
|    ep_len_mean          | 59.9        |
|    ep_rew_mean          | -207        |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 76          |
|    time_elapsed         | 375         |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 0.090216026 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.0501      |
|    learning_rate        | 0.0003      |
|    loss                 | 18.8        |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.127      |
|    std                  | 0.365       |
|    value_loss           | 240         |
-----------------------------------------
Num timesteps: 78000
Best mean reward: -179.34 - Last mean reward per episode: -206.66
----------------------------------------
| reward                  | -3.55      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.56      |
| reward_velocity         | 0.014      |
| rollout/                |            |
|    ep_len_mean          | 69.9       |
|    ep_rew_mean          | -241       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 77         |
|    time_elapsed         | 380        |
|    total_timesteps      | 78848      |
| train/                  |            |
|    approx_kl            | 0.10045168 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.388      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.9       |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.142     |
|    std                  | 0.365      |
|    value_loss           | 208        |
----------------------------------------
----------------------------------------
| reward                  | -3.57      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0124     |
| rollout/                |            |
|    ep_len_mean          | 43.2       |
|    ep_rew_mean          | -150       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 78         |
|    time_elapsed         | 384        |
|    total_timesteps      | 79872      |
| train/                  |            |
|    approx_kl            | 0.12553442 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.755      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.04       |
|    n_updates            | 1540       |
|    policy_gradient_loss | -0.134     |
|    std                  | 0.365      |
|    value_loss           | 126        |
----------------------------------------
---------------------------------------
| reward                  | -3.53     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0         |
| reward_torque           | -3.54     |
| reward_velocity         | 0.0144    |
| rollout/                |           |
|    ep_len_mean          | 43.2      |
|    ep_rew_mean          | -150      |
| time/                   |           |
|    fps                  | 207       |
|    iterations           | 79        |
|    time_elapsed         | 389       |
|    total_timesteps      | 80896     |
| train/                  |           |
|    approx_kl            | 0.1187946 |
|    clip_fraction        | 0.255     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.546     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.32      |
|    n_updates            | 1560      |
|    policy_gradient_loss | -0.137    |
|    std                  | 0.365     |
|    value_loss           | 163       |
---------------------------------------
-----------------------------------------
| reward                  | -3.52       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0           |
| reward_torque           | -3.53       |
| reward_velocity         | 0.015       |
| rollout/                |             |
|    ep_len_mean          | 53.1        |
|    ep_rew_mean          | -185        |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 80          |
|    time_elapsed         | 394         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.103914715 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.85        |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.149      |
|    std                  | 0.365       |
|    value_loss           | 227         |
-----------------------------------------
----------------------------------------
| reward                  | -3.57      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0197     |
| rollout/                |            |
|    ep_len_mean          | 53.9       |
|    ep_rew_mean          | -188       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 81         |
|    time_elapsed         | 399        |
|    total_timesteps      | 82944      |
| train/                  |            |
|    approx_kl            | 0.09956674 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.663      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.68       |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.121     |
|    std                  | 0.365      |
|    value_loss           | 70         |
----------------------------------------
----------------------------------------
| reward                  | -3.56      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0213     |
| rollout/                |            |
|    ep_len_mean          | 54.5       |
|    ep_rew_mean          | -190       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 82         |
|    time_elapsed         | 404        |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.07202363 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | -0.141     |
|    learning_rate        | 0.0003     |
|    loss                 | 13.3       |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.365      |
|    value_loss           | 250        |
----------------------------------------
Num timesteps: 84000
Best mean reward: -179.34 - Last mean reward per episode: -189.54
----------------------------------------
| reward                  | -3.57      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0211     |
| rollout/                |            |
|    ep_len_mean          | 54.4       |
|    ep_rew_mean          | -190       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 83         |
|    time_elapsed         | 409        |
|    total_timesteps      | 84992      |
| train/                  |            |
|    approx_kl            | 0.11812264 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.95       |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.365      |
|    value_loss           | 86.5       |
----------------------------------------
----------------------------------------
| reward                  | -3.56      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0235     |
| rollout/                |            |
|    ep_len_mean          | 64.2       |
|    ep_rew_mean          | -223       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 84         |
|    time_elapsed         | 414        |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.12485481 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.22       |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.136     |
|    std                  | 0.365      |
|    value_loss           | 104        |
----------------------------------------
----------------------------------------
| reward                  | -3.57      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0233     |
| rollout/                |            |
|    ep_len_mean          | 68.7       |
|    ep_rew_mean          | -239       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 85         |
|    time_elapsed         | 419        |
|    total_timesteps      | 87040      |
| train/                  |            |
|    approx_kl            | 0.12843837 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.838      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.5       |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.125     |
|    std                  | 0.365      |
|    value_loss           | 155        |
----------------------------------------
----------------------------------------
| reward                  | -3.54      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0226     |
| rollout/                |            |
|    ep_len_mean          | 78.3       |
|    ep_rew_mean          | -272       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 86         |
|    time_elapsed         | 424        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.15686947 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.73       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.94       |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.136     |
|    std                  | 0.365      |
|    value_loss           | 126        |
----------------------------------------
----------------------------------------
| reward                  | -3.55      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.57      |
| reward_velocity         | 0.0261     |
| rollout/                |            |
|    ep_len_mean          | 86.3       |
|    ep_rew_mean          | -299       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 87         |
|    time_elapsed         | 429        |
|    total_timesteps      | 89088      |
| train/                  |            |
|    approx_kl            | 0.14253086 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.655      |
|    learning_rate        | 0.0003     |
|    loss                 | 12.6       |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.365      |
|    value_loss           | 225        |
----------------------------------------
Num timesteps: 90000
Best mean reward: -179.34 - Last mean reward per episode: -265.84
----------------------------------------
| reward                  | -3.53      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0208     |
| rollout/                |            |
|    ep_len_mean          | 77.5       |
|    ep_rew_mean          | -269       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 88         |
|    time_elapsed         | 434        |
|    total_timesteps      | 90112      |
| train/                  |            |
|    approx_kl            | 0.09138085 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.94       |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.365      |
|    value_loss           | 233        |
----------------------------------------
----------------------------------------
| reward                  | -3.57      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0212     |
| rollout/                |            |
|    ep_len_mean          | 56.3       |
|    ep_rew_mean          | -196       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 89         |
|    time_elapsed         | 439        |
|    total_timesteps      | 91136      |
| train/                  |            |
|    approx_kl            | 0.09164302 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | -0.227     |
|    learning_rate        | 0.0003     |
|    loss                 | 10.6       |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.123     |
|    std                  | 0.365      |
|    value_loss           | 297        |
----------------------------------------
---------------------------------------
| reward                  | -3.56     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0         |
| reward_torque           | -3.58     |
| reward_velocity         | 0.0218    |
| rollout/                |           |
|    ep_len_mean          | 46.8      |
|    ep_rew_mean          | -163      |
| time/                   |           |
|    fps                  | 207       |
|    iterations           | 90        |
|    time_elapsed         | 444       |
|    total_timesteps      | 92160     |
| train/                  |           |
|    approx_kl            | 0.1434091 |
|    clip_fraction        | 0.263     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.229     |
|    learning_rate        | 0.0003    |
|    loss                 | 11.2      |
|    n_updates            | 1780      |
|    policy_gradient_loss | -0.143    |
|    std                  | 0.365     |
|    value_loss           | 233       |
---------------------------------------
---------------------------------------
| reward                  | -3.56     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0         |
| reward_torque           | -3.58     |
| reward_velocity         | 0.0235    |
| rollout/                |           |
|    ep_len_mean          | 38.7      |
|    ep_rew_mean          | -136      |
| time/                   |           |
|    fps                  | 207       |
|    iterations           | 91        |
|    time_elapsed         | 449       |
|    total_timesteps      | 93184     |
| train/                  |           |
|    approx_kl            | 0.1124434 |
|    clip_fraction        | 0.283     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.328     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.06      |
|    n_updates            | 1800      |
|    policy_gradient_loss | -0.13     |
|    std                  | 0.365     |
|    value_loss           | 138       |
---------------------------------------
----------------------------------------
| reward                  | -3.57      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0235     |
| rollout/                |            |
|    ep_len_mean          | 47.6       |
|    ep_rew_mean          | -166       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 92         |
|    time_elapsed         | 454        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.09183948 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.345      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.27       |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.132     |
|    std                  | 0.365      |
|    value_loss           | 215        |
----------------------------------------
----------------------------------------
| reward                  | -3.6       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.63      |
| reward_velocity         | 0.0239     |
| rollout/                |            |
|    ep_len_mean          | 47.8       |
|    ep_rew_mean          | -167       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 93         |
|    time_elapsed         | 459        |
|    total_timesteps      | 95232      |
| train/                  |            |
|    approx_kl            | 0.14662409 |
|    clip_fraction        | 0.348      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.677      |
|    learning_rate        | 0.0003     |
|    loss                 | 8.02       |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.15      |
|    std                  | 0.365      |
|    value_loss           | 108        |
----------------------------------------
Num timesteps: 96000
Best mean reward: -179.34 - Last mean reward per episode: -199.31
----------------------------------------
| reward                  | -3.56      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0          |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0296     |
| rollout/                |            |
|    ep_len_mean          | 56.6       |
|    ep_rew_mean          | -197       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 94         |
|    time_elapsed         | 463        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.14176369 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.691      |
|    learning_rate        | 0.0003     |
|    loss                 | 16.8       |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.13      |
|    std                  | 0.365      |
|    value_loss           | 218        |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_10
---------------------------------
| reward             | -3.47    |
| reward_contact     | 0        |
| reward_ctrl        | 0        |
| reward_motion      | -0.1     |
| reward_torque      | -3.38    |
| reward_velocity    | 0.0115   |
| rollout/           |          |
|    ep_len_mean     | 72.6     |
|    ep_rew_mean     | -221     |
| time/              |          |
|    fps             | 341      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | -3.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | -0.1       |
| reward_torque           | -3.43      |
| reward_velocity         | 0.0134     |
| rollout/                |            |
|    ep_len_mean          | 68.3       |
|    ep_rew_mean          | -208       |
| time/                   |            |
|    fps                  | 289        |
|    iterations           | 2          |
|    time_elapsed         | 7          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.12546113 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | -0.00252   |
|    learning_rate        | 0.0003     |
|    loss                 | 26.3       |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.135     |
|    std                  | 0.367      |
|    value_loss           | 252        |
----------------------------------------
----------------------------------------
| reward                  | -3.54      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | -0.1       |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0128     |
| rollout/                |            |
|    ep_len_mean          | 95.9       |
|    ep_rew_mean          | -291       |
| time/                   |            |
|    fps                  | 276        |
|    iterations           | 3          |
|    time_elapsed         | 11         |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.09592333 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.0003     |
|    loss                 | 13.9       |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0987    |
|    std                  | 0.367      |
|    value_loss           | 184        |
----------------------------------------
----------------------------------------
| reward                  | -3.56      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | -0.085     |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0165     |
| rollout/                |            |
|    ep_len_mean          | 113        |
|    ep_rew_mean          | -345       |
| time/                   |            |
|    fps                  | 270        |
|    iterations           | 4          |
|    time_elapsed         | 15         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.11307147 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.576      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.25       |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.115     |
|    std                  | 0.367      |
|    value_loss           | 93.3       |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_11
---------------------------------
| reward             | -3.16    |
| reward_contact     | 0        |
| reward_ctrl        | 0        |
| reward_motion      | -0.1     |
| reward_torque      | -3.1     |
| reward_velocity    | 0.0345   |
| rollout/           |          |
|    ep_len_mean     | 28.5     |
|    ep_rew_mean     | -79.8    |
| time/              |          |
|    fps             | 340      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 1024     |
---------------------------------
---------------------------------------
| reward                  | -3.33     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.023     |
| reward_torque           | -3.37     |
| reward_velocity         | 0.0217    |
| rollout/                |           |
|    ep_len_mean          | 65.9      |
|    ep_rew_mean          | -173      |
| time/                   |           |
|    fps                  | 290       |
|    iterations           | 2         |
|    time_elapsed         | 7         |
|    total_timesteps      | 2048      |
| train/                  |           |
|    approx_kl            | 0.1118205 |
|    clip_fraction        | 0.335     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.6     |
|    explained_variance   | 0.000973  |
|    learning_rate        | 0.0003    |
|    loss                 | 8.74      |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.152    |
|    std                  | 0.367     |
|    value_loss           | 164       |
---------------------------------------
-----------------------------------------
| reward                  | -3.4        |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0.00494     |
| reward_torque           | -3.42       |
| reward_velocity         | 0.0206      |
| rollout/                |             |
|    ep_len_mean          | 62.2        |
|    ep_rew_mean          | -164        |
| time/                   |             |
|    fps                  | 277         |
|    iterations           | 3           |
|    time_elapsed         | 11          |
|    total_timesteps      | 3072        |
| train/                  |             |
|    approx_kl            | 0.075688794 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | 0.0377      |
|    learning_rate        | 0.0003      |
|    loss                 | 27.6        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.112      |
|    std                  | 0.367       |
|    value_loss           | 122         |
-----------------------------------------
----------------------------------------
| reward                  | -3.38      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0133     |
| reward_torque           | -3.41      |
| reward_velocity         | 0.0242     |
| rollout/                |            |
|    ep_len_mean          | 88.3       |
|    ep_rew_mean          | -226       |
| time/                   |            |
|    fps                  | 271        |
|    iterations           | 4          |
|    time_elapsed         | 15         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.08545355 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.734      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.9       |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0891    |
|    std                  | 0.367      |
|    value_loss           | 105        |
----------------------------------------
----------------------------------------
| reward                  | -3.35      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0411     |
| reward_torque           | -3.41      |
| reward_velocity         | 0.0249     |
| rollout/                |            |
|    ep_len_mean          | 104        |
|    ep_rew_mean          | -262       |
| time/                   |            |
|    fps                  | 267        |
|    iterations           | 5          |
|    time_elapsed         | 19         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.06722508 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.534      |
|    learning_rate        | 0.0003     |
|    loss                 | 37.8       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0931    |
|    std                  | 0.367      |
|    value_loss           | 117        |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -229.45
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.37      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0272     |
| reward_torque           | -3.42      |
| reward_velocity         | 0.0237     |
| rollout/                |            |
|    ep_len_mean          | 87.7       |
|    ep_rew_mean          | -225       |
| time/                   |            |
|    fps                  | 264        |
|    iterations           | 6          |
|    time_elapsed         | 23         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.07324174 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.244      |
|    learning_rate        | 0.0003     |
|    loss                 | 18.4       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0859    |
|    std                  | 0.367      |
|    value_loss           | 91.2       |
----------------------------------------
----------------------------------------
| reward                  | -3.37      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0236     |
| reward_torque           | -3.41      |
| reward_velocity         | 0.0236     |
| rollout/                |            |
|    ep_len_mean          | 71.9       |
|    ep_rew_mean          | -188       |
| time/                   |            |
|    fps                  | 262        |
|    iterations           | 7          |
|    time_elapsed         | 27         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.06833801 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.056      |
|    learning_rate        | 0.0003     |
|    loss                 | 70.8       |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.367      |
|    value_loss           | 205        |
----------------------------------------
-----------------------------------------
| reward                  | -3.41       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0.0259      |
| reward_torque           | -3.45       |
| reward_velocity         | 0.0231      |
| rollout/                |             |
|    ep_len_mean          | 61          |
|    ep_rew_mean          | -161        |
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 8           |
|    time_elapsed         | 31          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.062687606 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.9       |
|    explained_variance   | 0.125       |
|    learning_rate        | 0.0003      |
|    loss                 | 87.9        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.107      |
|    std                  | 0.367       |
|    value_loss           | 263         |
-----------------------------------------
-----------------------------------------
| reward                  | -3.44       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0.0141      |
| reward_torque           | -3.47       |
| reward_velocity         | 0.0222      |
| rollout/                |             |
|    ep_len_mean          | 35.9        |
|    ep_rew_mean          | -103        |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 9           |
|    time_elapsed         | 35          |
|    total_timesteps      | 9216        |
| train/                  |             |
|    approx_kl            | 0.054853775 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | 0.238       |
|    learning_rate        | 0.0003      |
|    loss                 | 136         |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0993     |
|    std                  | 0.367       |
|    value_loss           | 283         |
-----------------------------------------
----------------------------------------
| reward                  | -3.42      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0141     |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0242     |
| rollout/                |            |
|    ep_len_mean          | 41.2       |
|    ep_rew_mean          | -111       |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 10         |
|    time_elapsed         | 39         |
|    total_timesteps      | 10240      |
| train/                  |            |
|    approx_kl            | 0.09887732 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.404      |
|    learning_rate        | 0.0003     |
|    loss                 | 19.7       |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.124     |
|    std                  | 0.367      |
|    value_loss           | 121        |
----------------------------------------
----------------------------------------
| reward                  | -3.43      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0143     |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0249     |
| rollout/                |            |
|    ep_len_mean          | 43.1       |
|    ep_rew_mean          | -117       |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 11         |
|    time_elapsed         | 43         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.13207608 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | -0.000791  |
|    learning_rate        | 0.0003     |
|    loss                 | 38.3       |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.131     |
|    std                  | 0.366      |
|    value_loss           | 182        |
----------------------------------------
Num timesteps: 12000
Best mean reward: -229.45 - Last mean reward per episode: -127.00
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -3.37      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0396     |
| reward_torque           | -3.44      |
| reward_velocity         | 0.0304     |
| rollout/                |            |
|    ep_len_mean          | 46.9       |
|    ep_rew_mean          | -127       |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 12         |
|    time_elapsed         | 47         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.07386387 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.661      |
|    learning_rate        | 0.0003     |
|    loss                 | 57.1       |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.366      |
|    value_loss           | 206        |
----------------------------------------
----------------------------------------
| reward                  | -3.4       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | -0.00293   |
| reward_torque           | -3.43      |
| reward_velocity         | 0.027      |
| rollout/                |            |
|    ep_len_mean          | 45.9       |
|    ep_rew_mean          | -125       |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 13         |
|    time_elapsed         | 51         |
|    total_timesteps      | 13312      |
| train/                  |            |
|    approx_kl            | 0.06611325 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.291      |
|    learning_rate        | 0.0003     |
|    loss                 | 74.6       |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.366      |
|    value_loss           | 280        |
----------------------------------------
----------------------------------------
| reward                  | -3.39      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0148     |
| reward_torque           | -3.43      |
| reward_velocity         | 0.0272     |
| rollout/                |            |
|    ep_len_mean          | 45.9       |
|    ep_rew_mean          | -126       |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 14         |
|    time_elapsed         | 55         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.09070608 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.318      |
|    learning_rate        | 0.0003     |
|    loss                 | 72.5       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.366      |
|    value_loss           | 196        |
----------------------------------------
----------------------------------------
| reward                  | -3.41      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0148     |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0275     |
| rollout/                |            |
|    ep_len_mean          | 46.8       |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 15         |
|    time_elapsed         | 59         |
|    total_timesteps      | 15360      |
| train/                  |            |
|    approx_kl            | 0.11738958 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.373      |
|    learning_rate        | 0.0003     |
|    loss                 | 13         |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.115     |
|    std                  | 0.366      |
|    value_loss           | 84         |
----------------------------------------
-----------------------------------------
| reward                  | -3.37       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0.0325      |
| reward_torque           | -3.43       |
| reward_velocity         | 0.0281      |
| rollout/                |             |
|    ep_len_mean          | 50.2        |
|    ep_rew_mean          | -134        |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 16          |
|    time_elapsed         | 63          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.120515704 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.388       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.8        |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.125      |
|    std                  | 0.366       |
|    value_loss           | 108         |
-----------------------------------------
-----------------------------------------
| reward                  | -3.4        |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0.0321      |
| reward_torque           | -3.46       |
| reward_velocity         | 0.0262      |
| rollout/                |             |
|    ep_len_mean          | 51.5        |
|    ep_rew_mean          | -138        |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 17          |
|    time_elapsed         | 67          |
|    total_timesteps      | 17408       |
| train/                  |             |
|    approx_kl            | 0.114222154 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.7       |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.6        |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.144      |
|    std                  | 0.366       |
|    value_loss           | 129         |
-----------------------------------------
Num timesteps: 18000
Best mean reward: -127.00 - Last mean reward per episode: -156.47
-----------------------------------------
| reward                  | -3.42       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0.0513      |
| reward_torque           | -3.5        |
| reward_velocity         | 0.0262      |
| rollout/                |             |
|    ep_len_mean          | 59.8        |
|    ep_rew_mean          | -156        |
| time/                   |             |
|    fps                  | 256         |
|    iterations           | 18          |
|    time_elapsed         | 71          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.123548895 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.162       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.85        |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.138      |
|    std                  | 0.366       |
|    value_loss           | 73.4        |
-----------------------------------------
----------------------------------------
| reward                  | -3.43      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0232     |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0311     |
| rollout/                |            |
|    ep_len_mean          | 68         |
|    ep_rew_mean          | -174       |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 19         |
|    time_elapsed         | 75         |
|    total_timesteps      | 19456      |
| train/                  |            |
|    approx_kl            | 0.09727553 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.751      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.42       |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.366      |
|    value_loss           | 73.6       |
----------------------------------------
----------------------------------------
| reward                  | -3.41      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.0449     |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0317     |
| rollout/                |            |
|    ep_len_mean          | 73.9       |
|    ep_rew_mean          | -187       |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 20         |
|    time_elapsed         | 79         |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.10544833 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.46       |
|    learning_rate        | 0.0003     |
|    loss                 | 24.5       |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.366      |
|    value_loss           | 124        |
----------------------------------------
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
Using cuda device
Logging to rl/out_dir/models/exp74/PPO_12
---------------------------------
| reward             | -2.43    |
| reward_contact     | 0        |
| reward_ctrl        | 0        |
| reward_motion      | 1.02     |
| reward_torque      | -3.47    |
| reward_velocity    | 0.0149   |
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | -226     |
| time/              |          |
|    fps             | 342      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
----------------------------------------
| reward                  | -2.41      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.991      |
| reward_torque           | -3.41      |
| reward_velocity         | 0.0178     |
| rollout/                |            |
|    ep_len_mean          | 98         |
|    ep_rew_mean          | -200       |
| time/                   |            |
|    fps                  | 291        |
|    iterations           | 2          |
|    time_elapsed         | 7          |
|    total_timesteps      | 2048       |
| train/                  |            |
|    approx_kl            | 0.10207515 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.0184     |
|    learning_rate        | 0.0003     |
|    loss                 | 12.4       |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.135     |
|    std                  | 0.367      |
|    value_loss           | 156        |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.945      |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0242     |
| rollout/                |            |
|    ep_len_mean          | 79.3       |
|    ep_rew_mean          | -163       |
| time/                   |            |
|    fps                  | 278        |
|    iterations           | 3          |
|    time_elapsed         | 11         |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.11764982 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | -0.162     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.1        |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.367      |
|    value_loss           | 47.3       |
----------------------------------------
----------------------------------------
| reward                  | -2.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.932      |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0194     |
| rollout/                |            |
|    ep_len_mean          | 73.4       |
|    ep_rew_mean          | -150       |
| time/                   |            |
|    fps                  | 272        |
|    iterations           | 4          |
|    time_elapsed         | 15         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.12323756 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | -0.145     |
|    learning_rate        | 0.0003     |
|    loss                 | 21.7       |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.131     |
|    std                  | 0.367      |
|    value_loss           | 64         |
----------------------------------------
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.934      |
| reward_torque           | -3.46      |
| reward_velocity         | 0.021      |
| rollout/                |            |
|    ep_len_mean          | 64.3       |
|    ep_rew_mean          | -132       |
| time/                   |            |
|    fps                  | 268        |
|    iterations           | 5          |
|    time_elapsed         | 19         |
|    total_timesteps      | 5120       |
| train/                  |            |
|    approx_kl            | 0.09229784 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.135      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.83       |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.367      |
|    value_loss           | 50.8       |
----------------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -131.76
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.934      |
| reward_torque           | -3.46      |
| reward_velocity         | 0.021      |
| rollout/                |            |
|    ep_len_mean          | 76.3       |
|    ep_rew_mean          | -155       |
| time/                   |            |
|    fps                  | 265        |
|    iterations           | 6          |
|    time_elapsed         | 23         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.07450648 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.0812     |
|    learning_rate        | 0.0003     |
|    loss                 | 33.9       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.367      |
|    value_loss           | 89.2       |
----------------------------------------
----------------------------------------
| reward                  | -2.49      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.947      |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0201     |
| rollout/                |            |
|    ep_len_mean          | 71.5       |
|    ep_rew_mean          | -146       |
| time/                   |            |
|    fps                  | 264        |
|    iterations           | 7          |
|    time_elapsed         | 27         |
|    total_timesteps      | 7168       |
| train/                  |            |
|    approx_kl            | 0.11284697 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.289      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.51       |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.367      |
|    value_loss           | 28.3       |
----------------------------------------
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.949      |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0207     |
| rollout/                |            |
|    ep_len_mean          | 79.3       |
|    ep_rew_mean          | -161       |
| time/                   |            |
|    fps                  | 262        |
|    iterations           | 8          |
|    time_elapsed         | 31         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.07660948 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | -0.132     |
|    learning_rate        | 0.0003     |
|    loss                 | 17         |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.102     |
|    std                  | 0.367      |
|    value_loss           | 78.6       |
----------------------------------------
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.953      |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0243     |
| rollout/                |            |
|    ep_len_mean          | 87.5       |
|    ep_rew_mean          | -177       |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 9          |
|    time_elapsed         | 35         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.07484859 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.895      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.9       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.107     |
|    std                  | 0.367      |
|    value_loss           | 80.9       |
----------------------------------------
---------------------------------------
| reward                  | -2.5      |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.943     |
| reward_torque           | -3.47     |
| reward_velocity         | 0.027     |
| rollout/                |           |
|    ep_len_mean          | 90.3      |
|    ep_rew_mean          | -183      |
| time/                   |           |
|    fps                  | 261       |
|    iterations           | 10        |
|    time_elapsed         | 39        |
|    total_timesteps      | 10240     |
| train/                  |           |
|    approx_kl            | 0.0823557 |
|    clip_fraction        | 0.205     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.802     |
|    learning_rate        | 0.0003    |
|    loss                 | 10.1      |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.101    |
|    std                  | 0.366     |
|    value_loss           | 76.1      |
---------------------------------------
----------------------------------------
| reward                  | -2.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.944      |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0253     |
| rollout/                |            |
|    ep_len_mean          | 83.7       |
|    ep_rew_mean          | -170       |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 11         |
|    time_elapsed         | 43         |
|    total_timesteps      | 11264      |
| train/                  |            |
|    approx_kl            | 0.11638403 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.44       |
|    learning_rate        | 0.0003     |
|    loss                 | 14.7       |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.12      |
|    std                  | 0.366      |
|    value_loss           | 70.1       |
----------------------------------------
Num timesteps: 12000
Best mean reward: -131.76 - Last mean reward per episode: -170.43
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.954      |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0255     |
| rollout/                |            |
|    ep_len_mean          | 89.2       |
|    ep_rew_mean          | -182       |
| time/                   |            |
|    fps                  | 260        |
|    iterations           | 12         |
|    time_elapsed         | 47         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.07480501 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.27       |
|    learning_rate        | 0.0003     |
|    loss                 | 33.6       |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.366      |
|    value_loss           | 159        |
----------------------------------------
---------------------------------------
| reward                  | -2.52     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.954     |
| reward_torque           | -3.5      |
| reward_velocity         | 0.0265    |
| rollout/                |           |
|    ep_len_mean          | 96.4      |
|    ep_rew_mean          | -197      |
| time/                   |           |
|    fps                  | 259       |
|    iterations           | 13        |
|    time_elapsed         | 51        |
|    total_timesteps      | 13312     |
| train/                  |           |
|    approx_kl            | 0.0688664 |
|    clip_fraction        | 0.164     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22       |
|    explained_variance   | 0.829     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.11      |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.0863   |
|    std                  | 0.366     |
|    value_loss           | 85.3      |
---------------------------------------
----------------------------------------
| reward                  | -2.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.957      |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0247     |
| rollout/                |            |
|    ep_len_mean          | 90.3       |
|    ep_rew_mean          | -186       |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 14         |
|    time_elapsed         | 55         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.11092051 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.479      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.2       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.121     |
|    std                  | 0.366      |
|    value_loss           | 78.8       |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.962      |
| reward_torque           | -3.51      |
| reward_velocity         | 0.028      |
| rollout/                |            |
|    ep_len_mean          | 89.9       |
|    ep_rew_mean          | -186       |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 15         |
|    time_elapsed         | 59         |
|    total_timesteps      | 15360      |
| train/                  |            |
|    approx_kl            | 0.11095235 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.162      |
|    learning_rate        | 0.0003     |
|    loss                 | 19.6       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.142     |
|    std                  | 0.366      |
|    value_loss           | 129        |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.956      |
| reward_torque           | -3.5       |
| reward_velocity         | 0.0284     |
| rollout/                |            |
|    ep_len_mean          | 87         |
|    ep_rew_mean          | -181       |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 16         |
|    time_elapsed         | 63         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.11190578 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.2      |
|    explained_variance   | 0.579      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.38       |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.126     |
|    std                  | 0.366      |
|    value_loss           | 67.7       |
----------------------------------------
----------------------------------------
| reward                  | -2.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.961      |
| reward_torque           | -3.5       |
| reward_velocity         | 0.0276     |
| rollout/                |            |
|    ep_len_mean          | 87.3       |
|    ep_rew_mean          | -183       |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 17         |
|    time_elapsed         | 67         |
|    total_timesteps      | 17408      |
| train/                  |            |
|    approx_kl            | 0.10090847 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.702      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.68       |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.118     |
|    std                  | 0.366      |
|    value_loss           | 50.7       |
----------------------------------------
Num timesteps: 18000
Best mean reward: -131.76 - Last mean reward per episode: -162.58
----------------------------------------
| reward                  | -2.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.961      |
| reward_torque           | -3.49      |
| reward_velocity         | 0.0247     |
| rollout/                |            |
|    ep_len_mean          | 76.7       |
|    ep_rew_mean          | -163       |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 18         |
|    time_elapsed         | 71         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.11514368 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.92       |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.366      |
|    value_loss           | 56         |
----------------------------------------
-----------------------------------------
| reward                  | -2.49       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0.964       |
| reward_torque           | -3.48       |
| reward_velocity         | 0.0254      |
| rollout/                |             |
|    ep_len_mean          | 83.5        |
|    ep_rew_mean          | -176        |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 19          |
|    time_elapsed         | 75          |
|    total_timesteps      | 19456       |
| train/                  |             |
|    approx_kl            | 0.100146875 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.9       |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.6         |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.113      |
|    std                  | 0.366       |
|    value_loss           | 71.2        |
-----------------------------------------
----------------------------------------
| reward                  | -2.48      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.957      |
| reward_torque           | -3.46      |
| reward_velocity         | 0.028      |
| rollout/                |            |
|    ep_len_mean          | 82.6       |
|    ep_rew_mean          | -175       |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 20         |
|    time_elapsed         | 79         |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.10226883 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.481      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.01       |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.365      |
|    value_loss           | 58.2       |
----------------------------------------
----------------------------------------
| reward                  | -2.49      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.942      |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0265     |
| rollout/                |            |
|    ep_len_mean          | 76.2       |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 254        |
|    iterations           | 21         |
|    time_elapsed         | 84         |
|    total_timesteps      | 21504      |
| train/                  |            |
|    approx_kl            | 0.14570352 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.482      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.69       |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.365      |
|    value_loss           | 48.1       |
----------------------------------------
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.931      |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0255     |
| rollout/                |            |
|    ep_len_mean          | 68.1       |
|    ep_rew_mean          | -145       |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 22         |
|    time_elapsed         | 89         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.08297723 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | -0.133     |
|    learning_rate        | 0.0003     |
|    loss                 | 13         |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.365      |
|    value_loss           | 86.1       |
----------------------------------------
-----------------------------------------
| reward                  | -2.51       |
| reward_contact          | 0           |
| reward_ctrl             | 0           |
| reward_motion           | 0.923       |
| reward_torque           | -3.45       |
| reward_velocity         | 0.0242      |
| rollout/                |             |
|    ep_len_mean          | 74.5        |
|    ep_rew_mean          | -158        |
| time/                   |             |
|    fps                  | 248         |
|    iterations           | 23          |
|    time_elapsed         | 94          |
|    total_timesteps      | 23552       |
| train/                  |             |
|    approx_kl            | 0.107540056 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.31        |
|    learning_rate        | 0.0003      |
|    loss                 | 8.13        |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.113      |
|    std                  | 0.365       |
|    value_loss           | 72          |
-----------------------------------------
Num timesteps: 24000
Best mean reward: -131.76 - Last mean reward per episode: -143.16
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.926      |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0219     |
| rollout/                |            |
|    ep_len_mean          | 67.8       |
|    ep_rew_mean          | -145       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 24         |
|    time_elapsed         | 99         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.14310622 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.512      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.33       |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.118     |
|    std                  | 0.365      |
|    value_loss           | 54.6       |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.93       |
| reward_torque           | -3.47      |
| reward_velocity         | 0.022      |
| rollout/                |            |
|    ep_len_mean          | 67         |
|    ep_rew_mean          | -144       |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 25         |
|    time_elapsed         | 104        |
|    total_timesteps      | 25600      |
| train/                  |            |
|    approx_kl            | 0.11249768 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.26       |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.131     |
|    std                  | 0.365      |
|    value_loss           | 70.6       |
----------------------------------------
----------------------------------------
| reward                  | -2.53      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.952      |
| reward_torque           | -3.5       |
| reward_velocity         | 0.0208     |
| rollout/                |            |
|    ep_len_mean          | 60.4       |
|    ep_rew_mean          | -131       |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 26         |
|    time_elapsed         | 109        |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.18206921 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.654      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.46       |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.15      |
|    std                  | 0.365      |
|    value_loss           | 72.7       |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.943      |
| reward_torque           | -3.48      |
| reward_velocity         | 0.0202     |
| rollout/                |            |
|    ep_len_mean          | 61         |
|    ep_rew_mean          | -132       |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 27         |
|    time_elapsed         | 113        |
|    total_timesteps      | 27648      |
| train/                  |            |
|    approx_kl            | 0.17379266 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | -0.0549    |
|    learning_rate        | 0.0003     |
|    loss                 | 7.01       |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.16      |
|    std                  | 0.365      |
|    value_loss           | 70.3       |
----------------------------------------
----------------------------------------
| reward                  | -2.49      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.95       |
| reward_torque           | -3.46      |
| reward_velocity         | 0.0204     |
| rollout/                |            |
|    ep_len_mean          | 71.2       |
|    ep_rew_mean          | -153       |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 28         |
|    time_elapsed         | 118        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.13648833 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.65       |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.14      |
|    std                  | 0.365      |
|    value_loss           | 38.1       |
----------------------------------------
----------------------------------------
| reward                  | -2.5       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.95       |
| reward_torque           | -3.47      |
| reward_velocity         | 0.0204     |
| rollout/                |            |
|    ep_len_mean          | 77.7       |
|    ep_rew_mean          | -166       |
| time/                   |            |
|    fps                  | 241        |
|    iterations           | 29         |
|    time_elapsed         | 123        |
|    total_timesteps      | 29696      |
| train/                  |            |
|    approx_kl            | 0.10979601 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -22.1      |
|    explained_variance   | 0.726      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.47       |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.127     |
|    std                  | 0.365      |
|    value_loss           | 31.5       |
----------------------------------------
Num timesteps: 30000
Best mean reward: -131.76 - Last mean reward per episode: -188.92
---------------------------------------
| reward                  | -2.5      |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.982     |
| reward_torque           | -3.51     |
| reward_velocity         | 0.0255    |
| rollout/                |           |
|    ep_len_mean          | 79.4      |
|    ep_rew_mean          | -170      |
| time/                   |           |
|    fps                  | 240       |
|    iterations           | 30        |
|    time_elapsed         | 127       |
|    total_timesteps      | 30720     |
| train/                  |           |
|    approx_kl            | 0.0853755 |
|    clip_fraction        | 0.219     |
|    clip_range           | 0.4       |
|    entropy_loss         | -22.1     |
|    explained_variance   | 0.796     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.91      |
|    n_updates            | 580       |
|    policy_gradient_loss | -0.111    |
|    std                  | 0.365     |
|    value_loss           | 49.5      |
---------------------------------------
---------------------------------------
| reward                  | -2.54     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.96      |
| reward_torque           | -3.52     |
| reward_velocity         | 0.022     |
| rollout/                |           |
|    ep_len_mean          | 63.1      |
|    ep_rew_mean          | -136      |
| time/                   |           |
|    fps                  | 239       |
|    iterations           | 31        |
|    time_elapsed         | 132       |
|    total_timesteps      | 31744     |
| train/                  |           |
|    approx_kl            | 0.0878329 |
|    clip_fraction        | 0.219     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.2     |
|    explained_variance   | 0.17      |
|    learning_rate        | 0.0003    |
|    loss                 | 11.7      |
|    n_updates            | 600       |
|    policy_gradient_loss | -0.133    |
|    std                  | 0.365     |
|    value_loss           | 117       |
---------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.939      |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0201     |
| rollout/                |            |
|    ep_len_mean          | 62         |
|    ep_rew_mean          | -134       |
| time/                   |            |
|    fps                  | 238        |
|    iterations           | 32         |
|    time_elapsed         | 137        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.21021704 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.47       |
|    learning_rate        | 0.0003     |
|    loss                 | 3          |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.169     |
|    std                  | 0.365      |
|    value_loss           | 64.6       |
----------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.93       |
| reward_torque           | -3.53      |
| reward_velocity         | 0.0182     |
| rollout/                |            |
|    ep_len_mean          | 36.2       |
|    ep_rew_mean          | -82.1      |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 33         |
|    time_elapsed         | 142        |
|    total_timesteps      | 33792      |
| train/                  |            |
|    approx_kl            | 0.09699252 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | -0.0219    |
|    learning_rate        | 0.0003     |
|    loss                 | 8.91       |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.136     |
|    std                  | 0.365      |
|    value_loss           | 94.7       |
----------------------------------------
---------------------------------------
| reward                  | -2.63     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.928     |
| reward_torque           | -3.57     |
| reward_velocity         | 0.0189    |
| rollout/                |           |
|    ep_len_mean          | 44.5      |
|    ep_rew_mean          | -99.2     |
| time/                   |           |
|    fps                  | 235       |
|    iterations           | 34        |
|    time_elapsed         | 147       |
|    total_timesteps      | 34816     |
| train/                  |           |
|    approx_kl            | 0.1271945 |
|    clip_fraction        | 0.315     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | 0.0598    |
|    learning_rate        | 0.0003    |
|    loss                 | 6.57      |
|    n_updates            | 660       |
|    policy_gradient_loss | -0.145    |
|    std                  | 0.364     |
|    value_loss           | 83.9      |
---------------------------------------
---------------------------------------
| reward                  | -2.65     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.918     |
| reward_torque           | -3.59     |
| reward_velocity         | 0.0164    |
| rollout/                |           |
|    ep_len_mean          | 39.1      |
|    ep_rew_mean          | -87.8     |
| time/                   |           |
|    fps                  | 235       |
|    iterations           | 35        |
|    time_elapsed         | 152       |
|    total_timesteps      | 35840     |
| train/                  |           |
|    approx_kl            | 0.1630308 |
|    clip_fraction        | 0.347     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.7     |
|    explained_variance   | -0.283    |
|    learning_rate        | 0.0003    |
|    loss                 | 3.58      |
|    n_updates            | 680       |
|    policy_gradient_loss | -0.157    |
|    std                  | 0.364     |
|    value_loss           | 66.1      |
---------------------------------------
Num timesteps: 36000
Best mean reward: -131.76 - Last mean reward per episode: -87.77
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------------
| reward                  | -2.6       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.937      |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0158     |
| rollout/                |            |
|    ep_len_mean          | 48.9       |
|    ep_rew_mean          | -108       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 36         |
|    time_elapsed         | 157        |
|    total_timesteps      | 36864      |
| train/                  |            |
|    approx_kl            | 0.20378548 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.8      |
|    explained_variance   | 0.639      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.17       |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.16      |
|    std                  | 0.364      |
|    value_loss           | 37.4       |
----------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.937      |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0152     |
| rollout/                |            |
|    ep_len_mean          | 48.9       |
|    ep_rew_mean          | -108       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 37         |
|    time_elapsed         | 162        |
|    total_timesteps      | 37888      |
| train/                  |            |
|    approx_kl            | 0.15472117 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.651      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.62       |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.152     |
|    std                  | 0.364      |
|    value_loss           | 79.4       |
----------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.942      |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0153     |
| rollout/                |            |
|    ep_len_mean          | 57.9       |
|    ep_rew_mean          | -125       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 38         |
|    time_elapsed         | 167        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.08284654 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.9      |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.31       |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.128     |
|    std                  | 0.364      |
|    value_loss           | 64.4       |
----------------------------------------
----------------------------------------
| reward                  | -2.59      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.975      |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0166     |
| rollout/                |            |
|    ep_len_mean          | 65.7       |
|    ep_rew_mean          | -141       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 39         |
|    time_elapsed         | 172        |
|    total_timesteps      | 39936      |
| train/                  |            |
|    approx_kl            | 0.16583937 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.708      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.49       |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.153     |
|    std                  | 0.364      |
|    value_loss           | 53.5       |
----------------------------------------
---------------------------------------
| reward                  | -2.6      |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.982     |
| reward_torque           | -3.6      |
| reward_velocity         | 0.0172    |
| rollout/                |           |
|    ep_len_mean          | 76.1      |
|    ep_rew_mean          | -163      |
| time/                   |           |
|    fps                  | 230       |
|    iterations           | 40        |
|    time_elapsed         | 177       |
|    total_timesteps      | 40960     |
| train/                  |           |
|    approx_kl            | 0.1525042 |
|    clip_fraction        | 0.33      |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.5     |
|    explained_variance   | 0.68      |
|    learning_rate        | 0.0003    |
|    loss                 | 7.26      |
|    n_updates            | 780       |
|    policy_gradient_loss | -0.154    |
|    std                  | 0.364     |
|    value_loss           | 84.2      |
---------------------------------------
----------------------------------------
| reward                  | -2.57      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.963      |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0132     |
| rollout/                |            |
|    ep_len_mean          | 57.1       |
|    ep_rew_mean          | -124       |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 41         |
|    time_elapsed         | 182        |
|    total_timesteps      | 41984      |
| train/                  |            |
|    approx_kl            | 0.34836543 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.82       |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.133     |
|    std                  | 0.364      |
|    value_loss           | 28.8       |
----------------------------------------
Num timesteps: 42000
Best mean reward: -87.77 - Last mean reward per episode: -124.21
----------------------------------------
| reward                  | -2.59      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.939      |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0132     |
| rollout/                |            |
|    ep_len_mean          | 48         |
|    ep_rew_mean          | -107       |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 42         |
|    time_elapsed         | 187        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.11095698 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.0388     |
|    learning_rate        | 0.0003     |
|    loss                 | 7.94       |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.148     |
|    std                  | 0.364      |
|    value_loss           | 122        |
----------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.943      |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0129     |
| rollout/                |            |
|    ep_len_mean          | 38.2       |
|    ep_rew_mean          | -86.4      |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 43         |
|    time_elapsed         | 192        |
|    total_timesteps      | 44032      |
| train/                  |            |
|    approx_kl            | 0.13068032 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.199      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.72       |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.364      |
|    value_loss           | 104        |
----------------------------------------
----------------------------------------
| reward                  | -2.56      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.941      |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0167     |
| rollout/                |            |
|    ep_len_mean          | 47.4       |
|    ep_rew_mean          | -105       |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 44         |
|    time_elapsed         | 197        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.15542763 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.759      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.08       |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.364      |
|    value_loss           | 31.7       |
----------------------------------------
----------------------------------------
| reward                  | -2.51      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.92       |
| reward_torque           | -3.45      |
| reward_velocity         | 0.0171     |
| rollout/                |            |
|    ep_len_mean          | 45.8       |
|    ep_rew_mean          | -100       |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 45         |
|    time_elapsed         | 201        |
|    total_timesteps      | 46080      |
| train/                  |            |
|    approx_kl            | 0.17269671 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.798      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.47       |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.364      |
|    value_loss           | 47.2       |
----------------------------------------
---------------------------------------
| reward                  | -2.56     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.93      |
| reward_torque           | -3.51     |
| reward_velocity         | 0.0194    |
| rollout/                |           |
|    ep_len_mean          | 43.1      |
|    ep_rew_mean          | -94.2     |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 46        |
|    time_elapsed         | 206       |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.2158814 |
|    clip_fraction        | 0.421     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.818     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.49      |
|    n_updates            | 900       |
|    policy_gradient_loss | -0.18     |
|    std                  | 0.364     |
|    value_loss           | 70.5      |
---------------------------------------
Num timesteps: 48000
Best mean reward: -87.77 - Last mean reward per episode: -92.36
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.93       |
| reward_torque           | -3.53      |
| reward_velocity         | 0.0191     |
| rollout/                |            |
|    ep_len_mean          | 42.3       |
|    ep_rew_mean          | -92.4      |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 47         |
|    time_elapsed         | 211        |
|    total_timesteps      | 48128      |
| train/                  |            |
|    approx_kl            | 0.11759651 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | -0.115     |
|    learning_rate        | 0.0003     |
|    loss                 | 9.29       |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.157     |
|    std                  | 0.364      |
|    value_loss           | 108        |
----------------------------------------
----------------------------------------
| reward                  | -2.6       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.936      |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0176     |
| rollout/                |            |
|    ep_len_mean          | 32         |
|    ep_rew_mean          | -72.4      |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 48         |
|    time_elapsed         | 216        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.13015762 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.486      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.18       |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.147     |
|    std                  | 0.364      |
|    value_loss           | 43.9       |
----------------------------------------
---------------------------------------
| reward                  | -2.63     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.941     |
| reward_torque           | -3.59     |
| reward_velocity         | 0.02      |
| rollout/                |           |
|    ep_len_mean          | 43        |
|    ep_rew_mean          | -95.7     |
| time/                   |           |
|    fps                  | 226       |
|    iterations           | 49        |
|    time_elapsed         | 221       |
|    total_timesteps      | 50176     |
| train/                  |           |
|    approx_kl            | 0.1601876 |
|    clip_fraction        | 0.326     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.759     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.43      |
|    n_updates            | 960       |
|    policy_gradient_loss | -0.161    |
|    std                  | 0.364     |
|    value_loss           | 75        |
---------------------------------------
----------------------------------------
| reward                  | -2.64      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.941      |
| reward_torque           | -3.6       |
| reward_velocity         | 0.0201     |
| rollout/                |            |
|    ep_len_mean          | 43.5       |
|    ep_rew_mean          | -96.6      |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 50         |
|    time_elapsed         | 226        |
|    total_timesteps      | 51200      |
| train/                  |            |
|    approx_kl            | 0.33444732 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.886      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.43       |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.364      |
|    value_loss           | 72.7       |
----------------------------------------
----------------------------------------
| reward                  | -2.64      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.934      |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0204     |
| rollout/                |            |
|    ep_len_mean          | 53.7       |
|    ep_rew_mean          | -118       |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 51         |
|    time_elapsed         | 231        |
|    total_timesteps      | 52224      |
| train/                  |            |
|    approx_kl            | 0.16293636 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.81       |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.364      |
|    value_loss           | 72         |
----------------------------------------
----------------------------------------
| reward                  | -2.64      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.934      |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0208     |
| rollout/                |            |
|    ep_len_mean          | 63.8       |
|    ep_rew_mean          | -139       |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 52         |
|    time_elapsed         | 237        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.19210586 |
|    clip_fraction        | 0.419      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.39       |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.192     |
|    std                  | 0.364      |
|    value_loss           | 81         |
----------------------------------------
Num timesteps: 54000
Best mean reward: -87.77 - Last mean reward per episode: -159.14
----------------------------------------
| reward                  | -2.62      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.937      |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0247     |
| rollout/                |            |
|    ep_len_mean          | 73.5       |
|    ep_rew_mean          | -158       |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 53         |
|    time_elapsed         | 242        |
|    total_timesteps      | 54272      |
| train/                  |            |
|    approx_kl            | 0.14484899 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.51       |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.143     |
|    std                  | 0.363      |
|    value_loss           | 45.8       |
----------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.937      |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0261     |
| rollout/                |            |
|    ep_len_mean          | 83.7       |
|    ep_rew_mean          | -179       |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 54         |
|    time_elapsed         | 246        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.15225911 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.894      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.3        |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.161     |
|    std                  | 0.363      |
|    value_loss           | 69.9       |
----------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.911      |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0235     |
| rollout/                |            |
|    ep_len_mean          | 73.8       |
|    ep_rew_mean          | -159       |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 55         |
|    time_elapsed         | 251        |
|    total_timesteps      | 56320      |
| train/                  |            |
|    approx_kl            | 0.29744816 |
|    clip_fraction        | 0.469      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.594      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.43       |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.168     |
|    std                  | 0.363      |
|    value_loss           | 48.3       |
----------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.913      |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0237     |
| rollout/                |            |
|    ep_len_mean          | 73         |
|    ep_rew_mean          | -157       |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 56         |
|    time_elapsed         | 256        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.15328516 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.233      |
|    learning_rate        | 0.0003     |
|    loss                 | 5.02       |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.363      |
|    value_loss           | 88.7       |
----------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.913      |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0237     |
| rollout/                |            |
|    ep_len_mean          | 82.5       |
|    ep_rew_mean          | -176       |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 57         |
|    time_elapsed         | 261        |
|    total_timesteps      | 58368      |
| train/                  |            |
|    approx_kl            | 0.18133706 |
|    clip_fraction        | 0.358      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.6      |
|    explained_variance   | 0.36       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.37       |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.363      |
|    value_loss           | 81.5       |
----------------------------------------
----------------------------------------
| reward                  | -2.59      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.932      |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0247     |
| rollout/                |            |
|    ep_len_mean          | 83.6       |
|    ep_rew_mean          | -179       |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 58         |
|    time_elapsed         | 266        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.14764038 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.7      |
|    explained_variance   | 0.461      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.12       |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.363      |
|    value_loss           | 40.8       |
----------------------------------------
Num timesteps: 60000
Best mean reward: -87.77 - Last mean reward per episode: -183.77
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.939      |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0217     |
| rollout/                |            |
|    ep_len_mean          | 85.2       |
|    ep_rew_mean          | -184       |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 59         |
|    time_elapsed         | 271        |
|    total_timesteps      | 60416      |
| train/                  |            |
|    approx_kl            | 0.13345909 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.57       |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.147     |
|    std                  | 0.363      |
|    value_loss           | 67.9       |
----------------------------------------
----------------------------------------
| reward                  | -2.54      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.953      |
| reward_torque           | -3.51      |
| reward_velocity         | 0.018      |
| rollout/                |            |
|    ep_len_mean          | 75         |
|    ep_rew_mean          | -163       |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 60         |
|    time_elapsed         | 276        |
|    total_timesteps      | 61440      |
| train/                  |            |
|    approx_kl            | 0.20082566 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.585      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.42       |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.363      |
|    value_loss           | 81.8       |
----------------------------------------
---------------------------------------
| reward                  | -2.53     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.961     |
| reward_torque           | -3.51     |
| reward_velocity         | 0.0191    |
| rollout/                |           |
|    ep_len_mean          | 84.5      |
|    ep_rew_mean          | -182      |
| time/                   |           |
|    fps                  | 222       |
|    iterations           | 61        |
|    time_elapsed         | 280       |
|    total_timesteps      | 62464     |
| train/                  |           |
|    approx_kl            | 0.1963744 |
|    clip_fraction        | 0.384     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.4     |
|    explained_variance   | 0.173     |
|    learning_rate        | 0.0003    |
|    loss                 | 3.02      |
|    n_updates            | 1200      |
|    policy_gradient_loss | -0.172    |
|    std                  | 0.363     |
|    value_loss           | 70.1      |
---------------------------------------
---------------------------------------
| reward                  | -2.53     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.971     |
| reward_torque           | -3.52     |
| reward_velocity         | 0.0211    |
| rollout/                |           |
|    ep_len_mean          | 75.9      |
|    ep_rew_mean          | -166      |
| time/                   |           |
|    fps                  | 222       |
|    iterations           | 62        |
|    time_elapsed         | 285       |
|    total_timesteps      | 63488     |
| train/                  |           |
|    approx_kl            | 0.2019859 |
|    clip_fraction        | 0.349     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.3     |
|    explained_variance   | 0.757     |
|    learning_rate        | 0.0003    |
|    loss                 | 4.61      |
|    n_updates            | 1220      |
|    policy_gradient_loss | -0.155    |
|    std                  | 0.363     |
|    value_loss           | 68.7      |
---------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.971      |
| reward_torque           | -3.51      |
| reward_velocity         | 0.0206     |
| rollout/                |            |
|    ep_len_mean          | 81.5       |
|    ep_rew_mean          | -178       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 63         |
|    time_elapsed         | 290        |
|    total_timesteps      | 64512      |
| train/                  |            |
|    approx_kl            | 0.20065655 |
|    clip_fraction        | 0.402      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.204      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.83       |
|    n_updates            | 1240       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.363      |
|    value_loss           | 88.5       |
----------------------------------------
----------------------------------------
| reward                  | -2.52      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.982      |
| reward_torque           | -3.52      |
| reward_velocity         | 0.0224     |
| rollout/                |            |
|    ep_len_mean          | 91.9       |
|    ep_rew_mean          | -199       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 64         |
|    time_elapsed         | 295        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.17107004 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.767      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.99       |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.363      |
|    value_loss           | 49.7       |
----------------------------------------
Num timesteps: 66000
Best mean reward: -87.77 - Last mean reward per episode: -199.10
----------------------------------------
| reward                  | -2.53      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.98       |
| reward_torque           | -3.54      |
| reward_velocity         | 0.0223     |
| rollout/                |            |
|    ep_len_mean          | 91.5       |
|    ep_rew_mean          | -197       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 65         |
|    time_elapsed         | 300        |
|    total_timesteps      | 66560      |
| train/                  |            |
|    approx_kl            | 0.20050028 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.558      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.81       |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.363      |
|    value_loss           | 37.6       |
----------------------------------------
----------------------------------------
| reward                  | -2.56      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.973      |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0261     |
| rollout/                |            |
|    ep_len_mean          | 91.1       |
|    ep_rew_mean          | -196       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 66         |
|    time_elapsed         | 304        |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.28185272 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.0769     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.32       |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.363      |
|    value_loss           | 51.3       |
----------------------------------------
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.952      |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0256     |
| rollout/                |            |
|    ep_len_mean          | 80.8       |
|    ep_rew_mean          | -174       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 67         |
|    time_elapsed         | 309        |
|    total_timesteps      | 68608      |
| train/                  |            |
|    approx_kl            | 0.30631936 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.028      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.59       |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.363      |
|    value_loss           | 52.2       |
----------------------------------------
----------------------------------------
| reward                  | -2.59      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.956      |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0258     |
| rollout/                |            |
|    ep_len_mean          | 90.7       |
|    ep_rew_mean          | -194       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 68         |
|    time_elapsed         | 314        |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.33374453 |
|    clip_fraction        | 0.481      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.479      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.39       |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.363      |
|    value_loss           | 44.8       |
----------------------------------------
---------------------------------------
| reward                  | -2.63     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.936     |
| reward_torque           | -3.59     |
| reward_velocity         | 0.0241    |
| rollout/                |           |
|    ep_len_mean          | 79.9      |
|    ep_rew_mean          | -172      |
| time/                   |           |
|    fps                  | 221       |
|    iterations           | 69        |
|    time_elapsed         | 319       |
|    total_timesteps      | 70656     |
| train/                  |           |
|    approx_kl            | 0.1711858 |
|    clip_fraction        | 0.412     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21       |
|    explained_variance   | 0.609     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.58      |
|    n_updates            | 1360      |
|    policy_gradient_loss | -0.174    |
|    std                  | 0.363     |
|    value_loss           | 39.8      |
---------------------------------------
---------------------------------------
| reward                  | -2.63     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.936     |
| reward_torque           | -3.59     |
| reward_velocity         | 0.0222    |
| rollout/                |           |
|    ep_len_mean          | 89.4      |
|    ep_rew_mean          | -191      |
| time/                   |           |
|    fps                  | 221       |
|    iterations           | 70        |
|    time_elapsed         | 324       |
|    total_timesteps      | 71680     |
| train/                  |           |
|    approx_kl            | 0.1989198 |
|    clip_fraction        | 0.42      |
|    clip_range           | 0.4       |
|    entropy_loss         | -21       |
|    explained_variance   | 0.418     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.96      |
|    n_updates            | 1380      |
|    policy_gradient_loss | -0.187    |
|    std                  | 0.363     |
|    value_loss           | 80.3      |
---------------------------------------
Num timesteps: 72000
Best mean reward: -87.77 - Last mean reward per episode: -191.68
----------------------------------------
| reward                  | -2.65      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.926      |
| reward_torque           | -3.6       |
| reward_velocity         | 0.022      |
| rollout/                |            |
|    ep_len_mean          | 89.7       |
|    ep_rew_mean          | -192       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 71         |
|    time_elapsed         | 328        |
|    total_timesteps      | 72704      |
| train/                  |            |
|    approx_kl            | 0.21040803 |
|    clip_fraction        | 0.43       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.642      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.49       |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.363      |
|    value_loss           | 43.6       |
----------------------------------------
----------------------------------------
| reward                  | -2.63      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.935      |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0215     |
| rollout/                |            |
|    ep_len_mean          | 98.5       |
|    ep_rew_mean          | -209       |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 72         |
|    time_elapsed         | 334        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.23942284 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.5        |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.183     |
|    std                  | 0.363      |
|    value_loss           | 28.2       |
----------------------------------------
----------------------------------------
| reward                  | -2.62      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.929      |
| reward_torque           | -3.57      |
| reward_velocity         | 0.02       |
| rollout/                |            |
|    ep_len_mean          | 92.5       |
|    ep_rew_mean          | -196       |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 73         |
|    time_elapsed         | 338        |
|    total_timesteps      | 74752      |
| train/                  |            |
|    approx_kl            | 0.16479537 |
|    clip_fraction        | 0.382      |
|    clip_range           | 0.4        |
|    entropy_loss         | -20.8      |
|    explained_variance   | 0.773      |
|    learning_rate        | 0.0003     |
|    loss                 | 10.1       |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.363      |
|    value_loss           | 78.3       |
----------------------------------------
----------------------------------------
| reward                  | -2.6       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.942      |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0153     |
| rollout/                |            |
|    ep_len_mean          | 78.3       |
|    ep_rew_mean          | -168       |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 74         |
|    time_elapsed         | 344        |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.20323634 |
|    clip_fraction        | 0.418      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.45       |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.169     |
|    std                  | 0.363      |
|    value_loss           | 63.7       |
----------------------------------------
----------------------------------------
| reward                  | -2.6       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.947      |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0159     |
| rollout/                |            |
|    ep_len_mean          | 88.9       |
|    ep_rew_mean          | -191       |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 75         |
|    time_elapsed         | 349        |
|    total_timesteps      | 76800      |
| train/                  |            |
|    approx_kl            | 0.53468585 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.66       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.25       |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.363      |
|    value_loss           | 46.1       |
----------------------------------------
----------------------------------------
| reward                  | -2.59      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.952      |
| reward_torque           | -3.56      |
| reward_velocity         | 0.0149     |
| rollout/                |            |
|    ep_len_mean          | 99.2       |
|    ep_rew_mean          | -212       |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 76         |
|    time_elapsed         | 354        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.16609463 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.0003     |
|    loss                 | 7.3        |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.363      |
|    value_loss           | 67.6       |
----------------------------------------
Num timesteps: 78000
Best mean reward: -87.77 - Last mean reward per episode: -211.87
----------------------------------------
| reward                  | -2.58      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.951      |
| reward_torque           | -3.55      |
| reward_velocity         | 0.0142     |
| rollout/                |            |
|    ep_len_mean          | 88.7       |
|    ep_rew_mean          | -191       |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 77         |
|    time_elapsed         | 359        |
|    total_timesteps      | 78848      |
| train/                  |            |
|    approx_kl            | 0.21269989 |
|    clip_fraction        | 0.415      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.482      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.67       |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.363      |
|    value_loss           | 58.6       |
----------------------------------------
---------------------------------------
| reward                  | -2.62     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.951     |
| reward_torque           | -3.58     |
| reward_velocity         | 0.0131    |
| rollout/                |           |
|    ep_len_mean          | 89.3      |
|    ep_rew_mean          | -193      |
| time/                   |           |
|    fps                  | 219       |
|    iterations           | 78        |
|    time_elapsed         | 364       |
|    total_timesteps      | 79872     |
| train/                  |           |
|    approx_kl            | 0.3063129 |
|    clip_fraction        | 0.467     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.1     |
|    explained_variance   | 0.56      |
|    learning_rate        | 0.0003    |
|    loss                 | 2.45      |
|    n_updates            | 1540      |
|    policy_gradient_loss | -0.184    |
|    std                  | 0.362     |
|    value_loss           | 69.7      |
---------------------------------------
----------------------------------------
| reward                  | -2.6       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.951      |
| reward_torque           | -3.57      |
| reward_velocity         | 0.0132     |
| rollout/                |            |
|    ep_len_mean          | 88.8       |
|    ep_rew_mean          | -192       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 79         |
|    time_elapsed         | 369        |
|    total_timesteps      | 80896      |
| train/                  |            |
|    approx_kl            | 0.20394605 |
|    clip_fraction        | 0.444      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.483      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.57       |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.191     |
|    std                  | 0.362      |
|    value_loss           | 74.8       |
----------------------------------------
----------------------------------------
| reward                  | -2.6       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.964      |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0151     |
| rollout/                |            |
|    ep_len_mean          | 88.4       |
|    ep_rew_mean          | -191       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 80         |
|    time_elapsed         | 374        |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.21989313 |
|    clip_fraction        | 0.377      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.556      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.76       |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.18      |
|    std                  | 0.362      |
|    value_loss           | 58.8       |
----------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.965      |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0168     |
| rollout/                |            |
|    ep_len_mean          | 87.3       |
|    ep_rew_mean          | -190       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 81         |
|    time_elapsed         | 379        |
|    total_timesteps      | 82944      |
| train/                  |            |
|    approx_kl            | 0.44319522 |
|    clip_fraction        | 0.557      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.733      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.77       |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.189     |
|    std                  | 0.362      |
|    value_loss           | 52.6       |
----------------------------------------
----------------------------------------
| reward                  | -2.62      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.955      |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0164     |
| rollout/                |            |
|    ep_len_mean          | 81.8       |
|    ep_rew_mean          | -177       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 82         |
|    time_elapsed         | 384        |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.19507873 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.633      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.19       |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.362      |
|    value_loss           | 57.1       |
----------------------------------------
Num timesteps: 84000
Best mean reward: -87.77 - Last mean reward per episode: -177.34
----------------------------------------
| reward                  | -2.63      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.947      |
| reward_torque           | -3.59      |
| reward_velocity         | 0.0187     |
| rollout/                |            |
|    ep_len_mean          | 76.2       |
|    ep_rew_mean          | -166       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 83         |
|    time_elapsed         | 389        |
|    total_timesteps      | 84992      |
| train/                  |            |
|    approx_kl            | 0.13268054 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.359      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.56       |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.16      |
|    std                  | 0.362      |
|    value_loss           | 76.2       |
----------------------------------------
----------------------------------------
| reward                  | -2.61      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.947      |
| reward_torque           | -3.58      |
| reward_velocity         | 0.02       |
| rollout/                |            |
|    ep_len_mean          | 76.2       |
|    ep_rew_mean          | -166       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 84         |
|    time_elapsed         | 394        |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.19481435 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.0003     |
|    loss                 | 4.05       |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.362      |
|    value_loss           | 88.6       |
----------------------------------------
----------------------------------------
| reward                  | -2.6       |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.955      |
| reward_torque           | -3.58      |
| reward_velocity         | 0.0215     |
| rollout/                |            |
|    ep_len_mean          | 85.7       |
|    ep_rew_mean          | -185       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 85         |
|    time_elapsed         | 399        |
|    total_timesteps      | 87040      |
| train/                  |            |
|    approx_kl            | 0.24213727 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.2      |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.64       |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.362      |
|    value_loss           | 25.4       |
----------------------------------------
----------------------------------------
| reward                  | -2.65      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.944      |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0248     |
| rollout/                |            |
|    ep_len_mean          | 87.3       |
|    ep_rew_mean          | -188       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 86         |
|    time_elapsed         | 404        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.28687358 |
|    clip_fraction        | 0.481      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21        |
|    explained_variance   | 0.743      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.31       |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.362      |
|    value_loss           | 28.3       |
----------------------------------------
----------------------------------------
| reward                  | -2.64      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.951      |
| reward_torque           | -3.62      |
| reward_velocity         | 0.0271     |
| rollout/                |            |
|    ep_len_mean          | 98.1       |
|    ep_rew_mean          | -210       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 87         |
|    time_elapsed         | 409        |
|    total_timesteps      | 89088      |
| train/                  |            |
|    approx_kl            | 0.17261931 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.544      |
|    learning_rate        | 0.0003     |
|    loss                 | 4.55       |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.362      |
|    value_loss           | 104        |
----------------------------------------
Num timesteps: 90000
Best mean reward: -87.77 - Last mean reward per episode: -192.07
----------------------------------------
| reward                  | -2.66      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.951      |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0291     |
| rollout/                |            |
|    ep_len_mean          | 89.6       |
|    ep_rew_mean          | -192       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 88         |
|    time_elapsed         | 413        |
|    total_timesteps      | 90112      |
| train/                  |            |
|    approx_kl            | 0.17289877 |
|    clip_fraction        | 0.419      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.3      |
|    explained_variance   | 0.326      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.32       |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.173     |
|    std                  | 0.362      |
|    value_loss           | 67.4       |
----------------------------------------
---------------------------------------
| reward                  | -2.65     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.943     |
| reward_torque           | -3.62     |
| reward_velocity         | 0.0276    |
| rollout/                |           |
|    ep_len_mean          | 88        |
|    ep_rew_mean          | -189      |
| time/                   |           |
|    fps                  | 217       |
|    iterations           | 89        |
|    time_elapsed         | 418       |
|    total_timesteps      | 91136     |
| train/                  |           |
|    approx_kl            | 0.2088778 |
|    clip_fraction        | 0.437     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.2     |
|    explained_variance   | 0.687     |
|    learning_rate        | 0.0003    |
|    loss                 | 2.98      |
|    n_updates            | 1760      |
|    policy_gradient_loss | -0.177    |
|    std                  | 0.362     |
|    value_loss           | 38.2      |
---------------------------------------
---------------------------------------
| reward                  | -2.68     |
| reward_contact          | 0         |
| reward_ctrl             | 0         |
| reward_motion           | 0.927     |
| reward_torque           | -3.63     |
| reward_velocity         | 0.0249    |
| rollout/                |           |
|    ep_len_mean          | 88.9      |
|    ep_rew_mean          | -191      |
| time/                   |           |
|    fps                  | 217       |
|    iterations           | 90        |
|    time_elapsed         | 423       |
|    total_timesteps      | 92160     |
| train/                  |           |
|    approx_kl            | 0.2544246 |
|    clip_fraction        | 0.472     |
|    clip_range           | 0.4       |
|    entropy_loss         | -21.1     |
|    explained_variance   | 0.721     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.96      |
|    n_updates            | 1780      |
|    policy_gradient_loss | -0.18     |
|    std                  | 0.362     |
|    value_loss           | 53.3      |
---------------------------------------
----------------------------------------
| reward                  | -2.68      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.929      |
| reward_torque           | -3.64      |
| reward_velocity         | 0.0303     |
| rollout/                |            |
|    ep_len_mean          | 98.8       |
|    ep_rew_mean          | -212       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 91         |
|    time_elapsed         | 428        |
|    total_timesteps      | 93184      |
| train/                  |            |
|    approx_kl            | 0.27079284 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.712      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.16       |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.158     |
|    std                  | 0.362      |
|    value_loss           | 36.4       |
----------------------------------------
----------------------------------------
| reward                  | -2.69      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.935      |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0312     |
| rollout/                |            |
|    ep_len_mean          | 111        |
|    ep_rew_mean          | -237       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 92         |
|    time_elapsed         | 433        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.16997279 |
|    clip_fraction        | 0.416      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.606      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.07       |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.139     |
|    std                  | 0.362      |
|    value_loss           | 37.7       |
----------------------------------------
----------------------------------------
| reward                  | -2.69      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.934      |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0255     |
| rollout/                |            |
|    ep_len_mean          | 78.5       |
|    ep_rew_mean          | -171       |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 93         |
|    time_elapsed         | 439        |
|    total_timesteps      | 95232      |
| train/                  |            |
|    approx_kl            | 0.14407443 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.1      |
|    explained_variance   | 0.431      |
|    learning_rate        | 0.0003     |
|    loss                 | 15.7       |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.362      |
|    value_loss           | 106        |
----------------------------------------
Num timesteps: 96000
Best mean reward: -87.77 - Last mean reward per episode: -170.84
----------------------------------------
| reward                  | -2.69      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.934      |
| reward_torque           | -3.65      |
| reward_velocity         | 0.0256     |
| rollout/                |            |
|    ep_len_mean          | 88.5       |
|    ep_rew_mean          | -192       |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 94         |
|    time_elapsed         | 444        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.34288812 |
|    clip_fraction        | 0.481      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.5      |
|    explained_variance   | 0.617      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.28       |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.204     |
|    std                  | 0.362      |
|    value_loss           | 93.1       |
----------------------------------------
----------------------------------------
| reward                  | -2.71      |
| reward_contact          | 0          |
| reward_ctrl             | 0          |
| reward_motion           | 0.925      |
| reward_torque           | -3.66      |
| reward_velocity         | 0.02       |
| rollout/                |            |
|    ep_len_mean          | 67.4       |
|    ep_rew_mean          | -149       |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 95         |
|    time_elapsed         | 449        |
|    total_timesteps      | 97280      |
| train/                  |            |
|    approx_kl            | 0.15484408 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.4        |
|    entropy_loss         | -21.4      |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 6.64       |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.175     |
|    std                  | 0.362      |
|    value_loss           | 68.6       |
----------------------------------------
usage: ddpg.py [-h] [--experiment EXPERIMENT] [--out_path OUT_PATH]
               [--env ENV] [--env_version ENV_VERSION] [--model_dir MODEL_DIR]
               [--test_env [TEST_ENV]] [--her [HER]] [--ppo [PPO]]
               [--sac [SAC]] [--a2c [A2C]] [--standard [STANDARD]]
               [--evaluate [EVALUATE]]
ddpg.py: error: argument --experiment: invalid int value: '74^C-out_path'
running build_ext
Failed to import optional module imus. Install optional dependencies
Failed to import optional module misc. Install optional dependencies
[DDPG] Using HER
Using cuda device
Logging to rl/out_dir/models/exp74/SAC_1
----------------------------------
| reward             | -1.16     |
| reward_contact     | -0.0297   |
| reward_motion      | 0.869     |
| reward_torque      | -0.423    |
| reward_velocity    | -1.57     |
| rollout/           |           |
|    ep_len_mean     | 1.02e+03  |
|    ep_rew_mean     | -1.93e+03 |
| time/              |           |
|    episodes        | 4         |
|    fps             | 981       |
|    time_elapsed    | 4         |
|    total timesteps | 4096      |
| train/             |           |
|    std             | 0.0498    |
----------------------------------
Num timesteps: 6000
Best mean reward: -inf - Last mean reward per episode: -1925.16
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.39     |
| reward_contact     | -0.0224   |
| reward_motion      | 1.21      |
| reward_torque      | -0.448    |
| reward_velocity    | -2.13     |
| rollout/           |           |
|    ep_len_mean     | 1.02e+03  |
|    ep_rew_mean     | -1.92e+03 |
| time/              |           |
|    episodes        | 8         |
|    fps             | 880       |
|    time_elapsed    | 9         |
|    total timesteps | 8192      |
| train/             |           |
|    std             | 0.0498    |
----------------------------------
---------------------------------
| reward             | -1.29    |
| reward_contact     | -0.0203  |
| reward_motion      | 1.18     |
| reward_torque      | -0.383   |
| reward_velocity    | -2.07    |
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | -1.8e+03 |
| time/              |          |
|    episodes        | 12       |
|    fps             | 237      |
|    time_elapsed    | 49       |
|    total timesteps | 11874    |
| train/             |          |
|    actor_loss      | 582      |
|    critic_loss     | 738      |
|    ent_coef        | 0.755    |
|    ent_coef_loss   | -0.464   |
|    learning_rate   | 0.0003   |
|    n_updates       | 1856     |
|    std             | 0.0498   |
---------------------------------
Num timesteps: 12000
Best mean reward: -1925.16 - Last mean reward per episode: -1800.38
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.27     |
| reward_contact     | -0.0259   |
| reward_motion      | 0.891     |
| reward_torque      | -0.292    |
| reward_velocity    | -1.84     |
| rollout/           |           |
|    ep_len_mean     | 998       |
|    ep_rew_mean     | -1.71e+03 |
| time/              |           |
|    episodes        | 16        |
|    fps             | 120       |
|    time_elapsed    | 132       |
|    total timesteps | 15970     |
| train/             |           |
|    actor_loss      | 1.55e+03  |
|    critic_loss     | 609       |
|    ent_coef        | 0.27      |
|    ent_coef_loss   | -5.55     |
|    learning_rate   | 0.0003    |
|    n_updates       | 5952      |
|    std             | 0.0497    |
----------------------------------
Num timesteps: 18000
Best mean reward: -1800.38 - Last mean reward per episode: -1707.11
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.26     |
| reward_contact     | -0.0268   |
| reward_motion      | 0.78      |
| reward_torque      | -0.249    |
| reward_velocity    | -1.77     |
| rollout/           |           |
|    ep_len_mean     | 1e+03     |
|    ep_rew_mean     | -1.68e+03 |
| time/              |           |
|    episodes        | 20        |
|    fps             | 93        |
|    time_elapsed    | 215       |
|    total timesteps | 20066     |
| train/             |           |
|    actor_loss      | 1.72e+03  |
|    critic_loss     | 602       |
|    ent_coef        | 0.207     |
|    ent_coef_loss   | 0.334     |
|    learning_rate   | 0.0003    |
|    n_updates       | 10048     |
|    std             | 0.0491    |
----------------------------------
Num timesteps: 24000
Best mean reward: -1707.11 - Last mean reward per episode: -1690.20
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.23     |
| reward_contact     | -0.0282   |
| reward_motion      | 0.696     |
| reward_torque      | -0.217    |
| reward_velocity    | -1.68     |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.69e+03 |
| time/              |           |
|    episodes        | 24        |
|    fps             | 81        |
|    time_elapsed    | 297       |
|    total timesteps | 24162     |
| train/             |           |
|    actor_loss      | 1.67e+03  |
|    critic_loss     | 693       |
|    ent_coef        | 0.212     |
|    ent_coef_loss   | -0.429    |
|    learning_rate   | 0.0003    |
|    n_updates       | 14144     |
|    std             | 0.0486    |
----------------------------------
----------------------------------
| reward             | -1.31     |
| reward_contact     | -0.0305   |
| reward_motion      | 0.647     |
| reward_torque      | -0.199    |
| reward_velocity    | -1.73     |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.67e+03 |
| time/              |           |
|    episodes        | 28        |
|    fps             | 74        |
|    time_elapsed    | 381       |
|    total timesteps | 28258     |
| train/             |           |
|    actor_loss      | 1.67e+03  |
|    critic_loss     | 678       |
|    ent_coef        | 0.154     |
|    ent_coef_loss   | -1.18     |
|    learning_rate   | 0.0003    |
|    n_updates       | 18240     |
|    std             | 0.0484    |
----------------------------------
Num timesteps: 30000
Best mean reward: -1690.20 - Last mean reward per episode: -1668.32
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.34     |
| reward_contact     | -0.0297   |
| reward_motion      | 0.643     |
| reward_torque      | -0.195    |
| reward_velocity    | -1.76     |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.67e+03 |
| time/              |           |
|    episodes        | 32        |
|    fps             | 69        |
|    time_elapsed    | 464       |
|    total timesteps | 32354     |
| train/             |           |
|    actor_loss      | 1.72e+03  |
|    critic_loss     | 656       |
|    ent_coef        | 0.141     |
|    ent_coef_loss   | -0.192    |
|    learning_rate   | 0.0003    |
|    n_updates       | 22336     |
|    std             | 0.0482    |
----------------------------------
----------------------------------
| reward             | -1.42     |
| reward_contact     | -0.0308   |
| reward_motion      | 0.596     |
| reward_torque      | -0.185    |
| reward_velocity    | -1.8      |
| rollout/           |           |
|    ep_len_mean     | 1e+03     |
|    ep_rew_mean     | -1.65e+03 |
| time/              |           |
|    episodes        | 36        |
|    fps             | 67        |
|    time_elapsed    | 537       |
|    total timesteps | 35997     |
| train/             |           |
|    actor_loss      | 1.72e+03  |
|    critic_loss     | 405       |
|    ent_coef        | 0.0992    |
|    ent_coef_loss   | -0.699    |
|    learning_rate   | 0.0003    |
|    n_updates       | 25984     |
|    std             | 0.048     |
----------------------------------
Num timesteps: 36000
Best mean reward: -1668.32 - Last mean reward per episode: -1653.08
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.42     |
| reward_contact     | -0.0299   |
| reward_motion      | 0.586     |
| reward_torque      | -0.182    |
| reward_velocity    | -1.8      |
| rollout/           |           |
|    ep_len_mean     | 1e+03     |
|    ep_rew_mean     | -1.64e+03 |
| time/              |           |
|    episodes        | 40        |
|    fps             | 64        |
|    time_elapsed    | 619       |
|    total timesteps | 40093     |
| train/             |           |
|    actor_loss      | 1.72e+03  |
|    critic_loss     | 541       |
|    ent_coef        | 0.0747    |
|    ent_coef_loss   | -1.07     |
|    learning_rate   | 0.0003    |
|    n_updates       | 30080     |
|    std             | 0.0475    |
----------------------------------
Num timesteps: 42000
Best mean reward: -1653.08 - Last mean reward per episode: -1651.14
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.43     |
| reward_contact     | -0.031    |
| reward_motion      | 0.536     |
| reward_torque      | -0.171    |
| reward_velocity    | -1.76     |
| rollout/           |           |
|    ep_len_mean     | 1e+03     |
|    ep_rew_mean     | -1.64e+03 |
| time/              |           |
|    episodes        | 44        |
|    fps             | 62        |
|    time_elapsed    | 702       |
|    total timesteps | 44189     |
| train/             |           |
|    actor_loss      | 1.68e+03  |
|    critic_loss     | 418       |
|    ent_coef        | 0.0831    |
|    ent_coef_loss   | -1.01     |
|    learning_rate   | 0.0003    |
|    n_updates       | 34176     |
|    std             | 0.0471    |
----------------------------------
Num timesteps: 48000
Best mean reward: -1651.14 - Last mean reward per episode: -1646.53
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.44     |
| reward_contact     | -0.0304   |
| reward_motion      | 0.524     |
| reward_torque      | -0.168    |
| reward_velocity    | -1.77     |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.65e+03 |
| time/              |           |
|    episodes        | 48        |
|    fps             | 61        |
|    time_elapsed    | 785       |
|    total timesteps | 48285     |
| train/             |           |
|    actor_loss      | 1.69e+03  |
|    critic_loss     | 947       |
|    ent_coef        | 0.0683    |
|    ent_coef_loss   | -0.652    |
|    learning_rate   | 0.0003    |
|    n_updates       | 38272     |
|    std             | 0.0468    |
----------------------------------
----------------------------------
| reward             | -1.46     |
| reward_contact     | -0.0314   |
| reward_motion      | 0.499     |
| reward_torque      | -0.162    |
| reward_velocity    | -1.76     |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.65e+03 |
| time/              |           |
|    episodes        | 52        |
|    fps             | 60        |
|    time_elapsed    | 868       |
|    total timesteps | 52381     |
| train/             |           |
|    actor_loss      | 1.7e+03   |
|    critic_loss     | 2.27e+03  |
|    ent_coef        | 0.0672    |
|    ent_coef_loss   | 2.05      |
|    learning_rate   | 0.0003    |
|    n_updates       | 42368     |
|    std             | 0.0465    |
----------------------------------
Num timesteps: 54000
Best mean reward: -1646.53 - Last mean reward per episode: -1640.94
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.45     |
| reward_contact     | -0.0324   |
| reward_motion      | 0.482     |
| reward_torque      | -0.162    |
| reward_velocity    | -1.74     |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.64e+03 |
| time/              |           |
|    episodes        | 56        |
|    fps             | 59        |
|    time_elapsed    | 950       |
|    total timesteps | 56477     |
| train/             |           |
|    actor_loss      | 1.75e+03  |
|    critic_loss     | 535       |
|    ent_coef        | 0.0666    |
|    ent_coef_loss   | -0.558    |
|    learning_rate   | 0.0003    |
|    n_updates       | 46464     |
|    std             | 0.0462    |
----------------------------------
Num timesteps: 60000
Best mean reward: -1640.94 - Last mean reward per episode: -1629.86
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.45     |
| reward_contact     | -0.0315   |
| reward_motion      | 0.505     |
| reward_torque      | -0.169    |
| reward_velocity    | -1.75     |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.63e+03 |
| time/              |           |
|    episodes        | 60        |
|    fps             | 58        |
|    time_elapsed    | 1027      |
|    total timesteps | 60301     |
| train/             |           |
|    actor_loss      | 1.79e+03  |
|    critic_loss     | 1.07e+03  |
|    ent_coef        | 0.0558    |
|    ent_coef_loss   | -0.522    |
|    learning_rate   | 0.0003    |
|    n_updates       | 50304     |
|    std             | 0.0459    |
----------------------------------
----------------------------------
| reward             | -1.48     |
| reward_contact     | -0.0305   |
| reward_motion      | 0.519     |
| reward_torque      | -0.171    |
| reward_velocity    | -1.79     |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.63e+03 |
| time/              |           |
|    episodes        | 64        |
|    fps             | 58        |
|    time_elapsed    | 1110      |
|    total timesteps | 64397     |
| train/             |           |
|    actor_loss      | 1.82e+03  |
|    critic_loss     | 790       |
|    ent_coef        | 0.0543    |
|    ent_coef_loss   | -0.724    |
|    learning_rate   | 0.0003    |
|    n_updates       | 54400     |
|    std             | 0.0455    |
----------------------------------
Num timesteps: 66000
Best mean reward: -1629.86 - Last mean reward per episode: -1636.83
----------------------------------
| reward             | -1.49     |
| reward_contact     | -0.0305   |
| reward_motion      | 0.509     |
| reward_torque      | -0.164    |
| reward_velocity    | -1.8      |
| rollout/           |           |
|    ep_len_mean     | 1.01e+03  |
|    ep_rew_mean     | -1.63e+03 |
| time/              |           |
|    episodes        | 68        |
|    fps             | 57        |
|    time_elapsed    | 1192      |
|    total timesteps | 68493     |
| train/             |           |
|    actor_loss      | 1.82e+03  |
|    critic_loss     | 921       |
|    ent_coef        | 0.0479    |
|    ent_coef_loss   | -0.744    |
|    learning_rate   | 0.0003    |
|    n_updates       | 58496     |
|    std             | 0.0451    |
----------------------------------
----------------------------------
| reward             | -1.45     |
| reward_contact     | -0.0296   |
| reward_motion      | 0.559     |
| reward_torque      | -0.162    |
| reward_velocity    | -1.82     |
| rollout/           |           |
|    ep_len_mean     | 997       |
|    ep_rew_mean     | -1.62e+03 |
| time/              |           |
|    episodes        | 72        |
|    fps             | 57        |
|    time_elapsed    | 1259      |
|    total timesteps | 71801     |
| train/             |           |
|    actor_loss      | 1.83e+03  |
|    critic_loss     | 642       |
|    ent_coef        | 0.0454    |
|    ent_coef_loss   | 1.17      |
|    learning_rate   | 0.0003    |
|    n_updates       | 61760     |
|    std             | 0.0447    |
----------------------------------
Num timesteps: 72000
Best mean reward: -1629.86 - Last mean reward per episode: -1595.16
Saving new best model to rl/out_dir/models/exp74/best_model.zip
---------------------------------
| reward             | -1.45    |
| reward_contact     | -0.029   |
| reward_motion      | 0.563    |
| reward_torque      | -0.16    |
| reward_velocity    | -1.82    |
| rollout/           |          |
|    ep_len_mean     | 986      |
|    ep_rew_mean     | -1.6e+03 |
| time/              |          |
|    episodes        | 76       |
|    fps             | 56       |
|    time_elapsed    | 1323     |
|    total timesteps | 74961    |
| train/             |          |
|    actor_loss      | 1.84e+03 |
|    critic_loss     | 603      |
|    ent_coef        | 0.0412   |
|    ent_coef_loss   | 0.318    |
|    learning_rate   | 0.0003   |
|    n_updates       | 64960    |
|    std             | 0.0443   |
---------------------------------
Num timesteps: 78000
Best mean reward: -1595.16 - Last mean reward per episode: -1598.11
---------------------------------
| reward             | -1.48    |
| reward_contact     | -0.03    |
| reward_motion      | 0.552    |
| reward_torque      | -0.16    |
| reward_velocity    | -1.84    |
| rollout/           |          |
|    ep_len_mean     | 988      |
|    ep_rew_mean     | -1.6e+03 |
| time/              |          |
|    episodes        | 80       |
|    fps             | 56       |
|    time_elapsed    | 1405     |
|    total timesteps | 79057    |
| train/             |          |
|    actor_loss      | 1.86e+03 |
|    critic_loss     | 636      |
|    ent_coef        | 0.0427   |
|    ent_coef_loss   | -1.35    |
|    learning_rate   | 0.0003   |
|    n_updates       | 69056    |
|    std             | 0.0439   |
---------------------------------
---------------------------------
| reward             | -1.49    |
| reward_contact     | -0.0302  |
| reward_motion      | 0.553    |
| reward_torque      | -0.164   |
| reward_velocity    | -1.85    |
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | -1.6e+03 |
| time/              |          |
|    episodes        | 84       |
|    fps             | 55       |
|    time_elapsed    | 1487     |
|    total timesteps | 83153    |
| train/             |          |
|    actor_loss      | 1.91e+03 |
|    critic_loss     | 762      |
|    ent_coef        | 0.0353   |
|    ent_coef_loss   | -0.806   |
|    learning_rate   | 0.0003   |
|    n_updates       | 73152    |
|    std             | 0.0434   |
---------------------------------
Num timesteps: 84000
Best mean reward: -1595.16 - Last mean reward per episode: -1594.25
Saving new best model to rl/out_dir/models/exp74/best_model.zip
----------------------------------
| reward             | -1.46     |
| reward_contact     | -0.0295   |
| reward_motion      | 0.557     |
| reward_torque      | -0.166    |
| reward_velocity    | -1.82     |
| rollout/           |           |
|    ep_len_mean     | 987       |
|    ep_rew_mean     | -1.59e+03 |
| time/              |           |
|    episodes        | 88        |
|    fps             | 55        |
|    time_elapsed    | 1560      |
|    total timesteps | 86825     |
| train/             |           |
|    actor_loss      | 1.91e+03  |
|    critic_loss     | 686       |
|    ent_coef        | 0.04      |
|    ent_coef_loss   | 0.938     |
|    learning_rate   | 0.0003    |
|    n_updates       | 76800     |
|    std             | 0.043     |
----------------------------------
